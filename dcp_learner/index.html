<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Discrimination-aware Channel Pruning - PocketFlow Docs</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Discrimination-aware Channel Pruning";
    var mkdocs_page_input_path = "dcp_learner.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> PocketFlow Docs</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../installation/">Installation</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../tutorial/">Tutorial</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Learners - Algorithms</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../cp_learner/">Channel Pruning</a>
                </li>
                <li class=" current">
                    
    <a class="current" href="./">Discrimination-aware Channel Pruning</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#discrimination-aware-channel-pruning">Discrimination-aware Channel Pruning</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#introduction">Introduction</a></li>
        
            <li><a class="toctree-l4" href="#algorithm-description">Algorithm Description</a></li>
        
            <li><a class="toctree-l4" href="#hyper-parameters">Hyper-parameters</a></li>
        
            <li><a class="toctree-l4" href="#usage-examples">Usage Examples</a></li>
        
        </ul>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../ws_learner/">Weight Sparsification</a>
                </li>
                <li class="">
                    
    <a class="" href="../uq_learner/">Uniform Quantization</a>
                </li>
                <li class="">
                    
    <a class="" href="../nuq_learner/">Non-uniform Quantization</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Learners - Misc.</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../distillation/">Distillation</a>
                </li>
                <li class="">
                    
    <a class="" href="../multi_gpu_training/">Multi-GPU Training</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Hyper-parameter Optimizers</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../reinforcement_learning/">Reinforcement Learning</a>
                </li>
                <li class="">
                    
    <a class="" href="../automl_based_methods/">AutoML-based Methods</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../performance/">Performance</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../faq/">Frequently Asked Questions</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Appendix</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../pre_trained_models/">Pre-trained Models</a>
                </li>
                <li class="">
                    
    <a class="" href="../test_cases/">Test Cases</a>
                </li>
                <li class="">
                    
    <a class="" href="../reference/">Reference</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">PocketFlow Docs</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
        
          <li>Learners - Algorithms &raquo;</li>
        
      
    
    <li>Discrimination-aware Channel Pruning</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="discrimination-aware-channel-pruning">Discrimination-aware Channel Pruning</h1>
<h2 id="introduction">Introduction</h2>
<p>Discrimination-aware channel pruning (DCP, Zhuang et al., 2018) introduces a group of additional discriminative losses into the network to be pruned, to find out which channels are really contributing to the discriminative power and should be preserved. After channel pruning, the number of input channels of each convolutional layer is reduced, so that the model becomes smaller and the inference speed can be improved.</p>
<h2 id="algorithm-description">Algorithm Description</h2>
<p>For a convolutional layer, we denote its input feature map as <span><span class="MathJax_Preview">\mathbf{X} \in \mathbb{R}^{N \times c_{i} \times h_{i} \times w_{i}}</span><script type="math/tex">\mathbf{X} \in \mathbb{R}^{N \times c_{i} \times h_{i} \times w_{i}}</script></span>, where <span><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span> is the batch size, <span><span class="MathJax_Preview">c_{i}</span><script type="math/tex">c_{i}</script></span> is the number of inputs channels, and <span><span class="MathJax_Preview">h_{i}</span><script type="math/tex">h_{i}</script></span> and <span><span class="MathJax_Preview">w_{i}</span><script type="math/tex">w_{i}</script></span> are the spatial height and width. The convolutional kernel is denoted as <span><span class="MathJax_Preview">\mathbf{W} \in \mathbb{R}^{c_{o} \times c_{i} \times k \times k}</span><script type="math/tex">\mathbf{W} \in \mathbb{R}^{c_{o} \times c_{i} \times k \times k}</script></span>, where <span><span class="MathJax_Preview">c_{o}</span><script type="math/tex">c_{o}</script></span> is the number of output channels and <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> is the kernel size. The resulting output feature map is given by <span><span class="MathJax_Preview">\mathbf{Y} = f \left( \mathbf{X}; \mathbf{W} \right)</span><script type="math/tex">\mathbf{Y} = f \left( \mathbf{X}; \mathbf{W} \right)</script></span>, where <span><span class="MathJax_Preview">f \left( \cdot \right)</span><script type="math/tex">f \left( \cdot \right)</script></span> represents the convolutional operation.</p>
<p>The idea of channel pruning is to impose the sparsity constraint on the convolutional kernel, so that some of its input channels only contains all-zero weights and can be safely removed. For instance, if the convolutional kernel satisifies:</p>
<div>
<div class="MathJax_Preview">
\left\| \left\| \mathbf{W}_{:, j, :, :} \right\|_{F}^{2} \right\|_{0} = c'_{i},
</div>
<script type="math/tex; mode=display">
\left\| \left\| \mathbf{W}_{:, j, :, :} \right\|_{F}^{2} \right\|_{0} = c'_{i},
</script>
</div>
<p>where <span><span class="MathJax_Preview">c'_{i} \lt c_{i}</span><script type="math/tex">c'_{i} \lt c_{i}</script></span>, then the convolutional layer simplified to with <span><span class="MathJax_Preview">c'_{i}</span><script type="math/tex">c'_{i}</script></span> input channels only, and the computational complexity is reduced by a ratio of <span><span class="MathJax_Preview">\frac{c_{i} - c'_{i}}{c_{i}}</span><script type="math/tex">\frac{c_{i} - c'_{i}}{c_{i}}</script></span>.</p>
<p>In order to reduce the performance degradation caused by channel pruning, the DCP algorithm introduces a novel channel selection algorithm by incorporating additional discrimination-aware and reconstruction loss terms, as shown below.</p>
<p><img alt="DCP Learner" src="../dcp_learner.png" />
<strong>Source:</strong> Zhuang et al., <em>Discrimination-aware Channel Pruning for Deep Neural Networks</em>. NIPS '18.</p>
<p>The network is evenly divided into <span><span class="MathJax_Preview">\left( P + 1 \right)</span><script type="math/tex">\left( P + 1 \right)</script></span> blocks. For each of the first <span><span class="MathJax_Preview">P</span><script type="math/tex">P</script></span> blocks, an extra branch is derived from the output feature map of this block's last layer. The output feature map is then passed through batch normalization &amp; ReLU &amp; average pooling &amp; softmax layers to produce predictions, from which a discrimination-aware loss is constructed, denoted as <span><span class="MathJax_Preview">L_{p}</span><script type="math/tex">L_{p}</script></span>. For the last block, the final loss of whole network, denoted as <span><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span>, is used as its discrimination-aware loss. Additionally, for each layer in the channel pruned network, a reconstruction loss is introduced to force it to re-produce the corresponding output feature map in the original network. We denote the <span><span class="MathJax_Preview">q</span><script type="math/tex">q</script></span>-th layer's reconstruction loss as <span><span class="MathJax_Preview">L_{q}^{( R )}</span><script type="math/tex">L_{q}^{( R )}</script></span>.</p>
<p>Based on a pre-trained model, the DCP algorithm performs channel pruning with <span><span class="MathJax_Preview">\left( P + 1 \right)</span><script type="math/tex">\left( P + 1 \right)</script></span> stages. During the <span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span>-th stage, the network is fine-tuned with the <span><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span>-th discrimination-aware loss <span><span class="MathJax_Preview">L_{p}</span><script type="math/tex">L_{p}</script></span> plus the final loss <span><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span>. After the block-wise fine-tuning, we sequentially perform channel pruning for each convolutional layer within the block. For channel pruning, we compute each input channel's gradients <em>w.r.t.</em> the reconstruction loss <span><span class="MathJax_Preview">L_{q}^{( R )}</span><script type="math/tex">L_{q}^{( R )}</script></span> plus the discrimination-aware loss <span><span class="MathJax_Preview">L_{p}</span><script type="math/tex">L_{p}</script></span>, and remove the input channel with the minimal Frobenius norm of gradients. After that, this layer is fine-tuned with the remaining input channels only to (partially) recover the discriminative power. We repeat this process until the target pruning ratio is reached.</p>
<p>After all convolutional layers have been pruned, the resulting network can be further fine-tuned for a few epochs to further reduce the performance loss.</p>
<h2 id="hyper-parameters">Hyper-parameters</h2>
<p>Below is the full list of hyper-parameters used in the discrimination-aware channel pruning learner:</p>
<table>
<thead>
<tr>
<th align="left">Name</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><code>dcp_save_path</code></td>
<td align="left">model's save path</td>
</tr>
<tr>
<td align="left"><code>dcp_save_path_eval</code></td>
<td align="left">model's save path for evaluation</td>
</tr>
<tr>
<td align="left"><code>dcp_prune_ratio</code></td>
<td align="left">target pruning ratio</td>
</tr>
<tr>
<td align="left"><code>dcp_nb_stages</code></td>
<td align="left">number of channel pruning stages</td>
</tr>
<tr>
<td align="left"><code>dcp_lrn_rate_adam</code></td>
<td align="left">Adam's learning rate for block-wise &amp; layer-wise fine-tuning</td>
</tr>
<tr>
<td align="left"><code>dcp_nb_iters_block</code></td>
<td align="left">number of iterations for block-wise fine-tuning</td>
</tr>
<tr>
<td align="left"><code>dcp_nb_iters_layer</code></td>
<td align="left">number of iterations for layer-wise fine-tuning</td>
</tr>
</tbody>
</table>
<p>Here, we provide detailed description (and some analysis) for above hyper-parameters:</p>
<ul>
<li><code>dcp_save_path</code>: save path for model created in the training graph. The resulting checkpoint files can be used to resume training from a previous run and compute model's loss function's value and some other evaluation metrics.</li>
<li><code>dcp_save_path_eval</code>: save path for model created in the evaluation graph. The resulting checkpoint files can be used to export GraphDef &amp; TensorFlow Lite model files.</li>
<li><code>dcp_prune_ratio</code>: target pruning ratio for input channels of each convolutional layer. The larger <code>dcp_prune_ratio</code> is, the more input channels will be pruned. If <code>dcp_prune_ratio</code> equals 0, then no input channels will be pruned and model remains the same; if <code>dcp_prune_ratio</code> equals 1, then all input channels will be pruned.</li>
<li><code>dcp_nb_stages</code>: number of channel pruning stages / number of discrimination-aware losses. The training process of DCP algorithm is divided into multiple stages. For each discrimination-aware loss, a channel pruning stage is involved to select channels within corresponding layers. The final classification loss corresponds to a pseudo channel pruning stage, which is not counted in <code>dcp_nb_stages</code>.The larger <code>dcp_nb_stages</code> is, the slower the training process will be.</li>
<li><code>dcp_lrn_rate_adam</code>: Adam's learning rate for block-wise &amp; layer-wise fine-tuning. If <code>dcp_lrn_rate_adam</code> is too large, then the fine-tuning process may become unstable; if <code>dcp_lrn_rate_adam</code> is too small, then the fine-tuning process may take long time to converge.</li>
<li><code>dcp_nb_iters_block</code>: number of iterations for block-wise fine-tuning. This should be set to some value that the block-wise fine-tuning can almost converge and the loss function's value does not decrease much even if more iterations are used.</li>
<li><code>dcp_nb_iters_layer</code>: number of iterations for layer-wise fine-tuning. This should be set to some value that the layer-wise fine-tuning can almost converge and the loss function's value does not decrease much even if more iterations are used.</li>
</ul>
<h2 id="usage-examples">Usage Examples</h2>
<p>In this section, we provide some usage examples to demonstrate how to use <code>DisChnPrunedLearner</code> under different execution modes and hyper-parameter combinations:</p>
<p>To compress a ResNet-20 model for CIFAR-10 classification task in the local mode, use:</p>
<pre><code class="bash"># set the target pruning ratio to 0.75
./scripts/run_local.sh nets/resnet_at_cifar10_run.py \
    --learner dis-chn-pruned \
    --dcp_prune_ratio 0.75
</code></pre>

<p>To compress a ResNet-34 model for ILSVRC-12 classification task in the docker mode with 4 GPUs, use:</p>
<pre><code class="bash"># set the number of channel pruning stages to 4
./scripts/run_docker.sh nets/resnet_at_ilsvrc12_run.py -n=4 \
    --learner dis-chn-pruned \
    --data_disk docker \
    --resnet_size 34 \
    --dcp_nb_stages 4
</code></pre>

<p>To compress a MobileNet-v2 model for ILSVRC-12 classification task in the seven mode with 8 GPUs, use:</p>
<pre><code class="bash"># enable training with distillation loss
./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py -n=8 \
    --learner dis-chn-pruned \
    --data_disk seven \
    --mobilenet_version 2 \
    --enbl_dst
</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../ws_learner/" class="btn btn-neutral float-right" title="Weight Sparsification">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../cp_learner/" class="btn btn-neutral" title="Channel Pruning"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../cp_learner/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../ws_learner/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../mathjax-config.js" defer></script>
      <script src="../MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../search/main.js" defer></script>

</body>
</html>
