<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Reinforcement Learning - PocketFlow Docs</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Reinforcement Learning";
    var mkdocs_page_input_path = "reinforcement_learning.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> PocketFlow Docs</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../installation/">Installation</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../tutorial/">Tutorial</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Learners - Algorithms</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../cp_learner/">Channel Pruning</a>
                </li>
                <li class="">
                    
    <a class="" href="../cpr_learner/">Channel Pruning - Remastered</a>
                </li>
                <li class="">
                    
    <a class="" href="../dcp_learner/">Discrimination-aware Channel Pruning</a>
                </li>
                <li class="">
                    
    <a class="" href="../ws_learner/">Weight Sparsification</a>
                </li>
                <li class="">
                    
    <a class="" href="../uq_learner/">Uniform Quantization</a>
                </li>
                <li class="">
                    
    <a class="" href="../nuq_learner/">Non-uniform Quantization</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Learners - Misc.</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../distillation/">Distillation</a>
                </li>
                <li class="">
                    
    <a class="" href="../multi_gpu_training/">Multi-GPU Training</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Hyper-parameter Optimizers</span>
    <ul class="subnav">
                <li class=" current">
                    
    <a class="current" href="./">Reinforcement Learning</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#reinforcement-learning">Reinforcement Learning</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#algorithm-description">Algorithm Description</a></li>
        
        </ul>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../automl_based_methods/">AutoML-based Methods</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../self_defined_models/">Self-defined Models</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../performance/">Performance</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../faq/">Frequently Asked Questions</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Appendix</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../pre_trained_models/">Pre-trained Models</a>
                </li>
                <li class="">
                    
    <a class="" href="../test_cases/">Test Cases</a>
                </li>
                <li class="">
                    
    <a class="" href="../reference/">Reference</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">PocketFlow Docs</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
        
          <li>Hyper-parameter Optimizers &raquo;</li>
        
      
    
    <li>Reinforcement Learning</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="reinforcement-learning">Reinforcement Learning</h1>
<p>For most deep learning models, the parameter redundancy differs from one layer to another.
Some layers may be more robust to model compression algorithms due to larger redundancy, while others may be more sensitive.
Therefore, it is often sub-optimal to use a unified pruning ratio or number of quantization bits for all layers, which completely omits the redundancy difference.
However, it is also time-consuming or even impractical to manually setup the optimal value of such hyper-parameter for each layer, especially for deep networks with tens or hundreds of layers.</p>
<p>To overcome this dilemma, in PocketFlow, we adopt reinforcement learning to automatically determine the optimal pruning ratio or number of quantization bits for each layer.
Our approach is innovated from (He et al., 2018), which automatically determines each layer's optimal pruning ratio, and generalize it to hyper-parameter optimization for more model compression methods.</p>
<p>In this documentation, we take <code>UniformQuantLearner</code> as an example to explain how the reinforcement learning method is used to iteratively optimize the number of quantization bits for each layer.
It is worthy mentioning that this feature is also available for <code>ChannelPrunedLearner</code>, <code>WeightSparseLearner</code>, and <code>NonUniformQuantLearner</code>.</p>
<h2 id="algorithm-description">Algorithm Description</h2>
<p>Here, we assume the original model to be compressed consists of <span><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span> layers, and denote the <span><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>-th layer's weight tensor as <span><span class="MathJax_Preview">\mathbf{W}_{t}</span><script type="math/tex">\mathbf{W}_{t}</script></span> and its quantization bit-width as <span><span class="MathJax_Preview">b_{t}</span><script type="math/tex">b_{t}</script></span>.
In order to maximally exploit the parameter redundancy of each layer, we need to find the optimal combination of layer-wise quantization bit-width that achieves the highest accuracy after compression while satisfying:</p>
<div>
<div class="MathJax_Preview">
\sum_{t = 1}^{T} b_{t} \left| \mathbf{W}_{t} \right| \le b \cdot \sum_{t = 1}^{T} \left| \mathbf{W}_{t} \right|
</div>
<script type="math/tex; mode=display">
\sum_{t = 1}^{T} b_{t} \left| \mathbf{W}_{t} \right| \le b \cdot \sum_{t = 1}^{T} \left| \mathbf{W}_{t} \right|
</script>
</div>
<p>where <span><span class="MathJax_Preview">\left| \mathbf{W}_{t} \right|</span><script type="math/tex">\left| \mathbf{W}_{t} \right|</script></span> denotes the number of parameters in the weight tensor <span><span class="MathJax_Preview">\mathbf{W}_{t}</span><script type="math/tex">\mathbf{W}_{t}</script></span> and <span><span class="MathJax_Preview">b</span><script type="math/tex">b</script></span> is the whole network's target quantization bit-width.</p>
<p>Below, we present the overall workflow of adopting reinforcement learning, or more specifically, the DDPG algorithm (Lillicrap et al., 2016) to search for the optimal combination of layer-wise quantization bit-width:</p>
<p><img alt="RL Workflow" src="../pics/rl_workflow.png" /></p>
<p>To start with, we initialize an DDPG agent and set the best reward <span><span class="MathJax_Preview">r_{best}</span><script type="math/tex">r_{best}</script></span> to negative infinity to track the optimal combination of layer-wise quantization bit-width.
The search process consists of multiple roll-outs.
In each roll-out, we sequentially traverse each layer in the network to determine its quantization bit-width.
For the <span><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>-th layer, we construct its state vector with following information:</p>
<ul>
<li>one-hot embedding of layer index</li>
<li>shape of weight tensor</li>
<li>number of parameters in the weight tensor</li>
<li>number of quantization bits used by previous layers</li>
<li>budget of quantization bits for remaining layers</li>
</ul>
<p>Afterwards, we feed this state vector into the DDPG agent to choose an action, which is then converted into the quantization bit-width under certain constraints.
A commonly-used constraint is that with the selected quantization bit-width, the budget of quantization bits for remaining layers should be sufficient, <em>e.g.</em> ensuring the minimal quantization bit-width can be satisfied.</p>
<p>After obtaining all layer's quantization bit-width, we quantize each layer's weights with the corresponding quantization bit-width, and fine-tune the quantized network for a few iteration (as supported by each learner's "Fast Fine-tuning" mode).
We then evaluate the fine-tuned network' accuracy and use it as the reward signal <span><span class="MathJax_Preview">r_{n}</span><script type="math/tex">r_{n}</script></span>.
The reward signal is compared against the best reward discovered so far, and the optimal combination of layer-wise quantization bit-width is updated if the current reward is larger.</p>
<p>Finally, we generate a list of transitions from all the <span><span class="MathJax_Preview">\left( \mathbf{s}_{t}, a_{t}, r_{t}, \mathbf{s}_{t + 1} \right)</span><script type="math/tex">\left( \mathbf{s}_{t}, a_{t}, r_{t}, \mathbf{s}_{t + 1} \right)</script></span> tuples in the roll-out, and store them in the DDPG agent's replay buffer.
The DDPG agent is then trained with one or more mini-batches of sampled transitions, so that it can choose better actions in the following roll-outs.</p>
<p>After obtaining the optimal combination of layer-wise quantization bit-width, we can optionally use <code>UniformQuantLearner</code>'s "Re-training with Full Data" mode (also supported by others learners) for a complete quantization-aware training to further reduce the accuracy loss.</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../automl_based_methods/" class="btn btn-neutral float-right" title="AutoML-based Methods">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../multi_gpu_training/" class="btn btn-neutral" title="Multi-GPU Training"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../multi_gpu_training/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../automl_based_methods/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../mathjax-config.js" defer></script>
      <script src="../MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../search/main.js" defer></script>

</body>
</html>
