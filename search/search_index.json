{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PocketFlow PocketFlow is an open-source framework for compressing and accelerating deep learning models with minimal human effort. Deep learning is widely used in various areas, such as computer vision, speech recognition, and natural language translation. However, deep learning models are often computational expensive, which limits further applications on mobile devices with limited computational resources. PocketFlow aims at providing an easy-to-use toolkit for developers to improve the inference efficiency with little or no performance degradation. Developers only needs to specify the desired compression and/or acceleration ratios and then PocketFlow will automatically choose proper hyper-parameters to generate a highly efficient compressed model for deployment. Framework The proposed framework mainly consists of two categories of algorithm components, i.e. learners and hyper-parameter optimizers, as depicted in the figure below. Given an uncompressed original model, the learner module generates a candidate compressed model using some randomly chosen hyper-parameter combination. The candidate model's accuracy and computation efficiency is then evaluated and used by hyper-parameter optimizer module as the feedback signal to determine the next hyper-parameter combination to be explored by the learner module. After a few iterations, the best one of all the candidate models is output as the final compressed model. Learners A learner refers to some model compression algorithm augmented with several training techniques as shown in the figure above. Below is a list of model compression algorithms supported in PocketFlow: Name Description ChannelPrunedLearner channel pruning with LASSO-based channel selection (He et al., 2017) DisChnPrunedLearner discrimination-aware channel pruning (Zhuang et al., 2018) WeightSparseLearner weight sparsification with dynamic pruning schedule (Zhu & Gupta, 2017) UniformQuantLearner weight quantization with uniform reconstruction levels (Jacob et al., 2018) UniformQuantTFLearner weight quantization with uniform reconstruction levels and TensorFlow APIs NonUniformQuantLearner weight quantization with non-uniform reconstruction levels (Han et al., 2016) All the above model compression algorithms can trained with fast fine-tuning, which is to directly derive a compressed model from the original one by applying either pruning masks or quantization functions. The resulting model can be fine-tuned with a few iterations to recover the accuracy to some extent. Alternatively, the compressed model can be re-trained with the full training data, which leads to higher accuracy but usually takes longer to complete. To further reduce the compressed model's performance degradation, we adopt network distillation to augment its training process with an extra loss term, using the original uncompressed model's outputs as soft labels. Additionally, multi-GPU distributed training is enabled for all learners to speed-up the time-consuming training process. Hyper-parameter Optimizers For model compression algorithms, there are several hyper-parameters that may have a large impact on the final compressed model's performance. It can be quite difficult to manually determine proper values for these hyper-parameters, especially for developers that are not very familiar with algorithm details. Recently, several AutoML systems, e.g. Cloud AutoML from Google, have been developed to train high-quality machine learning models with minimal human effort. Particularly, the AMC algorithm (He et al., 2018) presents promising results for adopting reinforcement learning for automated model compression with channel pruning and fine-grained pruning. In PocketFlow, we introduce the hyper-parameter optimizer module to iteratively search for the optimal hyper-parameter setting. We provide several implementations of hyper-parameter optimizer, based on models including Gaussian Processes (GP, Mockus, 1975), Tree-structured Parzen Estimator (TPE, Bergstra et al., 2013), and Deterministic Deep Policy Gradients (DDPG, Lillicrap et al., 2016). The hyper-parameter setting is optimized through an iterative process. In each iteration, the hyper-parameter optimizer chooses a combination of hyper-parameter values, and the learner generates a candidate model with fast fast-tuning. The candidate model is evaluated to calculate the reward of the current hyper-parameter setting. After that, the hyper-parameter optimizer updates its model to improve its estimation on the hyper-parameter space. Finally, when the best candidate model (and corresponding hyper-parameter setting) is selected after some iterations, this model can be re-trained with full data to further reduce the performance loss. Performance In this section, we present some of our results for applying various model compression methods for ResNet and MobileNet models on the ImageNet classification task, including channel pruning, weight sparsification, and uniform quantization. For complete evaluation results, please refer to here . Channel Pruning We adopt the DDPG algorithm as the RL agent to find the optimal layer-wise pruning ratios, and use group fine-tuning to further improve the compressed model's accuracy: Model FLOPs Uniform RL-based RL-based + Group Fine-tuning MobileNet-v1 50% 66.5% 67.8% (+1.3%) 67.9% (+1.4%) MobileNet-v1 40% 66.2% 66.9% (+0.7%) 67.0% (+0.8%) MobileNet-v1 30% 64.4% 64.5% (+0.1%) 64.8% (+0.4%) Mobilenet-v1 20% 61.4% 61.4% (+0.0%) 62.2% (+0.8%) Weight Sparsification Comparing with the original algorithm (Zhu & Gupta, 2017) which uses the same sparsity for all layers, we incorporate the DDPG algorithm to iteratively search for the optimal sparsity of each layer, which leads to the increased accuracy: Model Sparsity (Zhu & Gupta, 2017) RL-based MobileNet-v1 50% 69.5% 70.5% (+1.0%) MobileNet-v1 75% 67.7% 68.5% (+0.8%) MobileNet-v1 90% 61.8% 63.4% (+1.6%) MobileNet-v1 95% 53.6% 56.8% (+3.2%) Uniform Quantization We show that models with 32-bit floating-point number weights can be safely quantized into their 8-bit counterpart without accuracy loss (sometimes even better!). The resulting model can be deployed on mobile devices for faster inference (Device: XiaoMi 8 with a Snapdragon 845 CPU): Model Acc. (32-bit) Acc. (8-bit) Time (32-bit) Time (8-bit) MobileNet-v1 70.89% 71.29% (+0.40%) 124.53 56.12 (2.22 \\times \\times ) MobileNet-v2 71.84% 72.26% (+0.42%) 120.59 49.04 (2.46 \\times \\times ) All the reported time are in milliseconds.","title":"Home"},{"location":"#pocketflow","text":"PocketFlow is an open-source framework for compressing and accelerating deep learning models with minimal human effort. Deep learning is widely used in various areas, such as computer vision, speech recognition, and natural language translation. However, deep learning models are often computational expensive, which limits further applications on mobile devices with limited computational resources. PocketFlow aims at providing an easy-to-use toolkit for developers to improve the inference efficiency with little or no performance degradation. Developers only needs to specify the desired compression and/or acceleration ratios and then PocketFlow will automatically choose proper hyper-parameters to generate a highly efficient compressed model for deployment.","title":"PocketFlow"},{"location":"#framework","text":"The proposed framework mainly consists of two categories of algorithm components, i.e. learners and hyper-parameter optimizers, as depicted in the figure below. Given an uncompressed original model, the learner module generates a candidate compressed model using some randomly chosen hyper-parameter combination. The candidate model's accuracy and computation efficiency is then evaluated and used by hyper-parameter optimizer module as the feedback signal to determine the next hyper-parameter combination to be explored by the learner module. After a few iterations, the best one of all the candidate models is output as the final compressed model.","title":"Framework"},{"location":"#learners","text":"A learner refers to some model compression algorithm augmented with several training techniques as shown in the figure above. Below is a list of model compression algorithms supported in PocketFlow: Name Description ChannelPrunedLearner channel pruning with LASSO-based channel selection (He et al., 2017) DisChnPrunedLearner discrimination-aware channel pruning (Zhuang et al., 2018) WeightSparseLearner weight sparsification with dynamic pruning schedule (Zhu & Gupta, 2017) UniformQuantLearner weight quantization with uniform reconstruction levels (Jacob et al., 2018) UniformQuantTFLearner weight quantization with uniform reconstruction levels and TensorFlow APIs NonUniformQuantLearner weight quantization with non-uniform reconstruction levels (Han et al., 2016) All the above model compression algorithms can trained with fast fine-tuning, which is to directly derive a compressed model from the original one by applying either pruning masks or quantization functions. The resulting model can be fine-tuned with a few iterations to recover the accuracy to some extent. Alternatively, the compressed model can be re-trained with the full training data, which leads to higher accuracy but usually takes longer to complete. To further reduce the compressed model's performance degradation, we adopt network distillation to augment its training process with an extra loss term, using the original uncompressed model's outputs as soft labels. Additionally, multi-GPU distributed training is enabled for all learners to speed-up the time-consuming training process.","title":"Learners"},{"location":"#hyper-parameter-optimizers","text":"For model compression algorithms, there are several hyper-parameters that may have a large impact on the final compressed model's performance. It can be quite difficult to manually determine proper values for these hyper-parameters, especially for developers that are not very familiar with algorithm details. Recently, several AutoML systems, e.g. Cloud AutoML from Google, have been developed to train high-quality machine learning models with minimal human effort. Particularly, the AMC algorithm (He et al., 2018) presents promising results for adopting reinforcement learning for automated model compression with channel pruning and fine-grained pruning. In PocketFlow, we introduce the hyper-parameter optimizer module to iteratively search for the optimal hyper-parameter setting. We provide several implementations of hyper-parameter optimizer, based on models including Gaussian Processes (GP, Mockus, 1975), Tree-structured Parzen Estimator (TPE, Bergstra et al., 2013), and Deterministic Deep Policy Gradients (DDPG, Lillicrap et al., 2016). The hyper-parameter setting is optimized through an iterative process. In each iteration, the hyper-parameter optimizer chooses a combination of hyper-parameter values, and the learner generates a candidate model with fast fast-tuning. The candidate model is evaluated to calculate the reward of the current hyper-parameter setting. After that, the hyper-parameter optimizer updates its model to improve its estimation on the hyper-parameter space. Finally, when the best candidate model (and corresponding hyper-parameter setting) is selected after some iterations, this model can be re-trained with full data to further reduce the performance loss.","title":"Hyper-parameter Optimizers"},{"location":"#performance","text":"In this section, we present some of our results for applying various model compression methods for ResNet and MobileNet models on the ImageNet classification task, including channel pruning, weight sparsification, and uniform quantization. For complete evaluation results, please refer to here .","title":"Performance"},{"location":"#channel-pruning","text":"We adopt the DDPG algorithm as the RL agent to find the optimal layer-wise pruning ratios, and use group fine-tuning to further improve the compressed model's accuracy: Model FLOPs Uniform RL-based RL-based + Group Fine-tuning MobileNet-v1 50% 66.5% 67.8% (+1.3%) 67.9% (+1.4%) MobileNet-v1 40% 66.2% 66.9% (+0.7%) 67.0% (+0.8%) MobileNet-v1 30% 64.4% 64.5% (+0.1%) 64.8% (+0.4%) Mobilenet-v1 20% 61.4% 61.4% (+0.0%) 62.2% (+0.8%)","title":"Channel Pruning"},{"location":"#weight-sparsification","text":"Comparing with the original algorithm (Zhu & Gupta, 2017) which uses the same sparsity for all layers, we incorporate the DDPG algorithm to iteratively search for the optimal sparsity of each layer, which leads to the increased accuracy: Model Sparsity (Zhu & Gupta, 2017) RL-based MobileNet-v1 50% 69.5% 70.5% (+1.0%) MobileNet-v1 75% 67.7% 68.5% (+0.8%) MobileNet-v1 90% 61.8% 63.4% (+1.6%) MobileNet-v1 95% 53.6% 56.8% (+3.2%)","title":"Weight Sparsification"},{"location":"#uniform-quantization","text":"We show that models with 32-bit floating-point number weights can be safely quantized into their 8-bit counterpart without accuracy loss (sometimes even better!). The resulting model can be deployed on mobile devices for faster inference (Device: XiaoMi 8 with a Snapdragon 845 CPU): Model Acc. (32-bit) Acc. (8-bit) Time (32-bit) Time (8-bit) MobileNet-v1 70.89% 71.29% (+0.40%) 124.53 56.12 (2.22 \\times \\times ) MobileNet-v2 71.84% 72.26% (+0.42%) 120.59 49.04 (2.46 \\times \\times ) All the reported time are in milliseconds.","title":"Uniform Quantization"},{"location":"automl_based_methods/","text":"AutoML-based Methods Under construction ...","title":"AutoML-based Methods"},{"location":"automl_based_methods/#automl-based-methods","text":"Under construction ...","title":"AutoML-based Methods"},{"location":"cp_learner/","text":"Channel Pruning Introduction Channel pruning is a kind of structural model compression approach which can not only compress the model size, but accelerate the inference speed directly. PocketFlow uses the channel pruning algorithm proposed in (He et al., 2017) to pruning each channel of convolution layers with a certain ratio, and for details please refer to the channel pruning paper . For better performance and more robust, we modify some parts of the algorithm to achieve better result. In order to achieve a better performance, PocketFlow can take advantages of reinforcement learning to search a better compression ratio (He et al., 2018). User can also use the distilling (Hinton et al., 2015) and group tuning function to improve the accuracy after compression. Group tuning means setting a certain number of layers as group and then pruning and fine-tuning (or re-training) each group sequentially. For example, we can set each 3 layers as a group and then prune the first 3 layers. After that, we fine-tune (or re-train) the whole model and prune the next 3 layers and so on. Distilling and group tuning are experimentally proved as effective approaches to achieve higher accuracy at a certain compression ratio in most situations. Pruning Option The code of channel pruning are located at directory ./learners/channel_pruning . To use channel pruning. users can set --learners to channel . The Channel pruning supports 3 kinds of pruning setup by cp_prune_option option. Uniform Channel Pruning One is the uniform layer pruning, which means the user can set each convolution layer pruned with an uniform pruning ratio by --cp_prune_option=uniform and set the ratio (eg. making the ratio 0.5) by --cp_uniform_preserve_ratio=0.5 . Note that for a layer, if both of pruning ratio of the layer and its previous layer are 0.5, the real preserved FLOPs are 1/4 of original FLOPs. Because channel pruning only prune the c_out channels of the convolution and c_in channels of the next convolution, if both c_in and c_out channels are pruned by 0.5, it will preserve only 1/4 of original computation cost. For a layer by layer convolution networks without residual blocks, if the user set cp_uniform_preserve_ratio to 0.5 , the whole model will be the 0.25 computation of the original model. However for the residual networks, some convolutions can only prune their c_in or c_out channels, which means the total preserved computation ratio may be much greater than 0.25. Example: $ ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --learner channel \\ --batch_size_eval 64 \\ --cp_uniform_preserve_ratio 0.5 \\ --cp_prune_option uniform \\ --resnet_size 20 List Channel Pruning Another pruning option is pruning the corresponding layer with ratios listed in a named ratio.list file, the file name of which can be set by --cp_prune_list_file option. the ratio value must be separated by a comma. User can set --cp_prune_option=list to prune the model by list ratios. Example: Add list 1.0, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 1.0, 0.25, 1.0, 0.25, 0.21875, 0.21875, 0.21875, 1.0, 0.5625, 1.0, 0.546875, 0.546875, 0.546875, 1 in ./ratio.list $ ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --learner channel \\ --batch_size_eval 64 \\ --cp_prune_option list \\ --cp_prune_list_file ./ratio.list \\ --resnet_size 20 Automatic Channel Pruning The last one pruning option is searching better pruning ratios by reinforcement learning and you only need to give a value which represents what the ratio of total FLOPs/Computation you wants the compressed model preserve. You can set --cp_prune_option=auto and set a preserve ratio number such as --cp_preserve_ratio=0.5 . User can also use cp_nb_rlouts_min to control reinforcement learning warm up iterations, which means the RL agent start to learn after the iterations, the default value is 50 . User can also use cp_nb_rlouts_min to control the total iteration RL agent to search, the default value is 200 . If the user want to control other parameters of the agents, please refer to the reinforcement component page. Example: $ ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --learner channel \\ --batch_size_eval 64 \\ --cp_preserve_ratio 0.5 \\ --cp_prune_option auto \\ --resnet_size 20 Channel pruning parameters The implementation of the channel pruning use Lasso algorithm to do channel selection and linear regression to do feature map reconstruction. During these two phases, sampling is done on the feature map to reduce computation cost. The users can use --cp_nb_points_per_layer to set how many sampling points on each layer are taken, the default value is 10 . For some dataset, if the images contain too many zero pixels (eg. black color), the value should be greater. The users can also set using how many batches to do channel selection and feature reconstruction by cp_nb_batches , the default value is 60 . Small value of cp_nb_batches may cause over-fitting and large value may slow down the solving speed, so a good value depends on the nets and dataset. For more practical usage, user may consider make the channel number of each layer is the quadruple for fast inference of mobile devices. In this case, user can set --cp_quadruple to True to make the compressed model have a quadruple number of channels. Distilling Distilling is an effective approach to improve the final accuracy of compressed model with PocketFlow in most situations of classification. User can set --enbl_dst=True to enable distilling. Group Tuning As introduced above, group tuning was proposed by the PocketFlow team and finding it is very useful to improve the performance of model compression. In PocketFlow, users can set --cp_finetune=True to enable group fine-tuning and set the group number by --cp_list_group , the default value is 1000 . There is a trade-off between the small value and large value, because if the value is 1 , PocketFlow will prune convolution and fine-tune/re-train by each layer, which may have better effect but be more time-consuming. If we set the value large, the function will be less effective. User can also set the number of iterations to fine-tune by setting cp_nb_iters_ft_ratio which mean the ratio the total iterations to be used in fine-tuning. The learning rate of fine-tuning can be set by cp_lrn_rate_ft .","title":"Channel Pruning"},{"location":"cp_learner/#channel-pruning","text":"","title":"Channel Pruning"},{"location":"cp_learner/#introduction","text":"Channel pruning is a kind of structural model compression approach which can not only compress the model size, but accelerate the inference speed directly. PocketFlow uses the channel pruning algorithm proposed in (He et al., 2017) to pruning each channel of convolution layers with a certain ratio, and for details please refer to the channel pruning paper . For better performance and more robust, we modify some parts of the algorithm to achieve better result. In order to achieve a better performance, PocketFlow can take advantages of reinforcement learning to search a better compression ratio (He et al., 2018). User can also use the distilling (Hinton et al., 2015) and group tuning function to improve the accuracy after compression. Group tuning means setting a certain number of layers as group and then pruning and fine-tuning (or re-training) each group sequentially. For example, we can set each 3 layers as a group and then prune the first 3 layers. After that, we fine-tune (or re-train) the whole model and prune the next 3 layers and so on. Distilling and group tuning are experimentally proved as effective approaches to achieve higher accuracy at a certain compression ratio in most situations.","title":"Introduction"},{"location":"cp_learner/#pruning-option","text":"The code of channel pruning are located at directory ./learners/channel_pruning . To use channel pruning. users can set --learners to channel . The Channel pruning supports 3 kinds of pruning setup by cp_prune_option option.","title":"Pruning Option"},{"location":"cp_learner/#uniform-channel-pruning","text":"One is the uniform layer pruning, which means the user can set each convolution layer pruned with an uniform pruning ratio by --cp_prune_option=uniform and set the ratio (eg. making the ratio 0.5) by --cp_uniform_preserve_ratio=0.5 . Note that for a layer, if both of pruning ratio of the layer and its previous layer are 0.5, the real preserved FLOPs are 1/4 of original FLOPs. Because channel pruning only prune the c_out channels of the convolution and c_in channels of the next convolution, if both c_in and c_out channels are pruned by 0.5, it will preserve only 1/4 of original computation cost. For a layer by layer convolution networks without residual blocks, if the user set cp_uniform_preserve_ratio to 0.5 , the whole model will be the 0.25 computation of the original model. However for the residual networks, some convolutions can only prune their c_in or c_out channels, which means the total preserved computation ratio may be much greater than 0.25. Example: $ ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --learner channel \\ --batch_size_eval 64 \\ --cp_uniform_preserve_ratio 0.5 \\ --cp_prune_option uniform \\ --resnet_size 20","title":"Uniform Channel Pruning"},{"location":"cp_learner/#list-channel-pruning","text":"Another pruning option is pruning the corresponding layer with ratios listed in a named ratio.list file, the file name of which can be set by --cp_prune_list_file option. the ratio value must be separated by a comma. User can set --cp_prune_option=list to prune the model by list ratios. Example: Add list 1.0, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 1.0, 0.25, 1.0, 0.25, 0.21875, 0.21875, 0.21875, 1.0, 0.5625, 1.0, 0.546875, 0.546875, 0.546875, 1 in ./ratio.list $ ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --learner channel \\ --batch_size_eval 64 \\ --cp_prune_option list \\ --cp_prune_list_file ./ratio.list \\ --resnet_size 20","title":"List Channel Pruning"},{"location":"cp_learner/#automatic-channel-pruning","text":"The last one pruning option is searching better pruning ratios by reinforcement learning and you only need to give a value which represents what the ratio of total FLOPs/Computation you wants the compressed model preserve. You can set --cp_prune_option=auto and set a preserve ratio number such as --cp_preserve_ratio=0.5 . User can also use cp_nb_rlouts_min to control reinforcement learning warm up iterations, which means the RL agent start to learn after the iterations, the default value is 50 . User can also use cp_nb_rlouts_min to control the total iteration RL agent to search, the default value is 200 . If the user want to control other parameters of the agents, please refer to the reinforcement component page. Example: $ ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --learner channel \\ --batch_size_eval 64 \\ --cp_preserve_ratio 0.5 \\ --cp_prune_option auto \\ --resnet_size 20","title":"Automatic Channel Pruning"},{"location":"cp_learner/#channel-pruning-parameters","text":"The implementation of the channel pruning use Lasso algorithm to do channel selection and linear regression to do feature map reconstruction. During these two phases, sampling is done on the feature map to reduce computation cost. The users can use --cp_nb_points_per_layer to set how many sampling points on each layer are taken, the default value is 10 . For some dataset, if the images contain too many zero pixels (eg. black color), the value should be greater. The users can also set using how many batches to do channel selection and feature reconstruction by cp_nb_batches , the default value is 60 . Small value of cp_nb_batches may cause over-fitting and large value may slow down the solving speed, so a good value depends on the nets and dataset. For more practical usage, user may consider make the channel number of each layer is the quadruple for fast inference of mobile devices. In this case, user can set --cp_quadruple to True to make the compressed model have a quadruple number of channels.","title":"Channel pruning parameters"},{"location":"cp_learner/#distilling","text":"Distilling is an effective approach to improve the final accuracy of compressed model with PocketFlow in most situations of classification. User can set --enbl_dst=True to enable distilling.","title":"Distilling"},{"location":"cp_learner/#group-tuning","text":"As introduced above, group tuning was proposed by the PocketFlow team and finding it is very useful to improve the performance of model compression. In PocketFlow, users can set --cp_finetune=True to enable group fine-tuning and set the group number by --cp_list_group , the default value is 1000 . There is a trade-off between the small value and large value, because if the value is 1 , PocketFlow will prune convolution and fine-tune/re-train by each layer, which may have better effect but be more time-consuming. If we set the value large, the function will be less effective. User can also set the number of iterations to fine-tune by setting cp_nb_iters_ft_ratio which mean the ratio the total iterations to be used in fine-tuning. The learning rate of fine-tuning can be set by cp_lrn_rate_ft .","title":"Group Tuning"},{"location":"cpr_learner/","text":"Channel Pruning - Remastered Introduction Channel pruning (He et al., 2017) aims at reducing the number of input channels of each convolutional layer while minimizing the reconstruction loss of its output feature maps, using preserved input channels only. Similar to other model compression components based on channel pruning, this can lead to direct reduction in both model size and computational complexity (in terms of FLOPs). In PocketFlow, we provide ChannelPrunedRmtLearner as the remastered version of the previous ChannelPrunedLearner , with simplified and easier-to-understand implementation. The underlying algorithm is based on (He et al., 2017), with a few modifications. However, the support for RL-based hyper-parameter optimization is not yet ready and will be provided in the near future. Algorithm Description For a convolutional layer, we denote its input feature map as \\mathcal{X} \\in \\mathbb{R}^{N \\times h_{i} \\times w_{i} \\times c_{i}} \\mathcal{X} \\in \\mathbb{R}^{N \\times h_{i} \\times w_{i} \\times c_{i}} , where N N is the batch size, h_{i} h_{i} and w_{i} w_{i} are the spatial height and width, and c_{i} c_{i} is the number of inputs channels. The convolutional kernel is denoted as \\mathcal{W} \\in \\mathbb{R}^{k_{h} \\times k_{w} \\times c_{i} \\times c_{o}} \\mathcal{W} \\in \\mathbb{R}^{k_{h} \\times k_{w} \\times c_{i} \\times c_{o}} , where \\left( k_{h}, k_{w} \\right) \\left( k_{h}, k_{w} \\right) is the kernel's spatial size and c_{o} c_{o} is the number of output channels. The resulting output feature map is given by \\mathcal{Y} = f \\left( \\mathcal{X}; \\mathcal{W} \\right) \\in \\mathbb{R}^{N \\times h_{o} \\times w_{o} \\times c_{o}} \\mathcal{Y} = f \\left( \\mathcal{X}; \\mathcal{W} \\right) \\in \\mathbb{R}^{N \\times h_{o} \\times w_{o} \\times c_{o}} , where h_{o} h_{o} and w_{o} w_{o} are the spatial height and width, and f \\left( \\cdot \\right) f \\left( \\cdot \\right) denotes the convolutional operation. The convolutional operation can be understood as standard matrix multiplication between two matrices, one from \\mathcal{X} \\mathcal{X} and the other from \\mathcal{W} \\mathcal{W} . The input feature map \\mathcal{X} \\mathcal{X} is re-arranged via the im2col operator to produce a matrix \\mathbf{X} \\mathbf{X} of size N h_{o} w_{o} \\times h_{k} w_{k} c_{i} N h_{o} w_{o} \\times h_{k} w_{k} c_{i} . The convolutional kernel \\mathcal{W} \\mathcal{W} is correspondingly reshaped into \\mathbf{W} \\mathbf{W} of size h_{k} w_{k} c_{i} \\times c_{o} h_{k} w_{k} c_{i} \\times c_{o} . The multiplication of these two matrices produces the output feature map in the matrix form, given by \\mathbf{Y} = \\mathbf{X} \\mathbf{W} \\mathbf{Y} = \\mathbf{X} \\mathbf{W} , which can be further reshaped back to the 4-D tensor \\mathcal{Y} \\mathcal{Y} . The matrix multiplication can be decomposed along the dimension of input channels. We divide \\mathbf{X} \\mathbf{X} into c_{i} c_{i} sub-matrices \\left\\{ \\mathbf{X}_{i} \\right\\} \\left\\{ \\mathbf{X}_{i} \\right\\} , each of size N h_{o} w_{o} \\times h_{k} w_{k} N h_{o} w_{o} \\times h_{k} w_{k} , and similarly divide \\mathbf{W} \\mathbf{W} into c_{i} c_{i} sub-matrices \\left\\{ \\mathbf{W}_{i} \\right\\} \\left\\{ \\mathbf{W}_{i} \\right\\} , each of size h_{k} w_{k} c_{i} \\times c_{o} h_{k} w_{k} c_{i} \\times c_{o} . The computation of output feature map \\mathbf{Y} \\mathbf{Y} can be rewritten as: \\mathbf{Y} = \\sum\\nolimits_{i = 1}^{c_{i}} \\mathbf{X}_{i} \\mathbf{W}_{i} \\mathbf{Y} = \\sum\\nolimits_{i = 1}^{c_{i}} \\mathbf{X}_{i} \\mathbf{W}_{i} In (He et al., 2017), a c_{i} c_{i} -dimensional binary-valued mask vector \\boldsymbol{\\beta} \\boldsymbol{\\beta} is introduced to indicate whether an input channel is pruned ( \\beta_{i} = 0 \\beta_{i} = 0 ) or not ( \\beta_{i} = 1 \\beta_{i} = 1 ). More formally, we consider the minimization of output feature map's reconstruction loss under sparsity constraint: \\min_{\\mathbf{W}, \\boldsymbol{\\beta}} \\left\\| \\mathbf{Y} - \\sum\\nolimits_{i = 1}^{c_{i}} \\beta_{i} \\mathbf{X}_{i} \\mathbf{W}_{i} \\right\\|_{F}^{2}, ~ \\text{s.t.} ~ \\left\\| \\boldsymbol{\\beta} \\right\\|_{0} \\le c'_{i} \\min_{\\mathbf{W}, \\boldsymbol{\\beta}} \\left\\| \\mathbf{Y} - \\sum\\nolimits_{i = 1}^{c_{i}} \\beta_{i} \\mathbf{X}_{i} \\mathbf{W}_{i} \\right\\|_{F}^{2}, ~ \\text{s.t.} ~ \\left\\| \\boldsymbol{\\beta} \\right\\|_{0} \\le c'_{i} The above problem can be tackled by firstly solving \\boldsymbol{\\beta} \\boldsymbol{\\beta} via a LASSO regression problem, and then updating \\mathbf{W} \\mathbf{W} with the closed-form solution (or iterative solution) to least-square regression. Particularly, in the first step, we rewrite the sparsity constraint as a l_{1} l_{1} -regularization term, so the optimization over \\boldsymbol{\\beta} \\boldsymbol{\\beta} is now given by: \\min_{\\boldsymbol{\\beta}} \\left\\| \\mathbf{Y} - \\sum\\nolimits_{i = 1}^{c_{i}} \\beta_{i} \\mathbf{X}_{i} \\mathbf{W}_{i} \\right\\|_{F}^{2} + \\lambda \\left\\| \\boldsymbol{\\beta} \\right\\|_{1} \\min_{\\boldsymbol{\\beta}} \\left\\| \\mathbf{Y} - \\sum\\nolimits_{i = 1}^{c_{i}} \\beta_{i} \\mathbf{X}_{i} \\mathbf{W}_{i} \\right\\|_{F}^{2} + \\lambda \\left\\| \\boldsymbol{\\beta} \\right\\|_{1} The coefficient of l_{1} l_{1} -regularization, \\lambda \\lambda , is determined via binary search so that the resulting solution \\boldsymbol{\\beta}^{*} \\boldsymbol{\\beta}^{*} has exactly c_{i} c_{i} non-zero entries. We solve the above unconstrained problem with the Iterative Shrinkage Thresholding Algorithm (ISTA). Hyper-parameters Below is the full list of hyper-parameters used in ChannelPrunedRmtLearner : Name Description cpr_save_path model's save path cpr_save_path_eval model's save path for evaluation cpr_save_path_ws model's save path for warm-start cpr_prune_ratio target pruning ratio cpr_skip_frst_layer skip the first convolutional layer for channel pruning cpr_skip_last_layer skip the last convolutional layer for channel pruning cpr_skip_op_names comma-separated Conv2D operations names to be skipped cpr_nb_smpls number of cached training samples for channel pruning cpr_nb_crops_per_smpl number of random crops per sample cpr_ista_lrn_rate ISTA's learning rate cpr_ista_nb_iters number of iterations in ISTA cpr_lstsq_lrn_rate least-square regression's learning rate cpr_lstsq_nb_iters number of iterations in least-square regression cpr_warm_start use a channel-pruned model for warm start Here, we provide detailed description (and some analysis) for above hyper-parameters: cpr_save_path : save path for model created in the training graph. The resulting checkpoint files can be used to resume training from a previous run and compute model's loss function's value and some other evaluation metrics. cpr_save_path_eval : save path for model created in the evaluation graph. The resulting checkpoint files can be used to export GraphDef & TensorFlow Lite model files. cpr_save_path_ws : save path for model used for warm-start. This learner supports loading a previously-saved channel-pruned model, so that no need to perform channel selection again. This is only used when cpr_warm_start is True . cpr_prune_ratio : target pruning ratio for input channels of each convolutional layer. The larger cpr_prune_ratio is, the more input channels will be pruned. If cpr_prune_ratio equals 0, then no input channels will be pruned and model remains the same; if cpr_prune_ratio equals 1, then all input channels will be pruned. cpr_skip_frst_layer : whether to skip the first convolutional layer for channel pruning. The first convolutional layer may be directly related to input images and pruning its input channel may harm the performance significantly. cpr_skip_last_layer : whether to skip the last convolutional layer for channel pruning. The first convolutional layer may be directly related to final outputs and pruning its input channel may harm the performance significantly. cpr_skip_op_names : comma-separated Conv2D operations names to be skipped. For instance, if cpr_skip_op_names is set to \"aaa,bbb\", then any Conv2D operation whose name contains either \"aaa\" or \"bbb\" will be skipped and no channel pruning will be applied on it. cpr_nb_smpls : number of cached training samples for channel pruning. Increasing this may lead to smaller performance degradation after channel pruning but also require more training time. cpr_nb_crops_per_smpl : number of random crops per sample. Increasing this may lead to smaller performance degradation after channel pruning but also require more training time. cpr_ista_lrn_rate : ISTA's learning rate for LASSO regression. If cpr_ista_lrn_rate is too large, then the optimization process may become unstable; if cpr_ista_lrn_rate is too small, then the optimization process may require lots of iterations until convergence. cpr_ista_nb_iters : number of iterations for LASSO regression. cpr_lstsq_lrn_rate : Adam's learning rate for least-square regression. If cpr_lstsq_lrn_rate is too large, then the optimization process may become unstable; if cpr_lstsq_lrn_rate is too small, then the optimization process may require lots of iterations until convergence. cpr_lstsq_nb_iters : number of iterations for least-square regression. cpr_warm_start : whether to use a previously-saved channel-pruned model for warm-start. Empirical Evaluation In this section, we present some of our results for applying ChannelPrunedRmtLearner for compression image classification and object detection models. For image classification, we use ChannelPrunedRmtLearner to compress the ResNet-18 model on the ILSVRC-12 dataset: Model Prune Ratio FLOPs Distillation? Top-1 Acc. Top-5 Acc. ResNet-18 0.2 73.32% No 69.43% 88.97% ResNet-18 0.2 73.32% Yes 68.78% 88.71% ResNet-18 0.3 61.31% No 68.44% 88.30% ResNet-18 0.3 61.31% Yes 68.85% 88.53% ResNet-18 0.4 50.70% No 67.17% 87.48% ResNet-18 0.4 50.70% Yes 67.35% 87.83% ResNet-18 0.5 41.27% No 65.73% 86.38% ResNet-18 0.5 41.27% Yes 65.98% 86.98% ResNet-18 0.6 32.07% No 63.38% 84.62% ResNet-18 0.6 32.07% Yes 63.65% 85.47% ResNet-18 0.7 24.28% No 60.26% 82.70% ResNet-18 0.7 24.28% Yes 60.43% 82.96% For object detection, we use ChannelPrunedRmtLearner to compress the SSD-VGG16 model on the Pascal VOC 07-12 dataset: Model Prune Ratio FLOPs Pruned Layers mAP ResNet-18 0.2 67.34% Backbone 77.53% ResNet-18 0.2 66.50% All 77.22% ResNet-18 0.3 53.58% Backbone 76.94% ResNet-18 0.3 52.32% All 76.90% ResNet-18 0.4 41.63% Backbone 75.81% ResNet-18 0.4 39.96% All 75.80% ResNet-18 0.5 31.56% Backbone 74.42% ResNet-18 0.5 29.47% All 73.76% Usage Examples In this section, we provide some usage examples to demonstrate how to use ChannelPrunedRmtLearner under different execution modes and hyper-parameter combinations: To compress a ResNet-20 model for CIFAR-10 classification task in the local mode, use: # set the target pruning ratio to 0.50 ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner=chn-pruned-rmt \\ --cpr_prune_ratio=0.50 To compress a ResNet-18 model for ILSVRC-12 classification task in the docker mode with 4 GPUs, use: # do no apply channel pruning to the last convolutional layer ./scripts/run_docker.sh nets/resnet_at_ilsvrc12_run.py -n=4 \\ --learner=chn-pruned-rmt \\ --cpr_skip_last_layer=True To compress a MobileNet-v1 model for ILSVRC-12 classification task in the seven mode with 8 GPUs, use: # use a channel-pruned model for warm-start, so no channel selection is needed ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py -n=8 \\ --learner=chn-pruned-rmt \\ --cpr_warm_start=True \\ --cpr_save_path_ws=./models_cpr_ws/model.ckpt","title":"Channel Pruning - Remastered"},{"location":"cpr_learner/#channel-pruning-remastered","text":"","title":"Channel Pruning - Remastered"},{"location":"cpr_learner/#introduction","text":"Channel pruning (He et al., 2017) aims at reducing the number of input channels of each convolutional layer while minimizing the reconstruction loss of its output feature maps, using preserved input channels only. Similar to other model compression components based on channel pruning, this can lead to direct reduction in both model size and computational complexity (in terms of FLOPs). In PocketFlow, we provide ChannelPrunedRmtLearner as the remastered version of the previous ChannelPrunedLearner , with simplified and easier-to-understand implementation. The underlying algorithm is based on (He et al., 2017), with a few modifications. However, the support for RL-based hyper-parameter optimization is not yet ready and will be provided in the near future.","title":"Introduction"},{"location":"cpr_learner/#algorithm-description","text":"For a convolutional layer, we denote its input feature map as \\mathcal{X} \\in \\mathbb{R}^{N \\times h_{i} \\times w_{i} \\times c_{i}} \\mathcal{X} \\in \\mathbb{R}^{N \\times h_{i} \\times w_{i} \\times c_{i}} , where N N is the batch size, h_{i} h_{i} and w_{i} w_{i} are the spatial height and width, and c_{i} c_{i} is the number of inputs channels. The convolutional kernel is denoted as \\mathcal{W} \\in \\mathbb{R}^{k_{h} \\times k_{w} \\times c_{i} \\times c_{o}} \\mathcal{W} \\in \\mathbb{R}^{k_{h} \\times k_{w} \\times c_{i} \\times c_{o}} , where \\left( k_{h}, k_{w} \\right) \\left( k_{h}, k_{w} \\right) is the kernel's spatial size and c_{o} c_{o} is the number of output channels. The resulting output feature map is given by \\mathcal{Y} = f \\left( \\mathcal{X}; \\mathcal{W} \\right) \\in \\mathbb{R}^{N \\times h_{o} \\times w_{o} \\times c_{o}} \\mathcal{Y} = f \\left( \\mathcal{X}; \\mathcal{W} \\right) \\in \\mathbb{R}^{N \\times h_{o} \\times w_{o} \\times c_{o}} , where h_{o} h_{o} and w_{o} w_{o} are the spatial height and width, and f \\left( \\cdot \\right) f \\left( \\cdot \\right) denotes the convolutional operation. The convolutional operation can be understood as standard matrix multiplication between two matrices, one from \\mathcal{X} \\mathcal{X} and the other from \\mathcal{W} \\mathcal{W} . The input feature map \\mathcal{X} \\mathcal{X} is re-arranged via the im2col operator to produce a matrix \\mathbf{X} \\mathbf{X} of size N h_{o} w_{o} \\times h_{k} w_{k} c_{i} N h_{o} w_{o} \\times h_{k} w_{k} c_{i} . The convolutional kernel \\mathcal{W} \\mathcal{W} is correspondingly reshaped into \\mathbf{W} \\mathbf{W} of size h_{k} w_{k} c_{i} \\times c_{o} h_{k} w_{k} c_{i} \\times c_{o} . The multiplication of these two matrices produces the output feature map in the matrix form, given by \\mathbf{Y} = \\mathbf{X} \\mathbf{W} \\mathbf{Y} = \\mathbf{X} \\mathbf{W} , which can be further reshaped back to the 4-D tensor \\mathcal{Y} \\mathcal{Y} . The matrix multiplication can be decomposed along the dimension of input channels. We divide \\mathbf{X} \\mathbf{X} into c_{i} c_{i} sub-matrices \\left\\{ \\mathbf{X}_{i} \\right\\} \\left\\{ \\mathbf{X}_{i} \\right\\} , each of size N h_{o} w_{o} \\times h_{k} w_{k} N h_{o} w_{o} \\times h_{k} w_{k} , and similarly divide \\mathbf{W} \\mathbf{W} into c_{i} c_{i} sub-matrices \\left\\{ \\mathbf{W}_{i} \\right\\} \\left\\{ \\mathbf{W}_{i} \\right\\} , each of size h_{k} w_{k} c_{i} \\times c_{o} h_{k} w_{k} c_{i} \\times c_{o} . The computation of output feature map \\mathbf{Y} \\mathbf{Y} can be rewritten as: \\mathbf{Y} = \\sum\\nolimits_{i = 1}^{c_{i}} \\mathbf{X}_{i} \\mathbf{W}_{i} \\mathbf{Y} = \\sum\\nolimits_{i = 1}^{c_{i}} \\mathbf{X}_{i} \\mathbf{W}_{i} In (He et al., 2017), a c_{i} c_{i} -dimensional binary-valued mask vector \\boldsymbol{\\beta} \\boldsymbol{\\beta} is introduced to indicate whether an input channel is pruned ( \\beta_{i} = 0 \\beta_{i} = 0 ) or not ( \\beta_{i} = 1 \\beta_{i} = 1 ). More formally, we consider the minimization of output feature map's reconstruction loss under sparsity constraint: \\min_{\\mathbf{W}, \\boldsymbol{\\beta}} \\left\\| \\mathbf{Y} - \\sum\\nolimits_{i = 1}^{c_{i}} \\beta_{i} \\mathbf{X}_{i} \\mathbf{W}_{i} \\right\\|_{F}^{2}, ~ \\text{s.t.} ~ \\left\\| \\boldsymbol{\\beta} \\right\\|_{0} \\le c'_{i} \\min_{\\mathbf{W}, \\boldsymbol{\\beta}} \\left\\| \\mathbf{Y} - \\sum\\nolimits_{i = 1}^{c_{i}} \\beta_{i} \\mathbf{X}_{i} \\mathbf{W}_{i} \\right\\|_{F}^{2}, ~ \\text{s.t.} ~ \\left\\| \\boldsymbol{\\beta} \\right\\|_{0} \\le c'_{i} The above problem can be tackled by firstly solving \\boldsymbol{\\beta} \\boldsymbol{\\beta} via a LASSO regression problem, and then updating \\mathbf{W} \\mathbf{W} with the closed-form solution (or iterative solution) to least-square regression. Particularly, in the first step, we rewrite the sparsity constraint as a l_{1} l_{1} -regularization term, so the optimization over \\boldsymbol{\\beta} \\boldsymbol{\\beta} is now given by: \\min_{\\boldsymbol{\\beta}} \\left\\| \\mathbf{Y} - \\sum\\nolimits_{i = 1}^{c_{i}} \\beta_{i} \\mathbf{X}_{i} \\mathbf{W}_{i} \\right\\|_{F}^{2} + \\lambda \\left\\| \\boldsymbol{\\beta} \\right\\|_{1} \\min_{\\boldsymbol{\\beta}} \\left\\| \\mathbf{Y} - \\sum\\nolimits_{i = 1}^{c_{i}} \\beta_{i} \\mathbf{X}_{i} \\mathbf{W}_{i} \\right\\|_{F}^{2} + \\lambda \\left\\| \\boldsymbol{\\beta} \\right\\|_{1} The coefficient of l_{1} l_{1} -regularization, \\lambda \\lambda , is determined via binary search so that the resulting solution \\boldsymbol{\\beta}^{*} \\boldsymbol{\\beta}^{*} has exactly c_{i} c_{i} non-zero entries. We solve the above unconstrained problem with the Iterative Shrinkage Thresholding Algorithm (ISTA).","title":"Algorithm Description"},{"location":"cpr_learner/#hyper-parameters","text":"Below is the full list of hyper-parameters used in ChannelPrunedRmtLearner : Name Description cpr_save_path model's save path cpr_save_path_eval model's save path for evaluation cpr_save_path_ws model's save path for warm-start cpr_prune_ratio target pruning ratio cpr_skip_frst_layer skip the first convolutional layer for channel pruning cpr_skip_last_layer skip the last convolutional layer for channel pruning cpr_skip_op_names comma-separated Conv2D operations names to be skipped cpr_nb_smpls number of cached training samples for channel pruning cpr_nb_crops_per_smpl number of random crops per sample cpr_ista_lrn_rate ISTA's learning rate cpr_ista_nb_iters number of iterations in ISTA cpr_lstsq_lrn_rate least-square regression's learning rate cpr_lstsq_nb_iters number of iterations in least-square regression cpr_warm_start use a channel-pruned model for warm start Here, we provide detailed description (and some analysis) for above hyper-parameters: cpr_save_path : save path for model created in the training graph. The resulting checkpoint files can be used to resume training from a previous run and compute model's loss function's value and some other evaluation metrics. cpr_save_path_eval : save path for model created in the evaluation graph. The resulting checkpoint files can be used to export GraphDef & TensorFlow Lite model files. cpr_save_path_ws : save path for model used for warm-start. This learner supports loading a previously-saved channel-pruned model, so that no need to perform channel selection again. This is only used when cpr_warm_start is True . cpr_prune_ratio : target pruning ratio for input channels of each convolutional layer. The larger cpr_prune_ratio is, the more input channels will be pruned. If cpr_prune_ratio equals 0, then no input channels will be pruned and model remains the same; if cpr_prune_ratio equals 1, then all input channels will be pruned. cpr_skip_frst_layer : whether to skip the first convolutional layer for channel pruning. The first convolutional layer may be directly related to input images and pruning its input channel may harm the performance significantly. cpr_skip_last_layer : whether to skip the last convolutional layer for channel pruning. The first convolutional layer may be directly related to final outputs and pruning its input channel may harm the performance significantly. cpr_skip_op_names : comma-separated Conv2D operations names to be skipped. For instance, if cpr_skip_op_names is set to \"aaa,bbb\", then any Conv2D operation whose name contains either \"aaa\" or \"bbb\" will be skipped and no channel pruning will be applied on it. cpr_nb_smpls : number of cached training samples for channel pruning. Increasing this may lead to smaller performance degradation after channel pruning but also require more training time. cpr_nb_crops_per_smpl : number of random crops per sample. Increasing this may lead to smaller performance degradation after channel pruning but also require more training time. cpr_ista_lrn_rate : ISTA's learning rate for LASSO regression. If cpr_ista_lrn_rate is too large, then the optimization process may become unstable; if cpr_ista_lrn_rate is too small, then the optimization process may require lots of iterations until convergence. cpr_ista_nb_iters : number of iterations for LASSO regression. cpr_lstsq_lrn_rate : Adam's learning rate for least-square regression. If cpr_lstsq_lrn_rate is too large, then the optimization process may become unstable; if cpr_lstsq_lrn_rate is too small, then the optimization process may require lots of iterations until convergence. cpr_lstsq_nb_iters : number of iterations for least-square regression. cpr_warm_start : whether to use a previously-saved channel-pruned model for warm-start.","title":"Hyper-parameters"},{"location":"cpr_learner/#empirical-evaluation","text":"In this section, we present some of our results for applying ChannelPrunedRmtLearner for compression image classification and object detection models. For image classification, we use ChannelPrunedRmtLearner to compress the ResNet-18 model on the ILSVRC-12 dataset: Model Prune Ratio FLOPs Distillation? Top-1 Acc. Top-5 Acc. ResNet-18 0.2 73.32% No 69.43% 88.97% ResNet-18 0.2 73.32% Yes 68.78% 88.71% ResNet-18 0.3 61.31% No 68.44% 88.30% ResNet-18 0.3 61.31% Yes 68.85% 88.53% ResNet-18 0.4 50.70% No 67.17% 87.48% ResNet-18 0.4 50.70% Yes 67.35% 87.83% ResNet-18 0.5 41.27% No 65.73% 86.38% ResNet-18 0.5 41.27% Yes 65.98% 86.98% ResNet-18 0.6 32.07% No 63.38% 84.62% ResNet-18 0.6 32.07% Yes 63.65% 85.47% ResNet-18 0.7 24.28% No 60.26% 82.70% ResNet-18 0.7 24.28% Yes 60.43% 82.96% For object detection, we use ChannelPrunedRmtLearner to compress the SSD-VGG16 model on the Pascal VOC 07-12 dataset: Model Prune Ratio FLOPs Pruned Layers mAP ResNet-18 0.2 67.34% Backbone 77.53% ResNet-18 0.2 66.50% All 77.22% ResNet-18 0.3 53.58% Backbone 76.94% ResNet-18 0.3 52.32% All 76.90% ResNet-18 0.4 41.63% Backbone 75.81% ResNet-18 0.4 39.96% All 75.80% ResNet-18 0.5 31.56% Backbone 74.42% ResNet-18 0.5 29.47% All 73.76%","title":"Empirical Evaluation"},{"location":"cpr_learner/#usage-examples","text":"In this section, we provide some usage examples to demonstrate how to use ChannelPrunedRmtLearner under different execution modes and hyper-parameter combinations: To compress a ResNet-20 model for CIFAR-10 classification task in the local mode, use: # set the target pruning ratio to 0.50 ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner=chn-pruned-rmt \\ --cpr_prune_ratio=0.50 To compress a ResNet-18 model for ILSVRC-12 classification task in the docker mode with 4 GPUs, use: # do no apply channel pruning to the last convolutional layer ./scripts/run_docker.sh nets/resnet_at_ilsvrc12_run.py -n=4 \\ --learner=chn-pruned-rmt \\ --cpr_skip_last_layer=True To compress a MobileNet-v1 model for ILSVRC-12 classification task in the seven mode with 8 GPUs, use: # use a channel-pruned model for warm-start, so no channel selection is needed ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py -n=8 \\ --learner=chn-pruned-rmt \\ --cpr_warm_start=True \\ --cpr_save_path_ws=./models_cpr_ws/model.ckpt","title":"Usage Examples"},{"location":"dcp_learner/","text":"Discrimination-aware Channel Pruning Introduction Discrimination-aware channel pruning (DCP, Zhuang et al., 2018) introduces a group of additional discriminative losses into the network to be pruned, to find out which channels are really contributing to the discriminative power and should be preserved. After channel pruning, the number of input channels of each convolutional layer is reduced, so that the model becomes smaller and the inference speed can be improved. Algorithm Description For a convolutional layer, we denote its input feature map as \\mathbf{X} \\in \\mathbb{R}^{N \\times c_{i} \\times h_{i} \\times w_{i}} \\mathbf{X} \\in \\mathbb{R}^{N \\times c_{i} \\times h_{i} \\times w_{i}} , where N N is the batch size, c_{i} c_{i} is the number of inputs channels, and h_{i} h_{i} and w_{i} w_{i} are the spatial height and width. The convolutional kernel is denoted as \\mathbf{W} \\in \\mathbb{R}^{c_{o} \\times c_{i} \\times k \\times k} \\mathbf{W} \\in \\mathbb{R}^{c_{o} \\times c_{i} \\times k \\times k} , where c_{o} c_{o} is the number of output channels and k k is the kernel size. The resulting output feature map is given by \\mathbf{Y} = f \\left( \\mathbf{X}; \\mathbf{W} \\right) \\mathbf{Y} = f \\left( \\mathbf{X}; \\mathbf{W} \\right) , where f \\left( \\cdot \\right) f \\left( \\cdot \\right) represents the convolutional operation. The idea of channel pruning is to impose the sparsity constraint on the convolutional kernel, so that some of its input channels only contains all-zero weights and can be safely removed. For instance, if the convolutional kernel satisfies: \\left\\| \\left\\| \\mathbf{W}_{:, j, :, :} \\right\\|_{F}^{2} \\right\\|_{0} = c'_{i}, \\left\\| \\left\\| \\mathbf{W}_{:, j, :, :} \\right\\|_{F}^{2} \\right\\|_{0} = c'_{i}, where c'_{i} \\lt c_{i} c'_{i} \\lt c_{i} , then the convolutional layer simplified to with c'_{i} c'_{i} input channels only, and the computational complexity is reduced by a ratio of \\frac{c_{i} - c'_{i}}{c_{i}} \\frac{c_{i} - c'_{i}}{c_{i}} . In order to reduce the performance degradation caused by channel pruning, the DCP algorithm introduces a novel channel selection algorithm by incorporating additional discrimination-aware and reconstruction loss terms, as shown below. Source: Zhuang et al., Discrimination-aware Channel Pruning for Deep Neural Networks . NIPS '18. The network is evenly divided into \\left( P + 1 \\right) \\left( P + 1 \\right) blocks. For each of the first P P blocks, an extra branch is derived from the output feature map of this block's last layer. The output feature map is then passed through batch normalization & ReLU & average pooling & softmax layers to produce predictions, from which a discrimination-aware loss is constructed, denoted as L_{p} L_{p} . For the last block, the final loss of whole network, denoted as L L , is used as its discrimination-aware loss. Additionally, for each layer in the channel pruned network, a reconstruction loss is introduced to force it to re-produce the corresponding output feature map in the original network. We denote the q q -th layer's reconstruction loss as L_{q}^{( R )} L_{q}^{( R )} . Based on a pre-trained model, the DCP algorithm performs channel pruning with \\left( P + 1 \\right) \\left( P + 1 \\right) stages. During the p p -th stage, the network is fine-tuned with the p p -th discrimination-aware loss L_{p} L_{p} plus the final loss L L . After the block-wise fine-tuning, we sequentially perform channel pruning for each convolutional layer within the block. For channel pruning, we compute each input channel's gradients w.r.t. the reconstruction loss L_{q}^{( R )} L_{q}^{( R )} plus the discrimination-aware loss L_{p} L_{p} , and remove the input channel with the minimal Frobenius norm of gradients. After that, this layer is fine-tuned with the remaining input channels only to (partially) recover the discriminative power. We repeat this process until the target pruning ratio is reached. After all convolutional layers have been pruned, the resulting network can be further fine-tuned for a few epochs to further reduce the performance loss. Hyper-parameters Below is the full list of hyper-parameters used in the discrimination-aware channel pruning learner: Name Description dcp_save_path model's save path dcp_save_path_eval model's save path for evaluation dcp_prune_ratio target pruning ratio dcp_nb_stages number of channel pruning stages dcp_lrn_rate_adam Adam's learning rate for block-wise & layer-wise fine-tuning dcp_nb_iters_block number of iterations for block-wise fine-tuning dcp_nb_iters_layer number of iterations for layer-wise fine-tuning Here, we provide detailed description (and some analysis) for above hyper-parameters: dcp_save_path : save path for model created in the training graph. The resulting checkpoint files can be used to resume training from a previous run and compute model's loss function's value and some other evaluation metrics. dcp_save_path_eval : save path for model created in the evaluation graph. The resulting checkpoint files can be used to export GraphDef & TensorFlow Lite model files. dcp_prune_ratio : target pruning ratio for input channels of each convolutional layer. The larger dcp_prune_ratio is, the more input channels will be pruned. If dcp_prune_ratio equals 0, then no input channels will be pruned and model remains the same; if dcp_prune_ratio equals 1, then all input channels will be pruned. dcp_nb_stages : number of channel pruning stages / number of discrimination-aware losses. The training process of DCP algorithm is divided into multiple stages. For each discrimination-aware loss, a channel pruning stage is involved to select channels within corresponding layers. The final classification loss corresponds to a pseudo channel pruning stage, which is not counted in dcp_nb_stages .The larger dcp_nb_stages is, the slower the training process will be. dcp_lrn_rate_adam : Adam's learning rate for block-wise & layer-wise fine-tuning. If dcp_lrn_rate_adam is too large, then the fine-tuning process may become unstable; if dcp_lrn_rate_adam is too small, then the fine-tuning process may take long time to converge. dcp_nb_iters_block : number of iterations for block-wise fine-tuning. This should be set to some value that the block-wise fine-tuning can almost converge and the loss function's value does not decrease much even if more iterations are used. dcp_nb_iters_layer : number of iterations for layer-wise fine-tuning. This should be set to some value that the layer-wise fine-tuning can almost converge and the loss function's value does not decrease much even if more iterations are used. Usage Examples In this section, we provide some usage examples to demonstrate how to use DisChnPrunedLearner under different execution modes and hyper-parameter combinations: To compress a ResNet-20 model for CIFAR-10 classification task in the local mode, use: # set the target pruning ratio to 0.75 ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner dis-chn-pruned \\ --dcp_prune_ratio 0.75 To compress a ResNet-34 model for ILSVRC-12 classification task in the docker mode with 4 GPUs, use: # set the number of channel pruning stages to 4 ./scripts/run_docker.sh nets/resnet_at_ilsvrc12_run.py -n=4 \\ --learner dis-chn-pruned \\ --resnet_size 34 \\ --dcp_nb_stages 4 To compress a MobileNet-v2 model for ILSVRC-12 classification task in the seven mode with 8 GPUs, use: # enable training with distillation loss ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py -n=8 \\ --learner dis-chn-pruned \\ --mobilenet_version 2 \\ --enbl_dst","title":"Discrimination-aware Channel Pruning"},{"location":"dcp_learner/#discrimination-aware-channel-pruning","text":"","title":"Discrimination-aware Channel Pruning"},{"location":"dcp_learner/#introduction","text":"Discrimination-aware channel pruning (DCP, Zhuang et al., 2018) introduces a group of additional discriminative losses into the network to be pruned, to find out which channels are really contributing to the discriminative power and should be preserved. After channel pruning, the number of input channels of each convolutional layer is reduced, so that the model becomes smaller and the inference speed can be improved.","title":"Introduction"},{"location":"dcp_learner/#algorithm-description","text":"For a convolutional layer, we denote its input feature map as \\mathbf{X} \\in \\mathbb{R}^{N \\times c_{i} \\times h_{i} \\times w_{i}} \\mathbf{X} \\in \\mathbb{R}^{N \\times c_{i} \\times h_{i} \\times w_{i}} , where N N is the batch size, c_{i} c_{i} is the number of inputs channels, and h_{i} h_{i} and w_{i} w_{i} are the spatial height and width. The convolutional kernel is denoted as \\mathbf{W} \\in \\mathbb{R}^{c_{o} \\times c_{i} \\times k \\times k} \\mathbf{W} \\in \\mathbb{R}^{c_{o} \\times c_{i} \\times k \\times k} , where c_{o} c_{o} is the number of output channels and k k is the kernel size. The resulting output feature map is given by \\mathbf{Y} = f \\left( \\mathbf{X}; \\mathbf{W} \\right) \\mathbf{Y} = f \\left( \\mathbf{X}; \\mathbf{W} \\right) , where f \\left( \\cdot \\right) f \\left( \\cdot \\right) represents the convolutional operation. The idea of channel pruning is to impose the sparsity constraint on the convolutional kernel, so that some of its input channels only contains all-zero weights and can be safely removed. For instance, if the convolutional kernel satisfies: \\left\\| \\left\\| \\mathbf{W}_{:, j, :, :} \\right\\|_{F}^{2} \\right\\|_{0} = c'_{i}, \\left\\| \\left\\| \\mathbf{W}_{:, j, :, :} \\right\\|_{F}^{2} \\right\\|_{0} = c'_{i}, where c'_{i} \\lt c_{i} c'_{i} \\lt c_{i} , then the convolutional layer simplified to with c'_{i} c'_{i} input channels only, and the computational complexity is reduced by a ratio of \\frac{c_{i} - c'_{i}}{c_{i}} \\frac{c_{i} - c'_{i}}{c_{i}} . In order to reduce the performance degradation caused by channel pruning, the DCP algorithm introduces a novel channel selection algorithm by incorporating additional discrimination-aware and reconstruction loss terms, as shown below. Source: Zhuang et al., Discrimination-aware Channel Pruning for Deep Neural Networks . NIPS '18. The network is evenly divided into \\left( P + 1 \\right) \\left( P + 1 \\right) blocks. For each of the first P P blocks, an extra branch is derived from the output feature map of this block's last layer. The output feature map is then passed through batch normalization & ReLU & average pooling & softmax layers to produce predictions, from which a discrimination-aware loss is constructed, denoted as L_{p} L_{p} . For the last block, the final loss of whole network, denoted as L L , is used as its discrimination-aware loss. Additionally, for each layer in the channel pruned network, a reconstruction loss is introduced to force it to re-produce the corresponding output feature map in the original network. We denote the q q -th layer's reconstruction loss as L_{q}^{( R )} L_{q}^{( R )} . Based on a pre-trained model, the DCP algorithm performs channel pruning with \\left( P + 1 \\right) \\left( P + 1 \\right) stages. During the p p -th stage, the network is fine-tuned with the p p -th discrimination-aware loss L_{p} L_{p} plus the final loss L L . After the block-wise fine-tuning, we sequentially perform channel pruning for each convolutional layer within the block. For channel pruning, we compute each input channel's gradients w.r.t. the reconstruction loss L_{q}^{( R )} L_{q}^{( R )} plus the discrimination-aware loss L_{p} L_{p} , and remove the input channel with the minimal Frobenius norm of gradients. After that, this layer is fine-tuned with the remaining input channels only to (partially) recover the discriminative power. We repeat this process until the target pruning ratio is reached. After all convolutional layers have been pruned, the resulting network can be further fine-tuned for a few epochs to further reduce the performance loss.","title":"Algorithm Description"},{"location":"dcp_learner/#hyper-parameters","text":"Below is the full list of hyper-parameters used in the discrimination-aware channel pruning learner: Name Description dcp_save_path model's save path dcp_save_path_eval model's save path for evaluation dcp_prune_ratio target pruning ratio dcp_nb_stages number of channel pruning stages dcp_lrn_rate_adam Adam's learning rate for block-wise & layer-wise fine-tuning dcp_nb_iters_block number of iterations for block-wise fine-tuning dcp_nb_iters_layer number of iterations for layer-wise fine-tuning Here, we provide detailed description (and some analysis) for above hyper-parameters: dcp_save_path : save path for model created in the training graph. The resulting checkpoint files can be used to resume training from a previous run and compute model's loss function's value and some other evaluation metrics. dcp_save_path_eval : save path for model created in the evaluation graph. The resulting checkpoint files can be used to export GraphDef & TensorFlow Lite model files. dcp_prune_ratio : target pruning ratio for input channels of each convolutional layer. The larger dcp_prune_ratio is, the more input channels will be pruned. If dcp_prune_ratio equals 0, then no input channels will be pruned and model remains the same; if dcp_prune_ratio equals 1, then all input channels will be pruned. dcp_nb_stages : number of channel pruning stages / number of discrimination-aware losses. The training process of DCP algorithm is divided into multiple stages. For each discrimination-aware loss, a channel pruning stage is involved to select channels within corresponding layers. The final classification loss corresponds to a pseudo channel pruning stage, which is not counted in dcp_nb_stages .The larger dcp_nb_stages is, the slower the training process will be. dcp_lrn_rate_adam : Adam's learning rate for block-wise & layer-wise fine-tuning. If dcp_lrn_rate_adam is too large, then the fine-tuning process may become unstable; if dcp_lrn_rate_adam is too small, then the fine-tuning process may take long time to converge. dcp_nb_iters_block : number of iterations for block-wise fine-tuning. This should be set to some value that the block-wise fine-tuning can almost converge and the loss function's value does not decrease much even if more iterations are used. dcp_nb_iters_layer : number of iterations for layer-wise fine-tuning. This should be set to some value that the layer-wise fine-tuning can almost converge and the loss function's value does not decrease much even if more iterations are used.","title":"Hyper-parameters"},{"location":"dcp_learner/#usage-examples","text":"In this section, we provide some usage examples to demonstrate how to use DisChnPrunedLearner under different execution modes and hyper-parameter combinations: To compress a ResNet-20 model for CIFAR-10 classification task in the local mode, use: # set the target pruning ratio to 0.75 ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner dis-chn-pruned \\ --dcp_prune_ratio 0.75 To compress a ResNet-34 model for ILSVRC-12 classification task in the docker mode with 4 GPUs, use: # set the number of channel pruning stages to 4 ./scripts/run_docker.sh nets/resnet_at_ilsvrc12_run.py -n=4 \\ --learner dis-chn-pruned \\ --resnet_size 34 \\ --dcp_nb_stages 4 To compress a MobileNet-v2 model for ILSVRC-12 classification task in the seven mode with 8 GPUs, use: # enable training with distillation loss ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py -n=8 \\ --learner dis-chn-pruned \\ --mobilenet_version 2 \\ --enbl_dst","title":"Usage Examples"},{"location":"distillation/","text":"Distillation Distillation (Hinton et al., 2015) is a kind of model compression approaches in which a pre-trained large model teaches a smaller model to achieve the similar prediction performance. It is often named as the \"teacher-student\" training, where the large model is the teacher and the smaller model is the student. With distillation, knowledge can be transferred from the teacher model to the student by minimizing a loss function to recover the distribution of class probabilities predicted by the teacher model. In most situations, the probability of the correct class predicted by the teacher model is very high, and probabilities of other classes are close to 0, which may not be able to provide extra information beyond ground-truth labels. To overcome this issue, a commonly-used solution is to raise the temperature of the final softmax function until the cumbersome model produces a suitably soft set of targets. The soften probability q_i q_i of class i i is calculated from the logit z_i z_i : q_i = \\frac{\\exp \\left( z_i / T \\right)}{\\sum_j{\\exp \\left( z_j / T \\right)}} q_i = \\frac{\\exp \\left( z_i / T \\right)}{\\sum_j{\\exp \\left( z_j / T \\right)}} where T T is the temperature. As T T grows, the probability distribution is more smooth, providing more information as to which classes the cumbersome model more similar to the predicted class. It is better to include the standard loss ( T = 1 T = 1 ) between the predicted class probabilities and ground-truth labels. The overall loss function is given by: L \\left( x; W \\right) = H \\left( y, \\sigma \\left( z_s; T = 1 \\right) \\right) + \\alpha \\cdot H \\left( \\sigma \\left( z_t; T = \\tau \\right), \\sigma \\left( z_s, T = \\tau \\right) \\right) L \\left( x; W \\right) = H \\left( y, \\sigma \\left( z_s; T = 1 \\right) \\right) + \\alpha \\cdot H \\left( \\sigma \\left( z_t; T = \\tau \\right), \\sigma \\left( z_s, T = \\tau \\right) \\right) where x x is the input, W W are parameters of the distilled small model and y y is ground-truth labels, \\sigma \\sigma is the softmax parameterized by temperature T T , H H is the cross-entropy loss, and \\alpha \\alpha is the coefficient of distillation loss. The coefficient \\alpha \\alpha can be set by --loss_w_dst and the temperature T T can be set by --tempr_dst . Combination with Other Model Compression Approaches Other model model compression techniques, such as channel pruning, weight pruning, and quantization, can be augmented with distillation. To enable the distillation loss, simply append the --enbl_dst argument when starting the program.","title":"Distillation"},{"location":"distillation/#distillation","text":"Distillation (Hinton et al., 2015) is a kind of model compression approaches in which a pre-trained large model teaches a smaller model to achieve the similar prediction performance. It is often named as the \"teacher-student\" training, where the large model is the teacher and the smaller model is the student. With distillation, knowledge can be transferred from the teacher model to the student by minimizing a loss function to recover the distribution of class probabilities predicted by the teacher model. In most situations, the probability of the correct class predicted by the teacher model is very high, and probabilities of other classes are close to 0, which may not be able to provide extra information beyond ground-truth labels. To overcome this issue, a commonly-used solution is to raise the temperature of the final softmax function until the cumbersome model produces a suitably soft set of targets. The soften probability q_i q_i of class i i is calculated from the logit z_i z_i : q_i = \\frac{\\exp \\left( z_i / T \\right)}{\\sum_j{\\exp \\left( z_j / T \\right)}} q_i = \\frac{\\exp \\left( z_i / T \\right)}{\\sum_j{\\exp \\left( z_j / T \\right)}} where T T is the temperature. As T T grows, the probability distribution is more smooth, providing more information as to which classes the cumbersome model more similar to the predicted class. It is better to include the standard loss ( T = 1 T = 1 ) between the predicted class probabilities and ground-truth labels. The overall loss function is given by: L \\left( x; W \\right) = H \\left( y, \\sigma \\left( z_s; T = 1 \\right) \\right) + \\alpha \\cdot H \\left( \\sigma \\left( z_t; T = \\tau \\right), \\sigma \\left( z_s, T = \\tau \\right) \\right) L \\left( x; W \\right) = H \\left( y, \\sigma \\left( z_s; T = 1 \\right) \\right) + \\alpha \\cdot H \\left( \\sigma \\left( z_t; T = \\tau \\right), \\sigma \\left( z_s, T = \\tau \\right) \\right) where x x is the input, W W are parameters of the distilled small model and y y is ground-truth labels, \\sigma \\sigma is the softmax parameterized by temperature T T , H H is the cross-entropy loss, and \\alpha \\alpha is the coefficient of distillation loss. The coefficient \\alpha \\alpha can be set by --loss_w_dst and the temperature T T can be set by --tempr_dst .","title":"Distillation"},{"location":"distillation/#combination-with-other-model-compression-approaches","text":"Other model model compression techniques, such as channel pruning, weight pruning, and quantization, can be augmented with distillation. To enable the distillation loss, simply append the --enbl_dst argument when starting the program.","title":"Combination with Other Model Compression Approaches"},{"location":"faq/","text":"Frequently Asked Questions Q: Under construction ... A: Under construction ...","title":"Frequently Asked Questions"},{"location":"faq/#frequently-asked-questions","text":"Q: Under construction ... A: Under construction ...","title":"Frequently Asked Questions"},{"location":"installation/","text":"Installation PocketFlow is developed and tested on Linux, using Python 3.6 and TensorFlow 1.10.0. We support the following three execution modes for PocketFlow: Local mode: run PocketFlow on the local machine. Docker mode: run PocketFlow within a docker image. Seven mode: run PocketFlow on the seven cluster (only available within Tencent). Clone PocketFlow To make a local copy of the PocketFlow repository, use: $ git clone https://github.com/Tencent/PocketFlow.git Create a Path Configuration File PocketFlow requires a path configuration file, named path.conf , to setup directory paths to data sets and pre-trained models under different execution modes, as well as HDFS / HTTP connection parameters. We have provided a template file to help you create your own path configuration file. You can find it in the PocketFlow repository, named path.conf.template , which contains more detailed descriptions on how to customize path configurations. For instance, if you want to use CIFAR-10 and ImageNet data sets stored on the local machine, then the path configuration file should look like this: # data files data_hdfs_host = None data_dir_local_cifar10 = /home/user_name/datasets/cifar-10-batches-bin # this line has been edited! data_dir_hdfs_cifar10 = None data_dir_seven_cifar10 = None data_dir_docker_cifar10 = /opt/ml/data # DO NOT EDIT data_dir_local_ilsvrc12 = /home/user_name/datasets/imagenet_tfrecord # this line has been edited! data_dir_hdfs_ilsvrc12 = None data_dir_seven_ilsvrc12 = None data_dir_docker_ilsvrc12 = /opt/ml/data # DO NOT EDIT # model files model_http_url = https://api.ai.tencent.com/pocketflow In short, you need to replace \"None\" in the template file with the actual path (or HDFS / HTTP connection parameters) if available, or leave it unchanged otherwise. Prepare for the Local Mode We recommend to use Anaconda as the Python environment, which has many essential packages built-in. The Anaconda installer can be downloaded from here . To install, use the following command: # install Anaconda; replace the installer's file name if needed $ bash Anaconda3-5.2.0-Linux-x86_64.sh # activate Anaconda's Python path $ source ~/.bashrc For Anaconda 5.3.0 or later, the default Python version is 3.7, which does not support installing TensorFlow with pip directly. Therefore, you need to manually switch to Python 3.6 once Anaconda is installed: # install Python 3.6 $ conda install python=3.6 To install TensorFlow, you may refer to TensorFlow's official documentation for detailed instructions. Specially, if GPU-based training is required, then you need to follow the GPU support guide to set up a CUDA-enabled GPU card in prior to installation. After that, install TensorFlow with: # TensorFlow with GPU support; use <tensorflow> if GPU is not available $ pip install tensorflow-gpu # verify the install $ python -c \"import tensorflow as tf; print(tf.__version__)\" To run PocketFlow in the local mode, e.g. to train a full-precision ResNet-20 model for the CIFAR-10 classification task, use the following command: $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py Prepare for the Docker Mode Docker offers an alternative way to run PocketFlow within an isolated container, so that your local Python environment remains untouched. We recommend you to use the horovod docker provided by Uber, which enables multi-GPU distributed training for TensorFlow with only a few lines modification. Once docker is installed, the docker image can be obtained via: # obtain the docker image $ docker pull uber/horovod To run PocketFlow in the docker mode, e.g. to train a full-precision ResNet-20 model for the CIFAR-10 classification task, use the following command: $ ./scripts/run_docker.sh nets/resnet_at_cifar10_run.py Prepare for the Seven Mode Seven is a distributed learning platform built for both CPU and GPU clusters. Users can submit tasks to the seven cluster, using built-in data sets and docker images seamlessly. To run PocketFlow in the seven mode, e.g. to train a full-precision ResNet-20 model for the CIFAR-10 classification task, use the following command: $ ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py","title":"Installation"},{"location":"installation/#installation","text":"PocketFlow is developed and tested on Linux, using Python 3.6 and TensorFlow 1.10.0. We support the following three execution modes for PocketFlow: Local mode: run PocketFlow on the local machine. Docker mode: run PocketFlow within a docker image. Seven mode: run PocketFlow on the seven cluster (only available within Tencent).","title":"Installation"},{"location":"installation/#clone-pocketflow","text":"To make a local copy of the PocketFlow repository, use: $ git clone https://github.com/Tencent/PocketFlow.git","title":"Clone PocketFlow"},{"location":"installation/#create-a-path-configuration-file","text":"PocketFlow requires a path configuration file, named path.conf , to setup directory paths to data sets and pre-trained models under different execution modes, as well as HDFS / HTTP connection parameters. We have provided a template file to help you create your own path configuration file. You can find it in the PocketFlow repository, named path.conf.template , which contains more detailed descriptions on how to customize path configurations. For instance, if you want to use CIFAR-10 and ImageNet data sets stored on the local machine, then the path configuration file should look like this: # data files data_hdfs_host = None data_dir_local_cifar10 = /home/user_name/datasets/cifar-10-batches-bin # this line has been edited! data_dir_hdfs_cifar10 = None data_dir_seven_cifar10 = None data_dir_docker_cifar10 = /opt/ml/data # DO NOT EDIT data_dir_local_ilsvrc12 = /home/user_name/datasets/imagenet_tfrecord # this line has been edited! data_dir_hdfs_ilsvrc12 = None data_dir_seven_ilsvrc12 = None data_dir_docker_ilsvrc12 = /opt/ml/data # DO NOT EDIT # model files model_http_url = https://api.ai.tencent.com/pocketflow In short, you need to replace \"None\" in the template file with the actual path (or HDFS / HTTP connection parameters) if available, or leave it unchanged otherwise.","title":"Create a Path Configuration File"},{"location":"installation/#prepare-for-the-local-mode","text":"We recommend to use Anaconda as the Python environment, which has many essential packages built-in. The Anaconda installer can be downloaded from here . To install, use the following command: # install Anaconda; replace the installer's file name if needed $ bash Anaconda3-5.2.0-Linux-x86_64.sh # activate Anaconda's Python path $ source ~/.bashrc For Anaconda 5.3.0 or later, the default Python version is 3.7, which does not support installing TensorFlow with pip directly. Therefore, you need to manually switch to Python 3.6 once Anaconda is installed: # install Python 3.6 $ conda install python=3.6 To install TensorFlow, you may refer to TensorFlow's official documentation for detailed instructions. Specially, if GPU-based training is required, then you need to follow the GPU support guide to set up a CUDA-enabled GPU card in prior to installation. After that, install TensorFlow with: # TensorFlow with GPU support; use <tensorflow> if GPU is not available $ pip install tensorflow-gpu # verify the install $ python -c \"import tensorflow as tf; print(tf.__version__)\" To run PocketFlow in the local mode, e.g. to train a full-precision ResNet-20 model for the CIFAR-10 classification task, use the following command: $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py","title":"Prepare for the Local Mode"},{"location":"installation/#prepare-for-the-docker-mode","text":"Docker offers an alternative way to run PocketFlow within an isolated container, so that your local Python environment remains untouched. We recommend you to use the horovod docker provided by Uber, which enables multi-GPU distributed training for TensorFlow with only a few lines modification. Once docker is installed, the docker image can be obtained via: # obtain the docker image $ docker pull uber/horovod To run PocketFlow in the docker mode, e.g. to train a full-precision ResNet-20 model for the CIFAR-10 classification task, use the following command: $ ./scripts/run_docker.sh nets/resnet_at_cifar10_run.py","title":"Prepare for the Docker Mode"},{"location":"installation/#prepare-for-the-seven-mode","text":"Seven is a distributed learning platform built for both CPU and GPU clusters. Users can submit tasks to the seven cluster, using built-in data sets and docker images seamlessly. To run PocketFlow in the seven mode, e.g. to train a full-precision ResNet-20 model for the CIFAR-10 classification task, use the following command: $ ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py","title":"Prepare for the Seven Mode"},{"location":"multi_gpu_training/","text":"Multi-GPU Training Due to the high computational complexity, it often takes hours or even days to fully train deep learning models using a single GPU. In PocketFlow, we adopt multi-GPU training to speed-up this time-consuming training process. Our implementation is compatible with: Horovod : a distributed training framework for TensorFlow, Keras, and PyTorch. TF-Plus: an optimized framework for TensorFlow-based distributed training (only available within Tencent). We have provide a wrapper class, MultiGpuWrapper , to seamlessly switch between the above two frameworks. It will sequentially check whether Horovod and TF-Plus can be used, and use the first available one as the underlying framework for multi-GPU training. The main reason that using Horovod or TF-Plus instead TensorFlow's original distributed training routine is that these frameworks provide many easy-to-use APIs and require far less code changes to change from single-GPU to multi-GPU training, as we shall see later. From Single-GPU to Multi-GPU To extend a single-GPU based training script to the multi-GPU scenario, at most 7 steps are needed: Import the Horovod or TF-Plus module. from utils.multi_gpu_wrapper import MultiGpuWrapper as mgw Initialize the multi-GPU training framework, as early as possible. mgw.init() For each worker, create a session with a distinct GPU device. config = tf.ConfigProto() config.gpu_options.visible_device_list = str(mgw.local_rank()) sess = tf.Session(config=config) (Optional) Let each worker use a distinct subset of training data. filenames = tf.data.Dataset.list_files(file_pattern, shuffle=True) filenames = filenames.shard(mgw.size(), mgw.rank()) Wrapper the optimizer for distributed gradient communication. optimizer = tf.train.AdamOptimizer(learning_rate=lrn_rate) optimizer = mgw.DistributedOptimizer(optimizer) train_op = optimizer.minimize(loss) Synchronize master's parameters to all the other workers. bcast_op = mgw.broadcast_global_variables(0) sess.run(tf.global_variables_initializer()) sess.run(bcast_op) (Optional) Save checkpoint files at the master node periodically. if mgw.rank() == 0: saver.save(sess, save_path, global_step) Usage Example Here, we provide a code snippet to demonstrate how to use multi-GPU training to speed-up training. Please note that many implementation details are omitted for clarity. import tensorflow as tf from utils.multi_gpu_wrapper import MultiGpuWrapper as mgw # initialization mgw.init() # create the training graph with tf.Graph().as_default(): # create a TensorFlow session config = tf.ConfigProto() config.gpu_options.visible_device_list = str(mgw.local_rank()) sess = tf.Session(config=config) # use tf.data.Dataset() to traverse images and labels filenames = tf.data.Dataset.list_files(file_pattern, shuffle=True) filenames = filenames.shard(mgw.size(), mgw.rank()) images, labels = get_images_n_labels(filenames) # define the network and its loss function logits = forward_pass(images) loss = calc_loss(labels, logits) # create an optimizer and setup training-related operations global_step = tf.train.get_or_create_global_step() optimizer = tf.train.AdamOptimizer(learning_rate=lrn_rate) optimizer = mgw.DistributedOptimizer(optimizer) train_op = optimizer.minimize(loss, global_step=global_step) bcast_op = mgw.broadcast_global_variables(0) # multi-GPU training sess.run(tf.global_variables_initializer()) sess.run(bcast_op) for idx_iter in range(nb_iters): sess.run(train_op) if mgw.rank() == 0 and (idx_iter + 1) % save_step == 0: saver.save(sess, save_path, global_step)","title":"Multi-GPU Training"},{"location":"multi_gpu_training/#multi-gpu-training","text":"Due to the high computational complexity, it often takes hours or even days to fully train deep learning models using a single GPU. In PocketFlow, we adopt multi-GPU training to speed-up this time-consuming training process. Our implementation is compatible with: Horovod : a distributed training framework for TensorFlow, Keras, and PyTorch. TF-Plus: an optimized framework for TensorFlow-based distributed training (only available within Tencent). We have provide a wrapper class, MultiGpuWrapper , to seamlessly switch between the above two frameworks. It will sequentially check whether Horovod and TF-Plus can be used, and use the first available one as the underlying framework for multi-GPU training. The main reason that using Horovod or TF-Plus instead TensorFlow's original distributed training routine is that these frameworks provide many easy-to-use APIs and require far less code changes to change from single-GPU to multi-GPU training, as we shall see later.","title":"Multi-GPU Training"},{"location":"multi_gpu_training/#from-single-gpu-to-multi-gpu","text":"To extend a single-GPU based training script to the multi-GPU scenario, at most 7 steps are needed: Import the Horovod or TF-Plus module. from utils.multi_gpu_wrapper import MultiGpuWrapper as mgw Initialize the multi-GPU training framework, as early as possible. mgw.init() For each worker, create a session with a distinct GPU device. config = tf.ConfigProto() config.gpu_options.visible_device_list = str(mgw.local_rank()) sess = tf.Session(config=config) (Optional) Let each worker use a distinct subset of training data. filenames = tf.data.Dataset.list_files(file_pattern, shuffle=True) filenames = filenames.shard(mgw.size(), mgw.rank()) Wrapper the optimizer for distributed gradient communication. optimizer = tf.train.AdamOptimizer(learning_rate=lrn_rate) optimizer = mgw.DistributedOptimizer(optimizer) train_op = optimizer.minimize(loss) Synchronize master's parameters to all the other workers. bcast_op = mgw.broadcast_global_variables(0) sess.run(tf.global_variables_initializer()) sess.run(bcast_op) (Optional) Save checkpoint files at the master node periodically. if mgw.rank() == 0: saver.save(sess, save_path, global_step)","title":"From Single-GPU to Multi-GPU"},{"location":"multi_gpu_training/#usage-example","text":"Here, we provide a code snippet to demonstrate how to use multi-GPU training to speed-up training. Please note that many implementation details are omitted for clarity. import tensorflow as tf from utils.multi_gpu_wrapper import MultiGpuWrapper as mgw # initialization mgw.init() # create the training graph with tf.Graph().as_default(): # create a TensorFlow session config = tf.ConfigProto() config.gpu_options.visible_device_list = str(mgw.local_rank()) sess = tf.Session(config=config) # use tf.data.Dataset() to traverse images and labels filenames = tf.data.Dataset.list_files(file_pattern, shuffle=True) filenames = filenames.shard(mgw.size(), mgw.rank()) images, labels = get_images_n_labels(filenames) # define the network and its loss function logits = forward_pass(images) loss = calc_loss(labels, logits) # create an optimizer and setup training-related operations global_step = tf.train.get_or_create_global_step() optimizer = tf.train.AdamOptimizer(learning_rate=lrn_rate) optimizer = mgw.DistributedOptimizer(optimizer) train_op = optimizer.minimize(loss, global_step=global_step) bcast_op = mgw.broadcast_global_variables(0) # multi-GPU training sess.run(tf.global_variables_initializer()) sess.run(bcast_op) for idx_iter in range(nb_iters): sess.run(train_op) if mgw.rank() == 0 and (idx_iter + 1) % save_step == 0: saver.save(sess, save_path, global_step)","title":"Usage Example"},{"location":"nuq_learner/","text":"Non-Uniform Quantization Learner Non-uniform quantization is a generalization to uniform quantization. In non-uniform quantization, the quantization points are not distributed evenly, and can be optimized via the back-propagation of the network gradients. Consequently, with the same number of bits, non-uniform quantization is more expressive to approximate the original full-precision network comparing to uniform quantization. Nevertheless, the non-uniform quantized model cannot be accelerated directly based on current deep learning frameworks, since the low-precision multiplication requires the intervals among quantization points to be equal. Therefore, the NonUniformQuantLearner can only help better compress the model. Algorithm NonUniformQuantLearner adopts a similar training and evaluation procedure to the UniformQuantLearner . In the training process, the quantized weights are forwarded, while in the backward pass, full precision weights are updated via the STE estimator. The major difference from uniform quantization is that the locations of quantization points are not evenly distributed, but can be optimized and initialized differently. In the following, we introduce the scheme to the update and initialization of quantization points. Optimization the quantization points Unlike uniform quantization, non-uniform quantization can optimize the location of quantization points dynamically during the training of the network, and thereon leads to less quantization loss. The location of quantization points can be updated by summing the gradients of weights that fall into the point ( Han et.al 2015 ), i.e.: $$ \\frac{\\partial \\mathcal{L}}{\\partial c_k} = \\sum_{i,j}\\frac{\\partial\\mathcal{L}}{\\partial w_{ij}}\\frac{\\partial{w_{ij}}}{\\partial c_k}=\\sum_{ij}\\frac{\\partial\\mathcal{L}}{\\partial{w_{ij}}}1(I_{ij}=k) $$ The following figure taken from Han et.al 2015 shows the above process of updating the clusters: Initialization of quantization points Aside from optimizing the quantization points, another helpful strategy is to properly initialize the quantization points according to the distribution of weights. PocketFlow currently supports two kinds of initialization: Uniform initialization: The quantization points are initialized to be evenly distributed along the range [w_{min}, w_{max}] [w_{min}, w_{max}] of that layer/bucket. Quantile initialization: The quantization points are initialized to be the quantiles of full-precision weights. Comparing to uniform initialization, quantile initialization can generally lead to better performance. Hyper-parameters To configure NonUniformQuantLearner , users can pass the options via the TensorFlow flag interface. The available options are as follows: Options Description nuql_opt_mode the fine-tuning mode: [ weights , clusters , both ]. Default: weight nuql_init_style the initialization of quantization point: [ quantile , uniform ]. Default: quantile . nuql_weight_bits the number of bits for weight. Default: 4 . nuql_activation_bits the number of bits for activation. Default: 32 . nuql_save_quant_mode_path the save path for quantized models. Default: ./nuql_quant_models/model.ckpt nuql_use_buckets the switch to quantize first and last layers of network. Default: False . nuql_bucket_type two bucket type available: ['split', 'channel']. Default: channel . nuql_bucket_size the number of bucket size for bucket type 'split'. Default: 256 . nuql_enbl_rl_agent the switch to enable RL to learn optimal bit strategy. Default: False . nuql_quantize_all_layers the switch to quantize first and last layers of network. Default: False . nuql_quant_epoch the number of epochs for fine-tuning. Default: 60 . Here, we provide detailed description (and some analysis) for some of the above hyper-parameters: nuql_opt_mode : the mode for fine-tuning the non-uniformly quantized network, choose among [ weights , clusters , both ]. weight refers to only updating the network weights, while clusters refers to only updating the quantization points, and both means updating weights and quantization points simultaneously. Experimentally, we found that weight and both achieve similar performance, both of which outperform clusters . nuql_init_style : the style of initialization of quantization points, currently supports [ quantile , uniform ]. The differences between the two strategies have been discussed earlier. nuql_weight_bits : The number of bits for weight quantization. Generally, for lower bit quantization (e.g., 2 bit on CIFAR10 and 4 bit on ILSVRC_12), NonUniformQuantLearner performs much better than UniformQuantLearner . The gap becomes less when using higher bits. nuql_activation_bits : The number of bits for activation quantization. Since non-uniform quantized models cannot be accelerated directly, by default we leave it as 32 bit. nuql_save_quant_mode_path : the path to save the quantized model. Quantization nodes have already been inserted into the graph. nuql_use_buckets : the switch to turn on the bucket. With bucketing, weights are split into multiple pieces, while the \\alpha \\alpha and \\beta \\beta are calculated individually for each piece. Therefore, turning on the bucketing can lead to more fine-grained quantization. nuql_bucket_type : the type of bucketing. Currently two types are supported: [ split , channel ]. split refers to that the weights of a layer are first concatenated into a long vector, and then cut it into short pieces according to uql_bucket_size . The remaining last piece is still regarded as a new piece. After quantization for each piece, the vectors are then folded back to the original shape as the quantized weights. channel refers to that weights with shape [k, k, cin, cout] in a convolutional layer are cut into cout buckets, where each bucket has the size of k * k * cin . For weights with shape [m, n] in fully connected layers, they are cut into n buckets, each of size m . In practice, bucketing with type channel can be calculated more quickly comparing to type split since there are less buckets and less computation to iterate through all buckets. nuql_bucket_size : the size of buckets when using bucket type split . Generally, smaller bucket size can lead to more fine grained quantization, while more storage are required since full precision statistics ( \\alpha \\alpha and \\beta \\beta ) of each bucket need to be kept. nuql_quantize_all_layers : the switch to quantize the first and last layers. The first and last layers of the network are connected directly with the input and output, and are arguably more sensitive to quantization. Keeping them un-quantized can slightly increase the performance, nevertheless, if you want to accelerate the inference speed, all layers are supposed to be quantized. nuql_quant_epoch : the epochs for fine-tuning a quantized network. nuql_enbl_rl_agent : the switch to turn on the RL agent as hyper parameter optimizer. Details about the RL agent and its configurations are described below. Configure the RL Agent Similar to uniform quantization, once nuql_enbl_rl_agent==True , the RL agent will automatically search for the optimal bit allocation strategy for each layer. In order to search efficiently, the agent need to be configured properly. While here we list all the configurable hyper parameters for the agent, users can just keep the default value for most parameters, while modify only a few of them if necessary. Options Description nuql_equivalent_bits the number of re-allocated bits that is equivalent to non-uniform quantization without RL agent. Default: 4 . nuql_nb_rlouts the number of roll outs for training the RL agent. Default: 200 . nuql_w_bit_min the minimal number of bits for each layer. Default: 2 . nuql_w_bit_max the maximal number of bits for each layer. Default: 8 . nuql_enbl_rl_global_tune the switch to fine-tune all layers of the network. Default: True . nuql_enbl_rl_layerwise_tune the switch to fine-tune the network layer by layer. Default: False . nuql_tune_layerwise_steps the number of steps for layer-wise fine-tuning. Default: 300 . nuql_tune_global_steps the number of steps for global fine-tuning. Default: 2000 . nuql_tune_disp_steps the display steps to show the fine-tuning progress. Default: 100 . nuql_enbl_random_layers the switch to randomly permute layers during RL agent training. Default: True . Detailed description can be found in Uniform Quantization , with the only difference that the prefix is changed to nuql_ . Usage Examples Again, users should first get the model prepared. Users can either use the pre-built models in PocketFlow, or develop their customized nets following the model definition in PocketFlow (for example, resnet_at_cifar10.py ) Once the model is built, the Non-Uniform Quantization Learner can be easily triggered as follows: To quantize a ResNet-20 model for CIFAR-10 classification task with 4 bits in the local mode, use: # quantize resnet-20 on CIFAR-10 sh ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner=non-uniform \\ --nuql_weight_bits=4 \\ --nuql_activation_bits=4 \\ To quantize a ResNet-18 model for ILSVRC_12 classification task with 8 bits in the docker mode with 4 GPUs, and allow to use the channel-wise bucketing, use: # quantize the resnet-18 on ILSVRC-12 sh ./scripts/run_docker.sh nets/resnet_at_ilsvrc12_run.py \\ -n=4 \\ --learner=non-uniform \\ --nuql_weight_bits=8 \\ --nuql_activation_bits=8 \\ --nuql_use_buckets=True \\ --nuql_bucket_type=channel To quantize a MobileNet-v1 model for ILSVRC_12 classification task with 4 bits in the seven mode with 8 GPUs, and allow the RL agent to search for the optimal bit strategy, use: # quantize mobilenet-v1 on ILSVRC-12 sh ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py \\ -n=8 \\ --learner=non-uniform \\ --nuql_enbl_rl_agent=True \\ --nuql_equivalent_bits=4 \\ References Han S, Mao H, and Dally W J. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv:1510.00149, 2015","title":"Non-uniform Quantization"},{"location":"nuq_learner/#non-uniform-quantization-learner","text":"Non-uniform quantization is a generalization to uniform quantization. In non-uniform quantization, the quantization points are not distributed evenly, and can be optimized via the back-propagation of the network gradients. Consequently, with the same number of bits, non-uniform quantization is more expressive to approximate the original full-precision network comparing to uniform quantization. Nevertheless, the non-uniform quantized model cannot be accelerated directly based on current deep learning frameworks, since the low-precision multiplication requires the intervals among quantization points to be equal. Therefore, the NonUniformQuantLearner can only help better compress the model.","title":"Non-Uniform Quantization Learner"},{"location":"nuq_learner/#algorithm","text":"NonUniformQuantLearner adopts a similar training and evaluation procedure to the UniformQuantLearner . In the training process, the quantized weights are forwarded, while in the backward pass, full precision weights are updated via the STE estimator. The major difference from uniform quantization is that the locations of quantization points are not evenly distributed, but can be optimized and initialized differently. In the following, we introduce the scheme to the update and initialization of quantization points.","title":"Algorithm"},{"location":"nuq_learner/#optimization-the-quantization-points","text":"Unlike uniform quantization, non-uniform quantization can optimize the location of quantization points dynamically during the training of the network, and thereon leads to less quantization loss. The location of quantization points can be updated by summing the gradients of weights that fall into the point ( Han et.al 2015 ), i.e.: $$ \\frac{\\partial \\mathcal{L}}{\\partial c_k} = \\sum_{i,j}\\frac{\\partial\\mathcal{L}}{\\partial w_{ij}}\\frac{\\partial{w_{ij}}}{\\partial c_k}=\\sum_{ij}\\frac{\\partial\\mathcal{L}}{\\partial{w_{ij}}}1(I_{ij}=k) $$ The following figure taken from Han et.al 2015 shows the above process of updating the clusters:","title":"Optimization the quantization points"},{"location":"nuq_learner/#initialization-of-quantization-points","text":"Aside from optimizing the quantization points, another helpful strategy is to properly initialize the quantization points according to the distribution of weights. PocketFlow currently supports two kinds of initialization: Uniform initialization: The quantization points are initialized to be evenly distributed along the range [w_{min}, w_{max}] [w_{min}, w_{max}] of that layer/bucket. Quantile initialization: The quantization points are initialized to be the quantiles of full-precision weights. Comparing to uniform initialization, quantile initialization can generally lead to better performance.","title":"Initialization of quantization points"},{"location":"nuq_learner/#hyper-parameters","text":"To configure NonUniformQuantLearner , users can pass the options via the TensorFlow flag interface. The available options are as follows: Options Description nuql_opt_mode the fine-tuning mode: [ weights , clusters , both ]. Default: weight nuql_init_style the initialization of quantization point: [ quantile , uniform ]. Default: quantile . nuql_weight_bits the number of bits for weight. Default: 4 . nuql_activation_bits the number of bits for activation. Default: 32 . nuql_save_quant_mode_path the save path for quantized models. Default: ./nuql_quant_models/model.ckpt nuql_use_buckets the switch to quantize first and last layers of network. Default: False . nuql_bucket_type two bucket type available: ['split', 'channel']. Default: channel . nuql_bucket_size the number of bucket size for bucket type 'split'. Default: 256 . nuql_enbl_rl_agent the switch to enable RL to learn optimal bit strategy. Default: False . nuql_quantize_all_layers the switch to quantize first and last layers of network. Default: False . nuql_quant_epoch the number of epochs for fine-tuning. Default: 60 . Here, we provide detailed description (and some analysis) for some of the above hyper-parameters: nuql_opt_mode : the mode for fine-tuning the non-uniformly quantized network, choose among [ weights , clusters , both ]. weight refers to only updating the network weights, while clusters refers to only updating the quantization points, and both means updating weights and quantization points simultaneously. Experimentally, we found that weight and both achieve similar performance, both of which outperform clusters . nuql_init_style : the style of initialization of quantization points, currently supports [ quantile , uniform ]. The differences between the two strategies have been discussed earlier. nuql_weight_bits : The number of bits for weight quantization. Generally, for lower bit quantization (e.g., 2 bit on CIFAR10 and 4 bit on ILSVRC_12), NonUniformQuantLearner performs much better than UniformQuantLearner . The gap becomes less when using higher bits. nuql_activation_bits : The number of bits for activation quantization. Since non-uniform quantized models cannot be accelerated directly, by default we leave it as 32 bit. nuql_save_quant_mode_path : the path to save the quantized model. Quantization nodes have already been inserted into the graph. nuql_use_buckets : the switch to turn on the bucket. With bucketing, weights are split into multiple pieces, while the \\alpha \\alpha and \\beta \\beta are calculated individually for each piece. Therefore, turning on the bucketing can lead to more fine-grained quantization. nuql_bucket_type : the type of bucketing. Currently two types are supported: [ split , channel ]. split refers to that the weights of a layer are first concatenated into a long vector, and then cut it into short pieces according to uql_bucket_size . The remaining last piece is still regarded as a new piece. After quantization for each piece, the vectors are then folded back to the original shape as the quantized weights. channel refers to that weights with shape [k, k, cin, cout] in a convolutional layer are cut into cout buckets, where each bucket has the size of k * k * cin . For weights with shape [m, n] in fully connected layers, they are cut into n buckets, each of size m . In practice, bucketing with type channel can be calculated more quickly comparing to type split since there are less buckets and less computation to iterate through all buckets. nuql_bucket_size : the size of buckets when using bucket type split . Generally, smaller bucket size can lead to more fine grained quantization, while more storage are required since full precision statistics ( \\alpha \\alpha and \\beta \\beta ) of each bucket need to be kept. nuql_quantize_all_layers : the switch to quantize the first and last layers. The first and last layers of the network are connected directly with the input and output, and are arguably more sensitive to quantization. Keeping them un-quantized can slightly increase the performance, nevertheless, if you want to accelerate the inference speed, all layers are supposed to be quantized. nuql_quant_epoch : the epochs for fine-tuning a quantized network. nuql_enbl_rl_agent : the switch to turn on the RL agent as hyper parameter optimizer. Details about the RL agent and its configurations are described below.","title":"Hyper-parameters"},{"location":"nuq_learner/#configure-the-rl-agent","text":"Similar to uniform quantization, once nuql_enbl_rl_agent==True , the RL agent will automatically search for the optimal bit allocation strategy for each layer. In order to search efficiently, the agent need to be configured properly. While here we list all the configurable hyper parameters for the agent, users can just keep the default value for most parameters, while modify only a few of them if necessary. Options Description nuql_equivalent_bits the number of re-allocated bits that is equivalent to non-uniform quantization without RL agent. Default: 4 . nuql_nb_rlouts the number of roll outs for training the RL agent. Default: 200 . nuql_w_bit_min the minimal number of bits for each layer. Default: 2 . nuql_w_bit_max the maximal number of bits for each layer. Default: 8 . nuql_enbl_rl_global_tune the switch to fine-tune all layers of the network. Default: True . nuql_enbl_rl_layerwise_tune the switch to fine-tune the network layer by layer. Default: False . nuql_tune_layerwise_steps the number of steps for layer-wise fine-tuning. Default: 300 . nuql_tune_global_steps the number of steps for global fine-tuning. Default: 2000 . nuql_tune_disp_steps the display steps to show the fine-tuning progress. Default: 100 . nuql_enbl_random_layers the switch to randomly permute layers during RL agent training. Default: True . Detailed description can be found in Uniform Quantization , with the only difference that the prefix is changed to nuql_ .","title":"Configure the RL Agent"},{"location":"nuq_learner/#usage-examples","text":"Again, users should first get the model prepared. Users can either use the pre-built models in PocketFlow, or develop their customized nets following the model definition in PocketFlow (for example, resnet_at_cifar10.py ) Once the model is built, the Non-Uniform Quantization Learner can be easily triggered as follows: To quantize a ResNet-20 model for CIFAR-10 classification task with 4 bits in the local mode, use: # quantize resnet-20 on CIFAR-10 sh ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner=non-uniform \\ --nuql_weight_bits=4 \\ --nuql_activation_bits=4 \\ To quantize a ResNet-18 model for ILSVRC_12 classification task with 8 bits in the docker mode with 4 GPUs, and allow to use the channel-wise bucketing, use: # quantize the resnet-18 on ILSVRC-12 sh ./scripts/run_docker.sh nets/resnet_at_ilsvrc12_run.py \\ -n=4 \\ --learner=non-uniform \\ --nuql_weight_bits=8 \\ --nuql_activation_bits=8 \\ --nuql_use_buckets=True \\ --nuql_bucket_type=channel To quantize a MobileNet-v1 model for ILSVRC_12 classification task with 4 bits in the seven mode with 8 GPUs, and allow the RL agent to search for the optimal bit strategy, use: # quantize mobilenet-v1 on ILSVRC-12 sh ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py \\ -n=8 \\ --learner=non-uniform \\ --nuql_enbl_rl_agent=True \\ --nuql_equivalent_bits=4 \\","title":"Usage Examples"},{"location":"nuq_learner/#references","text":"Han S, Mao H, and Dally W J. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv:1510.00149, 2015","title":"References"},{"location":"performance/","text":"Performance In this documentation, we present evaluation results for applying various model compression methods for ResNet and MobileNet models on the ImageNet classification task, including channel pruning, weight sparsification, and uniform quantization. We adopt ChannelPrunedLearner to shrink the number of channels for convolutional layers to reduce the computation complexity. Instead of using the same pruning ratio for all layers, we utilize the DDPG algorithm as the RL agent to iteratively search for the optimal pruning ratio of each layer. After obtaining the optimal pruning ratios, group fine-tuning is adopted to further improve the compressed model's accuracy, as demonstrated below: Model Pruning Ratio Uniform RL-based RL-based + Group Fine-tuning MobileNet-v1 50% 66.5% 67.8% (+1.3%) 67.9% (+1.4%) MobileNet-v1 60% 66.2% 66.9% (+0.7%) 67.0% (+0.8%) MobileNet-v1 70% 64.4% 64.5% (+0.1%) 64.8% (+0.4%) Mobilenet-v1 80% 61.4% 61.4% (+0.0%) 62.2% (+0.8%) We adopt WeightSparseLearner to introduce the sparsity constraint so that a large portion of model weights can be removed, which leads to smaller model and lower FLOPs for inference. Comparing with the original algorithm proposed in (Zhu & Gupta, 2017), we also incorporate network distillation and reinforcement learning algorithms to further improve the compressed model's accuracy, as shown in the table below: Model Sparsity (Zhu & Gupta, 2017) RL-based MobileNet-v1 50% 69.5% 70.5% (+1.0%) MobileNet-v1 75% 67.7% 68.5% (+0.8%) MobileNet-v1 90% 61.8% 63.4% (+1.6%) MobileNet-v1 95% 53.6% 56.8% (+3.2%) We adopt UniformQuantTFLearner to uniformly quantize model weights from 32-bit floating-point numbers to 8-bit fixed-point numbers. The resulting model can be converted into the TensorFlow Lite format for deployment on mobile devices. In the following two tables, we show that 8-bit quantized models can be as accurate as (or even better than) the original 32-bit ones, and the inference time can be significantly reduced after quantization. Model Top-1 Acc. (32-bit) Top-5 Acc. (32-bit) Top-1 Acc. (8-bit) Top-5 Acc. (8-bit) ResNet-18 70.28% 89.38% 70.31% (+0.03%) 89.40% (+0.02%) ResNet-50 75.97% 92.88% 76.01% (+0.04%) 92.87% (-0.01%) MobileNet-v1 70.89% 89.56% 71.29% (+0.40%) 89.79% (+0.23%) MobileNet-v2 71.84% 90.60% 72.26% (+0.42%) 90.77% (+0.17%) Model Hardware CPU Time (32-bit) Time (8-bit) Speed-up MobileNet-v1 XiaoMi 8 SE Snapdragon 710 156.33 62.60 2.50 \\times \\times MobileNet-v1 XiaoMI 8 Snapdragon 845 124.53 56.12 2.22 \\times \\times MobileNet-v1 Huawei P20 Kirin 970 152.54 68.43 2.23 \\times \\times MobileNet-v2 XiaoMi 8 SE Snapdragon 710 153.18 57.55 2.66 \\times \\times MobileNet-v2 XiaoMi 8 Snapdragon 845 120.59 49.04 2.46 \\times \\times MobileNet-v2 Huawei P20 Kirin 970 226.61 61.38 3.69 \\times \\times All the reported time are in milliseconds.","title":"Performance"},{"location":"performance/#performance","text":"In this documentation, we present evaluation results for applying various model compression methods for ResNet and MobileNet models on the ImageNet classification task, including channel pruning, weight sparsification, and uniform quantization. We adopt ChannelPrunedLearner to shrink the number of channels for convolutional layers to reduce the computation complexity. Instead of using the same pruning ratio for all layers, we utilize the DDPG algorithm as the RL agent to iteratively search for the optimal pruning ratio of each layer. After obtaining the optimal pruning ratios, group fine-tuning is adopted to further improve the compressed model's accuracy, as demonstrated below: Model Pruning Ratio Uniform RL-based RL-based + Group Fine-tuning MobileNet-v1 50% 66.5% 67.8% (+1.3%) 67.9% (+1.4%) MobileNet-v1 60% 66.2% 66.9% (+0.7%) 67.0% (+0.8%) MobileNet-v1 70% 64.4% 64.5% (+0.1%) 64.8% (+0.4%) Mobilenet-v1 80% 61.4% 61.4% (+0.0%) 62.2% (+0.8%) We adopt WeightSparseLearner to introduce the sparsity constraint so that a large portion of model weights can be removed, which leads to smaller model and lower FLOPs for inference. Comparing with the original algorithm proposed in (Zhu & Gupta, 2017), we also incorporate network distillation and reinforcement learning algorithms to further improve the compressed model's accuracy, as shown in the table below: Model Sparsity (Zhu & Gupta, 2017) RL-based MobileNet-v1 50% 69.5% 70.5% (+1.0%) MobileNet-v1 75% 67.7% 68.5% (+0.8%) MobileNet-v1 90% 61.8% 63.4% (+1.6%) MobileNet-v1 95% 53.6% 56.8% (+3.2%) We adopt UniformQuantTFLearner to uniformly quantize model weights from 32-bit floating-point numbers to 8-bit fixed-point numbers. The resulting model can be converted into the TensorFlow Lite format for deployment on mobile devices. In the following two tables, we show that 8-bit quantized models can be as accurate as (or even better than) the original 32-bit ones, and the inference time can be significantly reduced after quantization. Model Top-1 Acc. (32-bit) Top-5 Acc. (32-bit) Top-1 Acc. (8-bit) Top-5 Acc. (8-bit) ResNet-18 70.28% 89.38% 70.31% (+0.03%) 89.40% (+0.02%) ResNet-50 75.97% 92.88% 76.01% (+0.04%) 92.87% (-0.01%) MobileNet-v1 70.89% 89.56% 71.29% (+0.40%) 89.79% (+0.23%) MobileNet-v2 71.84% 90.60% 72.26% (+0.42%) 90.77% (+0.17%) Model Hardware CPU Time (32-bit) Time (8-bit) Speed-up MobileNet-v1 XiaoMi 8 SE Snapdragon 710 156.33 62.60 2.50 \\times \\times MobileNet-v1 XiaoMI 8 Snapdragon 845 124.53 56.12 2.22 \\times \\times MobileNet-v1 Huawei P20 Kirin 970 152.54 68.43 2.23 \\times \\times MobileNet-v2 XiaoMi 8 SE Snapdragon 710 153.18 57.55 2.66 \\times \\times MobileNet-v2 XiaoMi 8 Snapdragon 845 120.59 49.04 2.46 \\times \\times MobileNet-v2 Huawei P20 Kirin 970 226.61 61.38 3.69 \\times \\times All the reported time are in milliseconds.","title":"Performance"},{"location":"pre_trained_models/","text":"Pre-trained Models We maintain a list of pre-trained uncompressed models, so that the training process of model compression does not need to start from scratch. For the CIFAR-10 data set, we provide following pre-trained models: Model name Accuracy URL LeNet 81.79% Link ResNet-20 91.93% Link ResNet-32 92.59% Link ResNet-44 92.76% Link ResNet-56 93.23% Link For the ImageNet (ILSVRC-12) data set, we provide following pre-trained models: Model name Top-1 Acc. Top-5 Acc. URL ResNet-18 70.28% 89.38% Link ResNet-34 73.41% 91.27% Link ResNet-50 75.97% 92.88% Link MobileNet-v1 70.89% 89.56% Link MobileNet-v2 71.84% 90.60% Link","title":"Pre-trained Models"},{"location":"pre_trained_models/#pre-trained-models","text":"We maintain a list of pre-trained uncompressed models, so that the training process of model compression does not need to start from scratch. For the CIFAR-10 data set, we provide following pre-trained models: Model name Accuracy URL LeNet 81.79% Link ResNet-20 91.93% Link ResNet-32 92.59% Link ResNet-44 92.76% Link ResNet-56 93.23% Link For the ImageNet (ILSVRC-12) data set, we provide following pre-trained models: Model name Top-1 Acc. Top-5 Acc. URL ResNet-18 70.28% 89.38% Link ResNet-34 73.41% 91.27% Link ResNet-50 75.97% 92.88% Link MobileNet-v1 70.89% 89.56% Link MobileNet-v2 71.84% 90.60% Link","title":"Pre-trained Models"},{"location":"reference/","text":"Reference [ Bengio et al., 2015 ] Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation . CoRR, abs/1308.3432, 2013. [ Bergstra et al., 2013 ] J. Bergstra, D. Yamins, and D. D. Cox. Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures . In International Conference on Machine Learning (ICML), pages 115-123, Jun 2013. [ Han et al., 2016 ] Song Han, Huizi Mao, and William J. Dally. Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding . In International Conference on Learning Representations (ICLR), 2016. [ He et al., 2017 ] Yihui He, Xiangyu Zhang, and Jian Sun. Channel Pruning for Accelerating Very Deep Neural Networks . In IEEE International Conference on Computer Vision (ICCV), pages 1389-1397, 2017. [ He et al., 2018 ] Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. AMC: AutoML for Model Compression and Acceleration on Mobile Devices . In European Conference on Computer Vision (ECCV), pages 784-800, 2018. [ Hinton et al., 2015 ] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the Knowledge in a Neural Network . CoRR, abs/1503.02531, 2015. [ Jacob et al., 2018 ] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference . In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2704-2713, 2018. [ Lillicrap et al., 2016 ] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous Control with Deep Reinforcement Learning . In International Conference on Learning Representations (ICLR), 2016. [ Mockus, 1975 ] J. Mockus. On Bayesian Methods for Seeking the Extremum . In Optimization Techniques IFIP Technical Conference, pages 400-404, 1975. [ Zhu & Gupta, 2017 ] Michael Zhu and Suyog Gupta. To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression . CoRR, abs/1710.01878, 2017. [ Zhuang et al., 2018 ] Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Jiezhang Cao, Qingyao Wu, Junzhou Huang, and Jinhui Zhu. Discrimination-aware Channel Pruning for Deep Neural Networks . In Annual Conference on Neural Information Processing Systems (NIPS), 2018.","title":"Reference"},{"location":"reference/#reference","text":"[ Bengio et al., 2015 ] Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation . CoRR, abs/1308.3432, 2013. [ Bergstra et al., 2013 ] J. Bergstra, D. Yamins, and D. D. Cox. Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures . In International Conference on Machine Learning (ICML), pages 115-123, Jun 2013. [ Han et al., 2016 ] Song Han, Huizi Mao, and William J. Dally. Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding . In International Conference on Learning Representations (ICLR), 2016. [ He et al., 2017 ] Yihui He, Xiangyu Zhang, and Jian Sun. Channel Pruning for Accelerating Very Deep Neural Networks . In IEEE International Conference on Computer Vision (ICCV), pages 1389-1397, 2017. [ He et al., 2018 ] Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. AMC: AutoML for Model Compression and Acceleration on Mobile Devices . In European Conference on Computer Vision (ECCV), pages 784-800, 2018. [ Hinton et al., 2015 ] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the Knowledge in a Neural Network . CoRR, abs/1503.02531, 2015. [ Jacob et al., 2018 ] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference . In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2704-2713, 2018. [ Lillicrap et al., 2016 ] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous Control with Deep Reinforcement Learning . In International Conference on Learning Representations (ICLR), 2016. [ Mockus, 1975 ] J. Mockus. On Bayesian Methods for Seeking the Extremum . In Optimization Techniques IFIP Technical Conference, pages 400-404, 1975. [ Zhu & Gupta, 2017 ] Michael Zhu and Suyog Gupta. To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression . CoRR, abs/1710.01878, 2017. [ Zhuang et al., 2018 ] Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Jiezhang Cao, Qingyao Wu, Junzhou Huang, and Jinhui Zhu. Discrimination-aware Channel Pruning for Deep Neural Networks . In Annual Conference on Neural Information Processing Systems (NIPS), 2018.","title":"Reference"},{"location":"reinforcement_learning/","text":"Reinforcement Learning For most deep learning models, the parameter redundancy differs from one layer to another. Some layers may be more robust to model compression algorithms due to larger redundancy, while others may be more sensitive. Therefore, it is often sub-optimal to use a unified pruning ratio or number of quantization bits for all layers, which completely omits the redundancy difference. However, it is also time-consuming or even impractical to manually setup the optimal value of such hyper-parameter for each layer, especially for deep networks with tens or hundreds of layers. To overcome this dilemma, in PocketFlow, we adopt reinforcement learning to automatically determine the optimal pruning ratio or number of quantization bits for each layer. Our approach is innovated from (He et al., 2018), which automatically determines each layer's optimal pruning ratio, and generalize it to hyper-parameter optimization for more model compression methods. In this documentation, we take UniformQuantLearner as an example to explain how the reinforcement learning method is used to iteratively optimize the number of quantization bits for each layer. It is worthy mentioning that this feature is also available for ChannelPrunedLearner , WeightSparseLearner , and NonUniformQuantLearner . Algorithm Description Here, we assume the original model to be compressed consists of T T layers, and denote the t t -th layer's weight tensor as \\mathbf{W}_{t} \\mathbf{W}_{t} and its quantization bit-width as b_{t} b_{t} . In order to maximally exploit the parameter redundancy of each layer, we need to find the optimal combination of layer-wise quantization bit-width that achieves the highest accuracy after compression while satisfying: \\sum_{t = 1}^{T} b_{t} \\left| \\mathbf{W}_{t} \\right| \\le b \\cdot \\sum_{t = 1}^{T} \\left| \\mathbf{W}_{t} \\right| \\sum_{t = 1}^{T} b_{t} \\left| \\mathbf{W}_{t} \\right| \\le b \\cdot \\sum_{t = 1}^{T} \\left| \\mathbf{W}_{t} \\right| where \\left| \\mathbf{W}_{t} \\right| \\left| \\mathbf{W}_{t} \\right| denotes the number of parameters in the weight tensor \\mathbf{W}_{t} \\mathbf{W}_{t} and b b is the whole network's target quantization bit-width. Below, we present the overall workflow of adopting reinforcement learning, or more specifically, the DDPG algorithm (Lillicrap et al., 2016) to search for the optimal combination of layer-wise quantization bit-width: To start with, we initialize an DDPG agent and set the best reward r_{best} r_{best} to negative infinity to track the optimal combination of layer-wise quantization bit-width. The search process consists of multiple roll-outs. In each roll-out, we sequentially traverse each layer in the network to determine its quantization bit-width. For the t t -th layer, we construct its state vector with following information: one-hot embedding of layer index shape of weight tensor number of parameters in the weight tensor number of quantization bits used by previous layers budget of quantization bits for remaining layers Afterwards, we feed this state vector into the DDPG agent to choose an action, which is then converted into the quantization bit-width under certain constraints. A commonly-used constraint is that with the selected quantization bit-width, the budget of quantization bits for remaining layers should be sufficient, e.g. ensuring the minimal quantization bit-width can be satisfied. After obtaining all layer's quantization bit-width, we quantize each layer's weights with the corresponding quantization bit-width, and fine-tune the quantized network for a few iteration (as supported by each learner's \"Fast Fine-tuning\" mode). We then evaluate the fine-tuned network' accuracy and use it as the reward signal r_{n} r_{n} . The reward signal is compared against the best reward discovered so far, and the optimal combination of layer-wise quantization bit-width is updated if the current reward is larger. Finally, we generate a list of transitions from all the \\left( \\mathbf{s}_{t}, a_{t}, r_{t}, \\mathbf{s}_{t + 1} \\right) \\left( \\mathbf{s}_{t}, a_{t}, r_{t}, \\mathbf{s}_{t + 1} \\right) tuples in the roll-out, and store them in the DDPG agent's replay buffer. The DDPG agent is then trained with one or more mini-batches of sampled transitions, so that it can choose better actions in the following roll-outs. After obtaining the optimal combination of layer-wise quantization bit-width, we can optionally use UniformQuantLearner 's \"Re-training with Full Data\" mode (also supported by others learners) for a complete quantization-aware training to further reduce the accuracy loss.","title":"Reinforcement Learning"},{"location":"reinforcement_learning/#reinforcement-learning","text":"For most deep learning models, the parameter redundancy differs from one layer to another. Some layers may be more robust to model compression algorithms due to larger redundancy, while others may be more sensitive. Therefore, it is often sub-optimal to use a unified pruning ratio or number of quantization bits for all layers, which completely omits the redundancy difference. However, it is also time-consuming or even impractical to manually setup the optimal value of such hyper-parameter for each layer, especially for deep networks with tens or hundreds of layers. To overcome this dilemma, in PocketFlow, we adopt reinforcement learning to automatically determine the optimal pruning ratio or number of quantization bits for each layer. Our approach is innovated from (He et al., 2018), which automatically determines each layer's optimal pruning ratio, and generalize it to hyper-parameter optimization for more model compression methods. In this documentation, we take UniformQuantLearner as an example to explain how the reinforcement learning method is used to iteratively optimize the number of quantization bits for each layer. It is worthy mentioning that this feature is also available for ChannelPrunedLearner , WeightSparseLearner , and NonUniformQuantLearner .","title":"Reinforcement Learning"},{"location":"reinforcement_learning/#algorithm-description","text":"Here, we assume the original model to be compressed consists of T T layers, and denote the t t -th layer's weight tensor as \\mathbf{W}_{t} \\mathbf{W}_{t} and its quantization bit-width as b_{t} b_{t} . In order to maximally exploit the parameter redundancy of each layer, we need to find the optimal combination of layer-wise quantization bit-width that achieves the highest accuracy after compression while satisfying: \\sum_{t = 1}^{T} b_{t} \\left| \\mathbf{W}_{t} \\right| \\le b \\cdot \\sum_{t = 1}^{T} \\left| \\mathbf{W}_{t} \\right| \\sum_{t = 1}^{T} b_{t} \\left| \\mathbf{W}_{t} \\right| \\le b \\cdot \\sum_{t = 1}^{T} \\left| \\mathbf{W}_{t} \\right| where \\left| \\mathbf{W}_{t} \\right| \\left| \\mathbf{W}_{t} \\right| denotes the number of parameters in the weight tensor \\mathbf{W}_{t} \\mathbf{W}_{t} and b b is the whole network's target quantization bit-width. Below, we present the overall workflow of adopting reinforcement learning, or more specifically, the DDPG algorithm (Lillicrap et al., 2016) to search for the optimal combination of layer-wise quantization bit-width: To start with, we initialize an DDPG agent and set the best reward r_{best} r_{best} to negative infinity to track the optimal combination of layer-wise quantization bit-width. The search process consists of multiple roll-outs. In each roll-out, we sequentially traverse each layer in the network to determine its quantization bit-width. For the t t -th layer, we construct its state vector with following information: one-hot embedding of layer index shape of weight tensor number of parameters in the weight tensor number of quantization bits used by previous layers budget of quantization bits for remaining layers Afterwards, we feed this state vector into the DDPG agent to choose an action, which is then converted into the quantization bit-width under certain constraints. A commonly-used constraint is that with the selected quantization bit-width, the budget of quantization bits for remaining layers should be sufficient, e.g. ensuring the minimal quantization bit-width can be satisfied. After obtaining all layer's quantization bit-width, we quantize each layer's weights with the corresponding quantization bit-width, and fine-tune the quantized network for a few iteration (as supported by each learner's \"Fast Fine-tuning\" mode). We then evaluate the fine-tuned network' accuracy and use it as the reward signal r_{n} r_{n} . The reward signal is compared against the best reward discovered so far, and the optimal combination of layer-wise quantization bit-width is updated if the current reward is larger. Finally, we generate a list of transitions from all the \\left( \\mathbf{s}_{t}, a_{t}, r_{t}, \\mathbf{s}_{t + 1} \\right) \\left( \\mathbf{s}_{t}, a_{t}, r_{t}, \\mathbf{s}_{t + 1} \\right) tuples in the roll-out, and store them in the DDPG agent's replay buffer. The DDPG agent is then trained with one or more mini-batches of sampled transitions, so that it can choose better actions in the following roll-outs. After obtaining the optimal combination of layer-wise quantization bit-width, we can optionally use UniformQuantLearner 's \"Re-training with Full Data\" mode (also supported by others learners) for a complete quantization-aware training to further reduce the accuracy loss.","title":"Algorithm Description"},{"location":"self_defined_models/","text":"Self-defined Models Self-defined models (and data sets) can be incorporated into PocketFlow by implementing a new ModelHelper class. The ModelHelper class includes the definition of data input pipeline as well as the network's forward pass and loss function. With the self-defined ModelHelper , the network can be either trained without any constraints using FullPrecLearner , or trained with certain model compression algorithms using other learners, e.g. ChannelPrunedLearner for channel pruning or UniformQuantTFLearner for uniform quantization. In this tutorial, we will define a 4-layer convolutional neural network (2 conv. layers + 2 dense layers) for image classification on the Fashion-MNIST data set under the PocketFlow framework. Afterwards, we shall demonstrate how to train this self-defined model with different model compression components. The Essentials To use self-defined models and data sets in PocketFlow, we need to provide the following two items in advance to describe the overall training workflow: Data Input Pipeline : this tells PocketFlow how to parse features and ground-truth labels from data files. Network Definition : this tells PocketFlow how to compute the network's predictions and loss function's value. The ModelHelper class, which is a sub-class of the abstract base class AbstractModelHelper , is designed to provide such definitions. In PocketFlow, we have offered several ModelHelper classes to describe different combinations of data sets and model architectures. To use self-defined models, a new ModelHelper class should be implemented. Besides, we need an execution script to call this newly defined ModelHelper class. P.S.: You can find the full code used in this tutorial under the \"./examples\" directory. Data Input Pipeline To start with, we need to tell PocketFlow how data files should be parsed. Here, we define a class named FMnistDataset to create iterators over the Fashion-MNIST training and test subsets. Every time the iterator is called, it will return a mini-batch of images and corresponding ground-truth labels. Below is the full implementation of FMnistDataset class (this should be placed under the \"./datasets\" directory, named as \"fmnist_dataset.py\"): import os import gzip import numpy as np import tensorflow as tf from datasets.abstract_dataset import AbstractDataset FLAGS = tf.app.flags.FLAGS tf.app.flags.DEFINE_integer('nb_classes', 10, '# of classes') tf.app.flags.DEFINE_integer('nb_smpls_train', 60000, '# of samples for training') tf.app.flags.DEFINE_integer('nb_smpls_val', 5000, '# of samples for validation') tf.app.flags.DEFINE_integer('nb_smpls_eval', 10000, '# of samples for evaluation') tf.app.flags.DEFINE_integer('batch_size', 128, 'batch size per GPU for training') tf.app.flags.DEFINE_integer('batch_size_eval', 100, 'batch size for evaluation') # Fashion-MNIST specifications IMAGE_HEI = 28 IMAGE_WID = 28 IMAGE_CHN = 1 def load_mnist(image_file, label_file): \"\"\"Load images and labels from *.gz files. This function is modified from utils/mnist_reader.py in the Fashion-MNIST repo. Args: * image_file: file path to images * label_file: file path to labels Returns: * images: np.array of the image data * labels: np.array of the label data \"\"\" with gzip.open(label_file, 'rb') as i_file: labels = np.frombuffer(i_file.read(), dtype=np.uint8, offset=8) with gzip.open(image_file, 'rb') as i_file: images = np.frombuffer(i_file.read(), dtype=np.uint8, offset=16) image_size = IMAGE_HEI * IMAGE_WID * IMAGE_CHN assert images.size == image_size * len(labels) images = images.reshape(len(labels), image_size) return images, labels def parse_fn(image, label, is_train): \"\"\"Parse an (image, label) pair and apply data augmentation if needed. Args: * image: image tensor * label: label tensor * is_train: whether data augmentation should be applied Returns: * image: image tensor * label: one-hot label tensor \"\"\" # data parsing label = tf.one_hot(tf.reshape(label, []), FLAGS.nb_classes) image = tf.cast(tf.reshape(image, [IMAGE_HEI, IMAGE_WID, IMAGE_CHN]), tf.float32) image = tf.image.per_image_standardization(image) # data augmentation if is_train: image = tf.image.resize_image_with_crop_or_pad(image, IMAGE_HEI + 8, IMAGE_WID + 8) image = tf.random_crop(image, [IMAGE_HEI, IMAGE_WID, IMAGE_CHN]) image = tf.image.random_flip_left_right(image) return image, label class FMnistDataset(AbstractDataset): '''Fashion-MNIST dataset.''' def __init__(self, is_train): \"\"\"Constructor function. Args: * is_train: whether to construct the training subset \"\"\" # initialize the base class super(FMnistDataset, self).__init__(is_train) # choose local files or HDFS files w.r.t. FLAGS.data_disk if FLAGS.data_disk == 'local': assert FLAGS.data_dir_local is not None, '<FLAGS.data_dir_local> must not be None' data_dir = FLAGS.data_dir_local elif FLAGS.data_disk == 'hdfs': assert FLAGS.data_hdfs_host is not None and FLAGS.data_dir_hdfs is not None, \\ 'both <FLAGS.data_hdfs_host> and <FLAGS.data_dir_hdfs> must not be None' data_dir = FLAGS.data_hdfs_host + FLAGS.data_dir_hdfs else: raise ValueError('unrecognized data disk: ' + FLAGS.data_disk) # setup paths to image & label files, and read in images & labels if is_train: self.batch_size = FLAGS.batch_size image_file = os.path.join(data_dir, 'train-images-idx3-ubyte.gz') label_file = os.path.join(data_dir, 'train-labels-idx1-ubyte.gz') else: self.batch_size = FLAGS.batch_size_eval image_file = os.path.join(data_dir, 't10k-images-idx3-ubyte.gz') label_file = os.path.join(data_dir, 't10k-labels-idx1-ubyte.gz') self.images, self.labels = load_mnist(image_file, label_file) self.parse_fn = lambda x, y: parse_fn(x, y, is_train) def build(self, enbl_trn_val_split=False): \"\"\"Build iterator(s) for tf.data.Dataset() object. Args: * enbl_trn_val_split: whether to split into training & validation subsets Returns: * iterator_trn: iterator for the training subset * iterator_val: iterator for the validation subset OR * iterator: iterator for the chosen subset (training OR testing) \"\"\" # create a tf.data.Dataset() object from NumPy arrays dataset = tf.data.Dataset.from_tensor_slices((self.images, self.labels)) dataset = dataset.map(self.parse_fn, num_parallel_calls=FLAGS.nb_threads) # create iterators for training & validation subsets separately if self.is_train and enbl_trn_val_split: iterator_val = self.__make_iterator(dataset.take(FLAGS.nb_smpls_val)) iterator_trn = self.__make_iterator(dataset.skip(FLAGS.nb_smpls_val)) return iterator_trn, iterator_val return self.__make_iterator(dataset) def __make_iterator(self, dataset): \"\"\"Make an iterator from tf.data.Dataset. Args: * dataset: tf.data.Dataset object Returns: * iterator: iterator for the dataset \"\"\" dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(buffer_size=FLAGS.buffer_size)) dataset = dataset.batch(self.batch_size) dataset = dataset.prefetch(FLAGS.prefetch_size) iterator = dataset.make_one_shot_iterator() return iterator When creating an object of FMnistDataset class, an extra argument named is_train should be provided to toggle between the training and test subsets. The data files can be either store on the local machine or the HDFS cluster, and the directory path is specified in the path configuration file, e.g. : data_dir_local_fmnist = /home/user_name/datasets/Fashion-MNIST The constructor function loads images and labels from *.gz files, each stored in a NumPy array. The build function is then used to create a TensorFlow's data set iterator from these two NumPy arrays. Particularly, if both enbl_trn_val_split and is_train are True, then the original training subset will be divided into two parts, one for model training and the other for validation. Network Definition Now we implement a new ModelHelper class to utilize the above data input pipeline to define the network's training workflow. Below is the full implementation of ModelHelper class (this should be placed under the \"./nets\" directory, named as \"convnet_at_fmnist.py\"): import tensorflow as tf from nets.abstract_model_helper import AbstractModelHelper from datasets.fmnist_dataset import FMnistDataset from utils.lrn_rate_utils import setup_lrn_rate_piecewise_constant from utils.multi_gpu_wrapper import MultiGpuWrapper as mgw FLAGS = tf.app.flags.FLAGS tf.app.flags.DEFINE_float('nb_epochs_rat', 1.0, '# of training epochs\\' ratio') tf.app.flags.DEFINE_float('lrn_rate_init', 1e-1, 'initial learning rate') tf.app.flags.DEFINE_float('batch_size_norm', 128, 'normalization factor of batch size') tf.app.flags.DEFINE_float('momentum', 0.9, 'momentum coefficient') tf.app.flags.DEFINE_float('loss_w_dcy', 3e-4, 'weight decaying loss\\'s coefficient') def forward_fn(inputs, data_format): \"\"\"Forward pass function. Args: * inputs: inputs to the network's forward pass * data_format: data format ('channels_last' OR 'channels_first') Returns: * inputs: outputs from the network's forward pass \"\"\" # transpose the image tensor if needed if data_format == 'channel_first': inputs = tf.transpose(inputs, [0, 3, 1, 2]) # conv1 inputs = tf.layers.conv2d(inputs, 32, [5, 5], padding='same', data_format=data_format, activation=tf.nn.relu, name='conv1') inputs = tf.layers.max_pooling2d(inputs, [2, 2], 2, data_format=data_format, name='pool1') # conv2 inputs = tf.layers.conv2d(inputs, 64, [5, 5], padding='same', data_format=data_format, activation=tf.nn.relu, name='conv2') inputs = tf.layers.max_pooling2d(inputs, [2, 2], 2, data_format=data_format, name='pool2') # fc3 inputs = tf.layers.flatten(inputs, name='flatten') inputs = tf.layers.dense(inputs, 1024, activation=tf.nn.relu, name='fc3') # fc4 inputs = tf.layers.dense(inputs, FLAGS.nb_classes, name='fc4') inputs = tf.nn.softmax(inputs, name='softmax') return inputs class ModelHelper(AbstractModelHelper): \"\"\"Model helper for creating a ConvNet model for the Fashion-MNIST dataset.\"\"\" def __init__(self): \"\"\"Constructor function.\"\"\" # class-independent initialization super(ModelHelper, self).__init__() # initialize training & evaluation subsets self.dataset_train = FMnistDataset(is_train=True) self.dataset_eval = FMnistDataset(is_train=False) def build_dataset_train(self, enbl_trn_val_split=False): \"\"\"Build the data subset for training, usually with data augmentation.\"\"\" return self.dataset_train.build(enbl_trn_val_split) def build_dataset_eval(self): \"\"\"Build the data subset for evaluation, usually without data augmentation.\"\"\" return self.dataset_eval.build() def forward_train(self, inputs, data_format='channels_last'): \"\"\"Forward computation at training.\"\"\" return forward_fn(inputs, data_format) def forward_eval(self, inputs, data_format='channels_last'): \"\"\"Forward computation at evaluation.\"\"\" return forward_fn(inputs, data_format) def calc_loss(self, labels, outputs, trainable_vars): \"\"\"Calculate loss (and some extra evaluation metrics).\"\"\" loss = tf.losses.softmax_cross_entropy(labels, outputs) loss += FLAGS.loss_w_dcy * tf.add_n([tf.nn.l2_loss(var) for var in trainable_vars]) accuracy = tf.reduce_mean( tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(outputs, axis=1)), tf.float32)) metrics = {'accuracy': accuracy} return loss, metrics def setup_lrn_rate(self, global_step): \"\"\"Setup the learning rate (and number of training iterations).\"\"\" nb_epochs = 160 idxs_epoch = [40, 80, 120] decay_rates = [1.0, 0.1, 0.01, 0.001] batch_size = FLAGS.batch_size * (1 if not FLAGS.enbl_multi_gpu else mgw.size()) lrn_rate = setup_lrn_rate_piecewise_constant(global_step, batch_size, idxs_epoch, decay_rates) nb_iters = int(FLAGS.nb_smpls_train * nb_epochs * FLAGS.nb_epochs_rat / batch_size) return lrn_rate, nb_iters @property def model_name(self): \"\"\"Model's name.\"\"\" return 'convnet' @property def dataset_name(self): \"\"\"Dataset's name.\"\"\" return 'fmnist' In the build_dataset_train and build_dataset_eval functions, we adopt the previously introduced FMnistDataset class to define the data input pipeline. The network forward-pass computation is defined in the forward_train and forward_eval functions, which corresponds to the training and evaluation graph, respectively. The training graph is slightly different from evaluation graph, such as operations related to the batch normalization layers. The calc_loss function calculates the loss function's value and extra evaluation metrics, e.g. classification accuracy. Finally, the setup_lrn_rate function defines the learning rate schedule, as well as how many training iterations are need. Execution Script Besides the self-defined ModelHelper class, we still need an execution script to pass it to the corresponding model compression component to start the training process. Below is the full implementation (this should be placed under the \"./nets\" directory, named as \"convnet_at_fmnist_run.py\"): import traceback import tensorflow as tf from nets.convnet_at_fmnist import ModelHelper from learners.learner_utils import create_learner FLAGS = tf.app.flags.FLAGS tf.app.flags.DEFINE_string('log_dir', './logs', 'logging directory') tf.app.flags.DEFINE_boolean('enbl_multi_gpu', False, 'enable multi-GPU training') tf.app.flags.DEFINE_string('learner', 'full-prec', 'learner\\'s name') tf.app.flags.DEFINE_string('exec_mode', 'train', 'execution mode: train / eval') tf.app.flags.DEFINE_boolean('debug', False, 'debugging information') def main(unused_argv): \"\"\"Main entry.\"\"\" try: # setup the TF logging routine if FLAGS.debug: tf.logging.set_verbosity(tf.logging.DEBUG) else: tf.logging.set_verbosity(tf.logging.INFO) sm_writer = tf.summary.FileWriter(FLAGS.log_dir) # display FLAGS's values tf.logging.info('FLAGS:') for key, value in FLAGS.flag_values_dict().items(): tf.logging.info('{}: {}'.format(key, value)) # build the model helper & learner model_helper = ModelHelper() learner = create_learner(sm_writer, model_helper) # execute the learner if FLAGS.exec_mode == 'train': learner.train() elif FLAGS.exec_mode == 'eval': learner.download_model() learner.evaluate() else: raise ValueError('unrecognized execution mode: ' + FLAGS.exec_mode) # exit normally return 0 except ValueError: traceback.print_exc() return 1 # exit with errors if __name__ == '__main__': tf.app.run() Network Training with PocketFlow To train the self-defined model without any constraint, use FullPrecLearner : $ ./scripts/run_local.sh nets/convnet_at_fmnist_run.py \\ --learner full-prec To train the self-defined model with the uniform quantization constraint, use UniformQuantTFLearner : $ ./scripts/run_local.sh nets/convnet_at_fmnist_run.py \\ --learner uniform-tf","title":"Self-defined Models"},{"location":"self_defined_models/#self-defined-models","text":"Self-defined models (and data sets) can be incorporated into PocketFlow by implementing a new ModelHelper class. The ModelHelper class includes the definition of data input pipeline as well as the network's forward pass and loss function. With the self-defined ModelHelper , the network can be either trained without any constraints using FullPrecLearner , or trained with certain model compression algorithms using other learners, e.g. ChannelPrunedLearner for channel pruning or UniformQuantTFLearner for uniform quantization. In this tutorial, we will define a 4-layer convolutional neural network (2 conv. layers + 2 dense layers) for image classification on the Fashion-MNIST data set under the PocketFlow framework. Afterwards, we shall demonstrate how to train this self-defined model with different model compression components.","title":"Self-defined Models"},{"location":"self_defined_models/#the-essentials","text":"To use self-defined models and data sets in PocketFlow, we need to provide the following two items in advance to describe the overall training workflow: Data Input Pipeline : this tells PocketFlow how to parse features and ground-truth labels from data files. Network Definition : this tells PocketFlow how to compute the network's predictions and loss function's value. The ModelHelper class, which is a sub-class of the abstract base class AbstractModelHelper , is designed to provide such definitions. In PocketFlow, we have offered several ModelHelper classes to describe different combinations of data sets and model architectures. To use self-defined models, a new ModelHelper class should be implemented. Besides, we need an execution script to call this newly defined ModelHelper class. P.S.: You can find the full code used in this tutorial under the \"./examples\" directory.","title":"The Essentials"},{"location":"self_defined_models/#data-input-pipeline","text":"To start with, we need to tell PocketFlow how data files should be parsed. Here, we define a class named FMnistDataset to create iterators over the Fashion-MNIST training and test subsets. Every time the iterator is called, it will return a mini-batch of images and corresponding ground-truth labels. Below is the full implementation of FMnistDataset class (this should be placed under the \"./datasets\" directory, named as \"fmnist_dataset.py\"): import os import gzip import numpy as np import tensorflow as tf from datasets.abstract_dataset import AbstractDataset FLAGS = tf.app.flags.FLAGS tf.app.flags.DEFINE_integer('nb_classes', 10, '# of classes') tf.app.flags.DEFINE_integer('nb_smpls_train', 60000, '# of samples for training') tf.app.flags.DEFINE_integer('nb_smpls_val', 5000, '# of samples for validation') tf.app.flags.DEFINE_integer('nb_smpls_eval', 10000, '# of samples for evaluation') tf.app.flags.DEFINE_integer('batch_size', 128, 'batch size per GPU for training') tf.app.flags.DEFINE_integer('batch_size_eval', 100, 'batch size for evaluation') # Fashion-MNIST specifications IMAGE_HEI = 28 IMAGE_WID = 28 IMAGE_CHN = 1 def load_mnist(image_file, label_file): \"\"\"Load images and labels from *.gz files. This function is modified from utils/mnist_reader.py in the Fashion-MNIST repo. Args: * image_file: file path to images * label_file: file path to labels Returns: * images: np.array of the image data * labels: np.array of the label data \"\"\" with gzip.open(label_file, 'rb') as i_file: labels = np.frombuffer(i_file.read(), dtype=np.uint8, offset=8) with gzip.open(image_file, 'rb') as i_file: images = np.frombuffer(i_file.read(), dtype=np.uint8, offset=16) image_size = IMAGE_HEI * IMAGE_WID * IMAGE_CHN assert images.size == image_size * len(labels) images = images.reshape(len(labels), image_size) return images, labels def parse_fn(image, label, is_train): \"\"\"Parse an (image, label) pair and apply data augmentation if needed. Args: * image: image tensor * label: label tensor * is_train: whether data augmentation should be applied Returns: * image: image tensor * label: one-hot label tensor \"\"\" # data parsing label = tf.one_hot(tf.reshape(label, []), FLAGS.nb_classes) image = tf.cast(tf.reshape(image, [IMAGE_HEI, IMAGE_WID, IMAGE_CHN]), tf.float32) image = tf.image.per_image_standardization(image) # data augmentation if is_train: image = tf.image.resize_image_with_crop_or_pad(image, IMAGE_HEI + 8, IMAGE_WID + 8) image = tf.random_crop(image, [IMAGE_HEI, IMAGE_WID, IMAGE_CHN]) image = tf.image.random_flip_left_right(image) return image, label class FMnistDataset(AbstractDataset): '''Fashion-MNIST dataset.''' def __init__(self, is_train): \"\"\"Constructor function. Args: * is_train: whether to construct the training subset \"\"\" # initialize the base class super(FMnistDataset, self).__init__(is_train) # choose local files or HDFS files w.r.t. FLAGS.data_disk if FLAGS.data_disk == 'local': assert FLAGS.data_dir_local is not None, '<FLAGS.data_dir_local> must not be None' data_dir = FLAGS.data_dir_local elif FLAGS.data_disk == 'hdfs': assert FLAGS.data_hdfs_host is not None and FLAGS.data_dir_hdfs is not None, \\ 'both <FLAGS.data_hdfs_host> and <FLAGS.data_dir_hdfs> must not be None' data_dir = FLAGS.data_hdfs_host + FLAGS.data_dir_hdfs else: raise ValueError('unrecognized data disk: ' + FLAGS.data_disk) # setup paths to image & label files, and read in images & labels if is_train: self.batch_size = FLAGS.batch_size image_file = os.path.join(data_dir, 'train-images-idx3-ubyte.gz') label_file = os.path.join(data_dir, 'train-labels-idx1-ubyte.gz') else: self.batch_size = FLAGS.batch_size_eval image_file = os.path.join(data_dir, 't10k-images-idx3-ubyte.gz') label_file = os.path.join(data_dir, 't10k-labels-idx1-ubyte.gz') self.images, self.labels = load_mnist(image_file, label_file) self.parse_fn = lambda x, y: parse_fn(x, y, is_train) def build(self, enbl_trn_val_split=False): \"\"\"Build iterator(s) for tf.data.Dataset() object. Args: * enbl_trn_val_split: whether to split into training & validation subsets Returns: * iterator_trn: iterator for the training subset * iterator_val: iterator for the validation subset OR * iterator: iterator for the chosen subset (training OR testing) \"\"\" # create a tf.data.Dataset() object from NumPy arrays dataset = tf.data.Dataset.from_tensor_slices((self.images, self.labels)) dataset = dataset.map(self.parse_fn, num_parallel_calls=FLAGS.nb_threads) # create iterators for training & validation subsets separately if self.is_train and enbl_trn_val_split: iterator_val = self.__make_iterator(dataset.take(FLAGS.nb_smpls_val)) iterator_trn = self.__make_iterator(dataset.skip(FLAGS.nb_smpls_val)) return iterator_trn, iterator_val return self.__make_iterator(dataset) def __make_iterator(self, dataset): \"\"\"Make an iterator from tf.data.Dataset. Args: * dataset: tf.data.Dataset object Returns: * iterator: iterator for the dataset \"\"\" dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(buffer_size=FLAGS.buffer_size)) dataset = dataset.batch(self.batch_size) dataset = dataset.prefetch(FLAGS.prefetch_size) iterator = dataset.make_one_shot_iterator() return iterator When creating an object of FMnistDataset class, an extra argument named is_train should be provided to toggle between the training and test subsets. The data files can be either store on the local machine or the HDFS cluster, and the directory path is specified in the path configuration file, e.g. : data_dir_local_fmnist = /home/user_name/datasets/Fashion-MNIST The constructor function loads images and labels from *.gz files, each stored in a NumPy array. The build function is then used to create a TensorFlow's data set iterator from these two NumPy arrays. Particularly, if both enbl_trn_val_split and is_train are True, then the original training subset will be divided into two parts, one for model training and the other for validation.","title":"Data Input Pipeline"},{"location":"self_defined_models/#network-definition","text":"Now we implement a new ModelHelper class to utilize the above data input pipeline to define the network's training workflow. Below is the full implementation of ModelHelper class (this should be placed under the \"./nets\" directory, named as \"convnet_at_fmnist.py\"): import tensorflow as tf from nets.abstract_model_helper import AbstractModelHelper from datasets.fmnist_dataset import FMnistDataset from utils.lrn_rate_utils import setup_lrn_rate_piecewise_constant from utils.multi_gpu_wrapper import MultiGpuWrapper as mgw FLAGS = tf.app.flags.FLAGS tf.app.flags.DEFINE_float('nb_epochs_rat', 1.0, '# of training epochs\\' ratio') tf.app.flags.DEFINE_float('lrn_rate_init', 1e-1, 'initial learning rate') tf.app.flags.DEFINE_float('batch_size_norm', 128, 'normalization factor of batch size') tf.app.flags.DEFINE_float('momentum', 0.9, 'momentum coefficient') tf.app.flags.DEFINE_float('loss_w_dcy', 3e-4, 'weight decaying loss\\'s coefficient') def forward_fn(inputs, data_format): \"\"\"Forward pass function. Args: * inputs: inputs to the network's forward pass * data_format: data format ('channels_last' OR 'channels_first') Returns: * inputs: outputs from the network's forward pass \"\"\" # transpose the image tensor if needed if data_format == 'channel_first': inputs = tf.transpose(inputs, [0, 3, 1, 2]) # conv1 inputs = tf.layers.conv2d(inputs, 32, [5, 5], padding='same', data_format=data_format, activation=tf.nn.relu, name='conv1') inputs = tf.layers.max_pooling2d(inputs, [2, 2], 2, data_format=data_format, name='pool1') # conv2 inputs = tf.layers.conv2d(inputs, 64, [5, 5], padding='same', data_format=data_format, activation=tf.nn.relu, name='conv2') inputs = tf.layers.max_pooling2d(inputs, [2, 2], 2, data_format=data_format, name='pool2') # fc3 inputs = tf.layers.flatten(inputs, name='flatten') inputs = tf.layers.dense(inputs, 1024, activation=tf.nn.relu, name='fc3') # fc4 inputs = tf.layers.dense(inputs, FLAGS.nb_classes, name='fc4') inputs = tf.nn.softmax(inputs, name='softmax') return inputs class ModelHelper(AbstractModelHelper): \"\"\"Model helper for creating a ConvNet model for the Fashion-MNIST dataset.\"\"\" def __init__(self): \"\"\"Constructor function.\"\"\" # class-independent initialization super(ModelHelper, self).__init__() # initialize training & evaluation subsets self.dataset_train = FMnistDataset(is_train=True) self.dataset_eval = FMnistDataset(is_train=False) def build_dataset_train(self, enbl_trn_val_split=False): \"\"\"Build the data subset for training, usually with data augmentation.\"\"\" return self.dataset_train.build(enbl_trn_val_split) def build_dataset_eval(self): \"\"\"Build the data subset for evaluation, usually without data augmentation.\"\"\" return self.dataset_eval.build() def forward_train(self, inputs, data_format='channels_last'): \"\"\"Forward computation at training.\"\"\" return forward_fn(inputs, data_format) def forward_eval(self, inputs, data_format='channels_last'): \"\"\"Forward computation at evaluation.\"\"\" return forward_fn(inputs, data_format) def calc_loss(self, labels, outputs, trainable_vars): \"\"\"Calculate loss (and some extra evaluation metrics).\"\"\" loss = tf.losses.softmax_cross_entropy(labels, outputs) loss += FLAGS.loss_w_dcy * tf.add_n([tf.nn.l2_loss(var) for var in trainable_vars]) accuracy = tf.reduce_mean( tf.cast(tf.equal(tf.argmax(labels, axis=1), tf.argmax(outputs, axis=1)), tf.float32)) metrics = {'accuracy': accuracy} return loss, metrics def setup_lrn_rate(self, global_step): \"\"\"Setup the learning rate (and number of training iterations).\"\"\" nb_epochs = 160 idxs_epoch = [40, 80, 120] decay_rates = [1.0, 0.1, 0.01, 0.001] batch_size = FLAGS.batch_size * (1 if not FLAGS.enbl_multi_gpu else mgw.size()) lrn_rate = setup_lrn_rate_piecewise_constant(global_step, batch_size, idxs_epoch, decay_rates) nb_iters = int(FLAGS.nb_smpls_train * nb_epochs * FLAGS.nb_epochs_rat / batch_size) return lrn_rate, nb_iters @property def model_name(self): \"\"\"Model's name.\"\"\" return 'convnet' @property def dataset_name(self): \"\"\"Dataset's name.\"\"\" return 'fmnist' In the build_dataset_train and build_dataset_eval functions, we adopt the previously introduced FMnistDataset class to define the data input pipeline. The network forward-pass computation is defined in the forward_train and forward_eval functions, which corresponds to the training and evaluation graph, respectively. The training graph is slightly different from evaluation graph, such as operations related to the batch normalization layers. The calc_loss function calculates the loss function's value and extra evaluation metrics, e.g. classification accuracy. Finally, the setup_lrn_rate function defines the learning rate schedule, as well as how many training iterations are need.","title":"Network Definition"},{"location":"self_defined_models/#execution-script","text":"Besides the self-defined ModelHelper class, we still need an execution script to pass it to the corresponding model compression component to start the training process. Below is the full implementation (this should be placed under the \"./nets\" directory, named as \"convnet_at_fmnist_run.py\"): import traceback import tensorflow as tf from nets.convnet_at_fmnist import ModelHelper from learners.learner_utils import create_learner FLAGS = tf.app.flags.FLAGS tf.app.flags.DEFINE_string('log_dir', './logs', 'logging directory') tf.app.flags.DEFINE_boolean('enbl_multi_gpu', False, 'enable multi-GPU training') tf.app.flags.DEFINE_string('learner', 'full-prec', 'learner\\'s name') tf.app.flags.DEFINE_string('exec_mode', 'train', 'execution mode: train / eval') tf.app.flags.DEFINE_boolean('debug', False, 'debugging information') def main(unused_argv): \"\"\"Main entry.\"\"\" try: # setup the TF logging routine if FLAGS.debug: tf.logging.set_verbosity(tf.logging.DEBUG) else: tf.logging.set_verbosity(tf.logging.INFO) sm_writer = tf.summary.FileWriter(FLAGS.log_dir) # display FLAGS's values tf.logging.info('FLAGS:') for key, value in FLAGS.flag_values_dict().items(): tf.logging.info('{}: {}'.format(key, value)) # build the model helper & learner model_helper = ModelHelper() learner = create_learner(sm_writer, model_helper) # execute the learner if FLAGS.exec_mode == 'train': learner.train() elif FLAGS.exec_mode == 'eval': learner.download_model() learner.evaluate() else: raise ValueError('unrecognized execution mode: ' + FLAGS.exec_mode) # exit normally return 0 except ValueError: traceback.print_exc() return 1 # exit with errors if __name__ == '__main__': tf.app.run()","title":"Execution Script"},{"location":"self_defined_models/#network-training-with-pocketflow","text":"To train the self-defined model without any constraint, use FullPrecLearner : $ ./scripts/run_local.sh nets/convnet_at_fmnist_run.py \\ --learner full-prec To train the self-defined model with the uniform quantization constraint, use UniformQuantTFLearner : $ ./scripts/run_local.sh nets/convnet_at_fmnist_run.py \\ --learner uniform-tf","title":"Network Training with PocketFlow"},{"location":"test_cases/","text":"Test Cases This document contains various test cases to cover different combinations of learners and hyper-parameter settings. Any merge request to the master branch should be able to pass all the test cases to be approved. Full-Precision # local mode $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --enbl_dst $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --data_disk hdfs $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --data_disk hdfs \\ --enbl_dst # seven mode $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py \\ --enbl_dst $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py \\ --data_disk hdfs $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py \\ --data_disk hdfs \\ --enbl_dst # docker mode $ ./scripts/run_docker.sh nets/lenet_at_cifar10_run.py $ ./scripts/run_docker.sh nets/lenet_at_cifar10_run.py \\ --enbl_dst $ ./scripts/run_docker.sh nets/resnet_at_cifar10_run.py $ ./scripts/run_docker.sh nets/resnet_at_cifar10_run.py \\ --enbl_dst Channel Pruning # uniform preserve ratios for all layers $ ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --learner channel \\ --cp_prune_option uniform \\ --cp_uniform_preserve_ratio 0.5 # auto-tuned preserve ratios for each layer $ ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --cp_learner channel \\ --cp_prune_option auto \\ --cp_preserve_ratio 0.3 Discrimination-aware Channel Pruning # no network distillation $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner dis-chn-pruned \\ --dcp_nb_stages 3 \\ --data_disk hdfs # network distillation $ ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py \\ --learner dis-chn-pruned \\ --enbl_dst \\ --dcp_nb_stages 4 Weight Sparsification # uniform pruning ratios for all layers $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner weight-sparse \\ --ws_prune_ratio_prtl uniform \\ --data_disk hdfs # optimal pruning ratios for each layer $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner weight-sparse \\ --ws_prune_ratio_prtl optimal \\ --data_disk hdfs # heurist pruning ratios for each layer $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py \\ --learner weight-sparse \\ --ws_prune_ratio_prtl heurist # optimal pruning ratios for each layer $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py \\ --learner weight-sparse \\ --ws_prune_ratio_prtl optimal Uniform Quantization # channel-based bucketing $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner uniform \\ --uql_use_buckets \\ --uql_bucket_type channel \\ --data_disk hdfs # split-based bucketing $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner uniform \\ --uql_use_buckets \\ --uql_bucket_type split \\ --data_disk hdfs # channel-based bucketing + RL $ ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py -n=2 \\ --learner uniform \\ --uql_enbl_rl_agent \\ --uql_use_buckets \\ --uql_bucket_type channel # split-based bucketing + RL $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py -n=2 \\ --learner uniform \\ --uql_enbl_rl_agent \\ --uql_use_buckets \\ --uql_bucket_type split Non-uniform Quantization # channel-based bucketing + RL + optimize clusters $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner non-uniform \\ --nuql_enbl_rl_agent \\ --nuql_use_buckets \\ --nuql_bucket_type channel \\ --nuql_opt_mode clusters \\ --data_disk hdfs # split-based bucketing + RL + optimize weights $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner non-uniform \\ --nuql_enbl_rl_agent \\ --nuql_use_buckets \\ --nuql_bucket_type split \\ --nuql_opt_mode weights \\ --data_disk hdfs # channel-based bucketing + RL + optimize weights $ ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py -n=2 \\ --learner non-uniform \\ --nuql_enbl_rl_agent \\ --nuql_use_buckets \\ --nuql_bucket_type channel \\ --nuql_opt_mode weights # split-based bucketing + RL + optimize clusters $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py -n=2 \\ --learner non-uniform \\ --nuql_enbl_rl_agent \\ --nuql_use_buckets \\ --nuql_bucket_type split \\ --nuql_opt_mode clusters","title":"Test Cases"},{"location":"test_cases/#test-cases","text":"This document contains various test cases to cover different combinations of learners and hyper-parameter settings. Any merge request to the master branch should be able to pass all the test cases to be approved.","title":"Test Cases"},{"location":"test_cases/#full-precision","text":"# local mode $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --enbl_dst $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --data_disk hdfs $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --data_disk hdfs \\ --enbl_dst # seven mode $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py \\ --enbl_dst $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py \\ --data_disk hdfs $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py \\ --data_disk hdfs \\ --enbl_dst # docker mode $ ./scripts/run_docker.sh nets/lenet_at_cifar10_run.py $ ./scripts/run_docker.sh nets/lenet_at_cifar10_run.py \\ --enbl_dst $ ./scripts/run_docker.sh nets/resnet_at_cifar10_run.py $ ./scripts/run_docker.sh nets/resnet_at_cifar10_run.py \\ --enbl_dst","title":"Full-Precision"},{"location":"test_cases/#channel-pruning","text":"# uniform preserve ratios for all layers $ ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --learner channel \\ --cp_prune_option uniform \\ --cp_uniform_preserve_ratio 0.5 # auto-tuned preserve ratios for each layer $ ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --cp_learner channel \\ --cp_prune_option auto \\ --cp_preserve_ratio 0.3","title":"Channel Pruning"},{"location":"test_cases/#discrimination-aware-channel-pruning","text":"# no network distillation $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner dis-chn-pruned \\ --dcp_nb_stages 3 \\ --data_disk hdfs # network distillation $ ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py \\ --learner dis-chn-pruned \\ --enbl_dst \\ --dcp_nb_stages 4","title":"Discrimination-aware Channel Pruning"},{"location":"test_cases/#weight-sparsification","text":"# uniform pruning ratios for all layers $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner weight-sparse \\ --ws_prune_ratio_prtl uniform \\ --data_disk hdfs # optimal pruning ratios for each layer $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner weight-sparse \\ --ws_prune_ratio_prtl optimal \\ --data_disk hdfs # heurist pruning ratios for each layer $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py \\ --learner weight-sparse \\ --ws_prune_ratio_prtl heurist # optimal pruning ratios for each layer $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py \\ --learner weight-sparse \\ --ws_prune_ratio_prtl optimal","title":"Weight Sparsification"},{"location":"test_cases/#uniform-quantization","text":"# channel-based bucketing $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner uniform \\ --uql_use_buckets \\ --uql_bucket_type channel \\ --data_disk hdfs # split-based bucketing $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner uniform \\ --uql_use_buckets \\ --uql_bucket_type split \\ --data_disk hdfs # channel-based bucketing + RL $ ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py -n=2 \\ --learner uniform \\ --uql_enbl_rl_agent \\ --uql_use_buckets \\ --uql_bucket_type channel # split-based bucketing + RL $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py -n=2 \\ --learner uniform \\ --uql_enbl_rl_agent \\ --uql_use_buckets \\ --uql_bucket_type split","title":"Uniform Quantization"},{"location":"test_cases/#non-uniform-quantization","text":"# channel-based bucketing + RL + optimize clusters $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner non-uniform \\ --nuql_enbl_rl_agent \\ --nuql_use_buckets \\ --nuql_bucket_type channel \\ --nuql_opt_mode clusters \\ --data_disk hdfs # split-based bucketing + RL + optimize weights $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner non-uniform \\ --nuql_enbl_rl_agent \\ --nuql_use_buckets \\ --nuql_bucket_type split \\ --nuql_opt_mode weights \\ --data_disk hdfs # channel-based bucketing + RL + optimize weights $ ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py -n=2 \\ --learner non-uniform \\ --nuql_enbl_rl_agent \\ --nuql_use_buckets \\ --nuql_bucket_type channel \\ --nuql_opt_mode weights # split-based bucketing + RL + optimize clusters $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py -n=2 \\ --learner non-uniform \\ --nuql_enbl_rl_agent \\ --nuql_use_buckets \\ --nuql_bucket_type split \\ --nuql_opt_mode clusters","title":"Non-uniform Quantization"},{"location":"tutorial/","text":"Tutorial In this tutorial, we demonstrate how to compress a convolutional neural network and export the compressed model into a *.tflite file for deployment on mobile devices. The model we used here is a 18-layer residual network (denoted as \"ResNet-18\") trained for the ImageNet classification task. We will compress it with the discrimination-aware channel pruning algorithm (Zhuang et al., NIPS '18) to reduce the number of convolutional channels used in the network for speed-up. Prepare the Data To start with, we need to convert the ImageNet data set (ILSVRC-12) into TensorFlow's native TFRecord file format. You may follow the data preparation guide here to download the full data set and convert it into TFRecord files. After that, you should be able to find 1,024 training files and 128 validation files in the data directory, like this: # training files train-00000-of-01024 train-00001-of-01024 ... train-01023-of-01024 # validation files validation-00000-of-00128 validation-00001-of-00128 ... validation-00127-of-00128 Prepare the Pre-trained Model The discrimination-aware channel pruning algorithm requires a pre-trained uncompressed model provided in advance, so that a channel-pruned model can be trained with warm-start. You can download a pre-trained model from here , and then unzip files into the models sub-directory. Alternatively, you can train an uncompressed full-precision model from scratch using FullPrecLearner with the following command (choose whatever mode that fits you): # local mode with 1 GPU $ ./scripts/run_local.sh nets/resnet_at_ilsvrc12_run.py # docker mode with 8 GPUs $ ./scripts/run_docker.sh nets/resnet_at_ilsvrc12_run.py -n=8 # seven mode with 8 GPUs $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py -n=8 After the training process, you should be able to find the resulting model files located at the models sub-directory in PocketFlow's home directory. Train the Compressed Model Now, we can train a compressed model with the discrimination-aware channel pruning algorithm, as implemented by DisChnPrunedLearner . Assuming you are now in PocketFlow's home directory, the training process of model compression can be started using the following command (choose whatever mode that fits you): # local mode with 1 GPU $ ./scripts/run_local.sh nets/resnet_at_ilsvrc12_run.py \\ --learner dis-chn-pruned # docker mode with 8 GPUs $ ./scripts/run_docker.sh nets/resnet_at_ilsvrc12_run.py -n=8 \\ --learner dis-chn-pruned # seven mode with 8 GPUs $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py -n=8 \\ --learner dis-chn-pruned Let's take the execution command for the local mode as an example. In this command, run_local.sh is a shell script that executes the specified Python script with user-provided arguments. Here, we ask it to run the Python script named nets/resnet_at_ilsvrc12_run.py , which is the execution script for ResNet models on the ImageNet data set. After that, we use --learner dis-chn-pruned to specify that the DisChnPrunedLearner should be used for model compression. You may also use other learners by specifying the corresponding learner name. Below is a full list of available learners in PocketFlow: Learner name Learner class Note full-prec FullPrecLearner No model compression channel ChannelPrunedLearner Channel pruning with LASSO-based channel selection (He et al., 2017) dis-chn-pruned DisChnPrunedLearner Discrimination-aware channel pruning (Zhuang et al., 2018) weight-sparse WeightSparseLearner Weight sparsification with dynamic pruning schedule (Zhu & Gupta, 2017) uniform UniformQuantLearner Weight quantization with uniform reconstruction levels (Jacob et al., 2018) uniform-tf UniformQuantTFLearner Weight quantization with uniform reconstruction levels and TensorFlow APIs non-uniform NonUniformQuantLearner Weight quantization with non-uniform reconstruction levels (Han et al., 2016) The local mode only uses 1 GPU for the training process, which takes approximately 20-30 hours to complete. This can be accelerated by multi-GPU training in the docker and seven mode, which is enabled by adding -n=x right after the specified Python script, where x is the number of GPUs to be used. Optionally, you can pass some extra arguments to customize the training process. For the discrimination-aware channel pruning algorithm, some of key arguments are: Name Definition Default Value enbl_dst Enable training with distillation loss False dcp_prune_ratio DCP algorithm's pruning ratio 0.5 You may override the default value by appending customized arguments at the end of the execution command. For instance, the following command: $ ./scripts/run_local.sh nets/resnet_at_ilsvrc12_run.py \\ --learner dis-chn-pruned \\ --enbl_dst \\ --dcp_prune_ratio 0.75 requires the DisChnPrunedLearner to achieve an overall pruning ratio of 0.75 and the training process will be carried out with the distillation loss. As a result, the number of channels in each convolutional layer of the compressed model will be one quarter of the original one. After the training process is completed, you should be able to find a sub-directory named models_dcp_eval created in the home directory of PocketFlow. This sub-directory contains all the files that define the compressed model, and we will export them to a TensorFlow Lite formatted model file for deployment in the next section. Export to TensorFlow Lite TensorFlow's checkpoint files cannot be directly used for deployment on mobile devices. Instead, we need to firstly convert them into a single *.tflite file that is supported by the TensorFlow Lite Interpreter. For model compressed with channel-pruning based algorithms, e.g. ChannelPruningLearner and DisChnPrunedLearner , we have prepared a model conversion script, tools/conversion/export_pb_tflite_models.py , to generate a TF-Lite model from TensorFlow's checkpoint files. To convert checkpoint files into a *.tflite file, use the following command: # convert checkpoint files into a *.tflite model $ python tools/conversion/export_pb_tflite_models.py \\ --model_dir models_dcp_eval In the above command, we specify the model directory containing checkpoint files generated in the previous training process. The conversion script automatically detects which channels can be safely pruned, and then produces a light-weighted compressed model. The resulting TensorFlow Lite file is also placed at the models_dcp_eval directory, named as model_transformed.tflite . Deploy on Mobile Devices After exporting the compressed model to the TensorFlow Lite file format, you may follow the official guide for creating an Android demo App from it. Basically, this demo App uses a TensorFlow Lite model to continuously classifies images captured by the camera, and all the computation are performed on mobile devices in real time. To use the model_transformed.tflite model file, you need to place it in the asserts directory and create a Java class named ImageClassifierFloatResNet to use this model for classification. Below is the example code, which is modified from ImageClassifierFloatInception.java used in the official demo project: /* Copyright 2017 The TensorFlow Authors. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. ==============================================================================*/ package com.example.android.tflitecamerademo; import android.app.Activity; import java.io.IOException; /** * This classifier works with the ResNet-18 model. * It applies floating point inference rather than using a quantized model. */ public class ImageClassifierFloatResNet extends ImageClassifier { /** * The ResNet requires additional normalization of the used input. */ private static final float IMAGE_MEAN_RED = 123.58f; private static final float IMAGE_MEAN_GREEN = 116.779f; private static final float IMAGE_MEAN_BLUE = 103.939f; /** * An array to hold inference results, to be feed into Tensorflow Lite as outputs. * This isn't part of the super class, because we need a primitive array here. */ private float[][] labelProbArray = null; /** * Initializes an {@code ImageClassifier}. * * @param activity */ ImageClassifierFloatResNet(Activity activity) throws IOException { super(activity); labelProbArray = new float[1][getNumLabels()]; } @Override protected String getModelPath() { return \"model_transformed.tflite\"; } @Override protected String getLabelPath() { return \"labels_imagenet_slim.txt\"; } @Override protected int getImageSizeX() { return 224; } @Override protected int getImageSizeY() { return 224; } @Override protected int getNumBytesPerChannel() { // a 32bit float value requires 4 bytes return 4; } @Override protected void addPixelValue(int pixelValue) { imgData.putFloat(((pixelValue >> 16) & 0xFF) - IMAGE_MEAN_RED); imgData.putFloat(((pixelValue >> 8) & 0xFF) - IMAGE_MEAN_GREEN); imgData.putFloat((pixelValue & 0xFF) - IMAGE_MEAN_BLUE); } @Override protected float getProbability(int labelIndex) { return labelProbArray[0][labelIndex]; } @Override protected void setProbability(int labelIndex, Number value) { labelProbArray[0][labelIndex] = value.floatValue(); } @Override protected float getNormalizedProbability(int labelIndex) { // TODO the following value isn't in [0,1] yet, but may be greater. Why? return getProbability(labelIndex); } @Override protected void runInference() { tflite.run(imgData, labelProbArray); } } After that, you need to change the image classifier class used in Camera2BasicFragment.java . Locate the function named onActivityCreated and change its content as below. Now you will be able to use the compressed ResNet-18 model to classify objects on your mobile phone in real time. /** Load the model and labels. */ @Override public void onActivityCreated(Bundle savedInstanceState) { super.onActivityCreated(savedInstanceState); try { classifier = new ImageClassifierFloatResNet(getActivity()); } catch (IOException e) { Log.e(TAG, \"Failed to initialize an image classifier.\", e); } startBackgroundThread(); }","title":"Tutorial"},{"location":"tutorial/#tutorial","text":"In this tutorial, we demonstrate how to compress a convolutional neural network and export the compressed model into a *.tflite file for deployment on mobile devices. The model we used here is a 18-layer residual network (denoted as \"ResNet-18\") trained for the ImageNet classification task. We will compress it with the discrimination-aware channel pruning algorithm (Zhuang et al., NIPS '18) to reduce the number of convolutional channels used in the network for speed-up.","title":"Tutorial"},{"location":"tutorial/#prepare-the-data","text":"To start with, we need to convert the ImageNet data set (ILSVRC-12) into TensorFlow's native TFRecord file format. You may follow the data preparation guide here to download the full data set and convert it into TFRecord files. After that, you should be able to find 1,024 training files and 128 validation files in the data directory, like this: # training files train-00000-of-01024 train-00001-of-01024 ... train-01023-of-01024 # validation files validation-00000-of-00128 validation-00001-of-00128 ... validation-00127-of-00128","title":"Prepare the Data"},{"location":"tutorial/#prepare-the-pre-trained-model","text":"The discrimination-aware channel pruning algorithm requires a pre-trained uncompressed model provided in advance, so that a channel-pruned model can be trained with warm-start. You can download a pre-trained model from here , and then unzip files into the models sub-directory. Alternatively, you can train an uncompressed full-precision model from scratch using FullPrecLearner with the following command (choose whatever mode that fits you): # local mode with 1 GPU $ ./scripts/run_local.sh nets/resnet_at_ilsvrc12_run.py # docker mode with 8 GPUs $ ./scripts/run_docker.sh nets/resnet_at_ilsvrc12_run.py -n=8 # seven mode with 8 GPUs $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py -n=8 After the training process, you should be able to find the resulting model files located at the models sub-directory in PocketFlow's home directory.","title":"Prepare the Pre-trained Model"},{"location":"tutorial/#train-the-compressed-model","text":"Now, we can train a compressed model with the discrimination-aware channel pruning algorithm, as implemented by DisChnPrunedLearner . Assuming you are now in PocketFlow's home directory, the training process of model compression can be started using the following command (choose whatever mode that fits you): # local mode with 1 GPU $ ./scripts/run_local.sh nets/resnet_at_ilsvrc12_run.py \\ --learner dis-chn-pruned # docker mode with 8 GPUs $ ./scripts/run_docker.sh nets/resnet_at_ilsvrc12_run.py -n=8 \\ --learner dis-chn-pruned # seven mode with 8 GPUs $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py -n=8 \\ --learner dis-chn-pruned Let's take the execution command for the local mode as an example. In this command, run_local.sh is a shell script that executes the specified Python script with user-provided arguments. Here, we ask it to run the Python script named nets/resnet_at_ilsvrc12_run.py , which is the execution script for ResNet models on the ImageNet data set. After that, we use --learner dis-chn-pruned to specify that the DisChnPrunedLearner should be used for model compression. You may also use other learners by specifying the corresponding learner name. Below is a full list of available learners in PocketFlow: Learner name Learner class Note full-prec FullPrecLearner No model compression channel ChannelPrunedLearner Channel pruning with LASSO-based channel selection (He et al., 2017) dis-chn-pruned DisChnPrunedLearner Discrimination-aware channel pruning (Zhuang et al., 2018) weight-sparse WeightSparseLearner Weight sparsification with dynamic pruning schedule (Zhu & Gupta, 2017) uniform UniformQuantLearner Weight quantization with uniform reconstruction levels (Jacob et al., 2018) uniform-tf UniformQuantTFLearner Weight quantization with uniform reconstruction levels and TensorFlow APIs non-uniform NonUniformQuantLearner Weight quantization with non-uniform reconstruction levels (Han et al., 2016) The local mode only uses 1 GPU for the training process, which takes approximately 20-30 hours to complete. This can be accelerated by multi-GPU training in the docker and seven mode, which is enabled by adding -n=x right after the specified Python script, where x is the number of GPUs to be used. Optionally, you can pass some extra arguments to customize the training process. For the discrimination-aware channel pruning algorithm, some of key arguments are: Name Definition Default Value enbl_dst Enable training with distillation loss False dcp_prune_ratio DCP algorithm's pruning ratio 0.5 You may override the default value by appending customized arguments at the end of the execution command. For instance, the following command: $ ./scripts/run_local.sh nets/resnet_at_ilsvrc12_run.py \\ --learner dis-chn-pruned \\ --enbl_dst \\ --dcp_prune_ratio 0.75 requires the DisChnPrunedLearner to achieve an overall pruning ratio of 0.75 and the training process will be carried out with the distillation loss. As a result, the number of channels in each convolutional layer of the compressed model will be one quarter of the original one. After the training process is completed, you should be able to find a sub-directory named models_dcp_eval created in the home directory of PocketFlow. This sub-directory contains all the files that define the compressed model, and we will export them to a TensorFlow Lite formatted model file for deployment in the next section.","title":"Train the Compressed Model"},{"location":"tutorial/#export-to-tensorflow-lite","text":"TensorFlow's checkpoint files cannot be directly used for deployment on mobile devices. Instead, we need to firstly convert them into a single *.tflite file that is supported by the TensorFlow Lite Interpreter. For model compressed with channel-pruning based algorithms, e.g. ChannelPruningLearner and DisChnPrunedLearner , we have prepared a model conversion script, tools/conversion/export_pb_tflite_models.py , to generate a TF-Lite model from TensorFlow's checkpoint files. To convert checkpoint files into a *.tflite file, use the following command: # convert checkpoint files into a *.tflite model $ python tools/conversion/export_pb_tflite_models.py \\ --model_dir models_dcp_eval In the above command, we specify the model directory containing checkpoint files generated in the previous training process. The conversion script automatically detects which channels can be safely pruned, and then produces a light-weighted compressed model. The resulting TensorFlow Lite file is also placed at the models_dcp_eval directory, named as model_transformed.tflite .","title":"Export to TensorFlow Lite"},{"location":"tutorial/#deploy-on-mobile-devices","text":"After exporting the compressed model to the TensorFlow Lite file format, you may follow the official guide for creating an Android demo App from it. Basically, this demo App uses a TensorFlow Lite model to continuously classifies images captured by the camera, and all the computation are performed on mobile devices in real time. To use the model_transformed.tflite model file, you need to place it in the asserts directory and create a Java class named ImageClassifierFloatResNet to use this model for classification. Below is the example code, which is modified from ImageClassifierFloatInception.java used in the official demo project: /* Copyright 2017 The TensorFlow Authors. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. ==============================================================================*/ package com.example.android.tflitecamerademo; import android.app.Activity; import java.io.IOException; /** * This classifier works with the ResNet-18 model. * It applies floating point inference rather than using a quantized model. */ public class ImageClassifierFloatResNet extends ImageClassifier { /** * The ResNet requires additional normalization of the used input. */ private static final float IMAGE_MEAN_RED = 123.58f; private static final float IMAGE_MEAN_GREEN = 116.779f; private static final float IMAGE_MEAN_BLUE = 103.939f; /** * An array to hold inference results, to be feed into Tensorflow Lite as outputs. * This isn't part of the super class, because we need a primitive array here. */ private float[][] labelProbArray = null; /** * Initializes an {@code ImageClassifier}. * * @param activity */ ImageClassifierFloatResNet(Activity activity) throws IOException { super(activity); labelProbArray = new float[1][getNumLabels()]; } @Override protected String getModelPath() { return \"model_transformed.tflite\"; } @Override protected String getLabelPath() { return \"labels_imagenet_slim.txt\"; } @Override protected int getImageSizeX() { return 224; } @Override protected int getImageSizeY() { return 224; } @Override protected int getNumBytesPerChannel() { // a 32bit float value requires 4 bytes return 4; } @Override protected void addPixelValue(int pixelValue) { imgData.putFloat(((pixelValue >> 16) & 0xFF) - IMAGE_MEAN_RED); imgData.putFloat(((pixelValue >> 8) & 0xFF) - IMAGE_MEAN_GREEN); imgData.putFloat((pixelValue & 0xFF) - IMAGE_MEAN_BLUE); } @Override protected float getProbability(int labelIndex) { return labelProbArray[0][labelIndex]; } @Override protected void setProbability(int labelIndex, Number value) { labelProbArray[0][labelIndex] = value.floatValue(); } @Override protected float getNormalizedProbability(int labelIndex) { // TODO the following value isn't in [0,1] yet, but may be greater. Why? return getProbability(labelIndex); } @Override protected void runInference() { tflite.run(imgData, labelProbArray); } } After that, you need to change the image classifier class used in Camera2BasicFragment.java . Locate the function named onActivityCreated and change its content as below. Now you will be able to use the compressed ResNet-18 model to classify objects on your mobile phone in real time. /** Load the model and labels. */ @Override public void onActivityCreated(Bundle savedInstanceState) { super.onActivityCreated(savedInstanceState); try { classifier = new ImageClassifierFloatResNet(getActivity()); } catch (IOException e) { Log.e(TAG, \"Failed to initialize an image classifier.\", e); } startBackgroundThread(); }","title":"Deploy on Mobile Devices"},{"location":"uq_learner/","text":"Uniform Quantization Introduction Uniform quantization is widely used for model compression and acceleration. Originally the weights in the network are represented by 32-bit floating-point numbers. With uniform quantization, low-precision ( e.g. 4-bit or 8-bit) fixed-point numbers are used to approximate the full-precision network. For k k -bit quantization, the memory saving can be up to 32 / k\u200b 32 / k\u200b . For example, 8-bit quantization can reduce the network size by 4 folds with negligible drop of performance. Currently, PocketFlow supports two types of uniform quantization learners: UniformQuantLearner : a self-developed learner for uniform quantization. The learner is carefully optimized with various extensions and variations supported. UniformQuantTFLearner : a wrapper based on TensorFlow's quantization-aware training training APIs. For now, this wrapper only supports 8-bit quantization, which leads to approximately 4x memory reduction and 3x inference speed-up. A comparison of these two learners are shown below: Features UniformQuantLearner UniformQuantTFLearner Compression Yes Yes Acceleration Yes Fine-tuning Yes Bucketing Yes Hyper-param Optimization Yes Algorithm Training Workflow Both two uniform quantization learners generally follow the training workflow below: Given a pre-defined full-precision model, the learner inserts quantization nodes and operations into the computation graph of the model. With activation quantization enabled, quantization nodes will also be placed after activation operations ( e.g. ReLU). In the training phase, both full-precision and quantized weights are kept. In the forward pass, quantized weights are obtained by applying the quantization function on full-precision weights. To update full-precision weights in the backward pass, since gradients w.r.t. quantized weights are zeros almost everywhere, we use the straight-through estimator (STE, Bengio et al., 2015) to pass gradients of quantized weights directly to full-precision weights for update. Quantization Function Uniform quantization distributes all the quantization points evenly across the range \\left[ w_{min}, w_{max} \\right] \\left[ w_{min}, w_{max} \\right] , where w_{max} w_{max} and w_{min} w_{min} are the maximum and minimum values of weights in each layer (or bucket). The original full-precision weights are then assigned to their closest quantization points. To achieve this, we first normalize the full-precision weights x x to \\left[ 0, 1 \\right] \\left[ 0, 1 \\right] : \\text{sc} \\left( x \\right) = \\frac{ x - \\beta}{\\alpha}, \\text{sc} \\left( x \\right) = \\frac{ x - \\beta}{\\alpha}, where \\alpha = w_{max} - w_{min} \\alpha = w_{max} - w_{min} and \\beta = w_{min} \\beta = w_{min} . Then, we assign \\text{sc} \\left( x \\right) \\text{sc} \\left( x \\right) to its closest quantization point (assuming k k -bit quantization is used): \\hat{x} = \\frac{1}{2^{k} - 1} \\text{round} \\left( \\left( 2^{k} - 1 \\right) \\cdot \\text{sc} \\left( x \\right) \\right), \\hat{x} = \\frac{1}{2^{k} - 1} \\text{round} \\left( \\left( 2^{k} - 1 \\right) \\cdot \\text{sc} \\left( x \\right) \\right), and finally we use inverse linear transformation to recover the quantized weights to the original scale: Q \\left( x \\right) = \\alpha \\cdot \\hat{x} + \\beta. Q \\left( x \\right) = \\alpha \\cdot \\hat{x} + \\beta. UniformQuantLearner UniformQuantLearner is a self-developed learner, which allows a number of customized configurations for uniform quantization. For example, the learner supports bucketing, leading to more fine-grained quantization and better performance. The learner also allows to allocate different bits across layers, in which users can turn on the hyper-parameter optimizer with reinforcement learning to search for the optimal bit allocation strategy. Hyper-parameters To configure UniformQuantLearner , users can pass options via the TensorFlow flag interface. The available options are listed as follows: Option Description uql_weight_bits the number of bits for weights. Default: 4 . uql_activation_bits the number of bits for activation. Default: 32. uql_save_quant_model_path quantized model's save path. Default: ./uql_quant_models/model.ckpt uql_use_buckets the switch to use bucketing. Default: False. uql_bucket_type two bucket type available: [ split , channel ]. Default: channel. uql_bucket_size the number of bucket size for bucket type split . Default: 256 . uql_quantize_all_layers the switch to quantize first and last layers of network. Default: False. uql_quant_epoch the number of epochs for fine-tuning. Default: 60 . uql_enbl_rl_agent the switch to enable RL to learn optimal bit strategy. Default: False . Here, we provide detailed description (and some analysis) for above hyper-parameters: uql_weight_bits : The number of bits for weight quantization. Generally, 8 bit does not hurt the model performance while it can compress the model size by 4 folds. While 2 bit and 4 bit could lead to drop of performance on large datasets such as Imagenet. uql_activation_bits : The number of bits for activation quantization. When both weights and activations are quantized, 8 bit does not lead to apparent drop of performance, and sometimes can even increase the classification accuracy, which is probably due to better generalization ability. Nevertheless, the performance will be more challenged when both weights and activations are quantized to lower bits, comparing to weight-only quantization. uql_save_quant_mode_path : the path to save the quantized model. Quantization nodes have already been inserted into the graph. uql_use_buckets : the switch to turn on the bucket. With bucketing, weights are split into multiple pieces, while the \\alpha \\alpha and \\beta \\beta are calculated individually for each piece. Therefore, turning on the bucketing can lead to more fine-grained quantization. uql_bucket_type : the type of bucketing. Currently two types are supported: [ split , channel ]. split refers to that the weights of a layer are first concatenated into a long vector, and then cut it into pieces according to uql_bucket_size . The remaining last piece will be padded and taken as a new piece. After quantization of each piece, the vectors are then folded back to the original shape as the quantized weights. channel refers to that weights with shape [k, k, cin, cout] in a convolutional layer are cut into cout buckets, where each bucket has the size of k * k * cin . For weights with shape [m, n] in fully connected layers, they are cut into n buckets, each of size m . In practice, bucketing with type channel can be calculated more efficiently comparing to type split since there are less buckets and less computation to iterate through all of them. uql_bucket_size : the size of buckets when using bucket type split . Generally, smaller bucket size can lead to more fine grained quantization, while more storage are required since full precision statistics ( \\alpha \\alpha and \\beta \\beta ) of each bucket need to be kept. uql_quantize_all_layers : the switch to quantize the first and last layers. The first and last layers of the network are connected directly with the input and output, and are arguably more sensitive to quantization. Keeping them un-quantized can slightly increase the performance, nevertheless, if you want to accelerate the inference speed, all layers are supposed to be quantized. uql_quant_epoch : the epochs for fine-tuning a quantized network. uql_enbl_rl_agent : the switch to turn on the RL agent as hyper parameter optimizer. Details about the RL agent and its configurations are described below. Configure the RL Agent Once the hyper parameter optimizer is turned on, i.e., uql_enbl_rl_agent==True , the RL agent will automatically search for the optimal bit allocation strategy for each layer. In order to search efficiently, the agent need to be configured properly. While here we list all the configurable hyper parameters for the agent, users can just keep the default value for most parameters, while modify only a few of them if necessary. Option Description uql_equivalent_bits the number of re-allocated bits that is equivalent to uniform allocation of bits. Default: 4 . uql_nb_rlouts the number of roll outs for training the RL agent. Default: 200 . uql_w_bit_min the minimal number of bits for each layer. Default: 2 . uql_w_bit_max the maximal number of bits for each layer. Default: 8 . uql_enbl_rl_global_tune the switch to fine-tune all layers of the network. Default: True . uql_enbl_rl_layerwise_tune the switch to fine-tune the network layer by layer. Default: False . uql_tune_layerwise_steps the number of steps for layer-wise fine-tuning. Default: 300 . uql_tune_global_steps the number of steps for global fine-tuning. Default: 2000 . uql_tune_disp_steps the display steps to show the fine-tuning progress. Default: 100 . uql_enbl_random_layers the switch to randomly permute layers during RL agent training. Default: True . Detailed description and usages for above hyper-parameters are listed below: uql_equivalent_bits : the total number of bits used in the optimal strategy will not exceed n_{param}* n_{param}* uql_equivalent_bits . For example, by setting uql_equivalent_bits =4, the RL agent will try to find the best quantization strategy with the same compression ratio to that each layer is quantized by 4 bits. The following parameters can be kept in default value in most cases. Users can also modify them when using their customized models if necessary. uql_nb_rlouts : the number of roll-out for training the RL agent. Generally we will use the first quarter of uql_nb_rlouts for collection of the training buffer, and last three quarters for the training of the agent. The larger the uql_nb_rlouts , the slower the search for the hyper-parameter optimizer. uql_w_bit_min : the minimum number of quantization bit for a layer. This is used to constrain the searching space and avoid extreme strategies that crash the entire performance of the compressed model. uql_w_bit_max : the maximum number of quantization bit for a layer. This is used to constrain the searching space and avoid that one layer may use too much unnecessary bits. uql_enbl_rl_global_tune : the switch to globally fine-tune the network in each roll-out, which is done by updating the full-precision weights for all layers via the STE estimator. The aim of the fine-tune is to obtain effective reward from the current strategy. uql_enbl_rl_layerwise_tune : the switch to layer-wise fine-tune the network in each roll-out, which is done by minimizing the l2-norm between the quantized layer and full-precision layer. uql_tune_layerwise_steps : the number of steps for layer-wise fine-tuning. Generally, the larger the value, the more precise the reward and thereon the better the strategy. uql_tune_global_steps : the number of steps for global fine-tuning. Generally, the larger the value, the more precise the reward and thereon the better the strategy. uql_tune_disp_steps : the intervals to display the global training process in each roll-out. uql_enbl_random_layers : the switch to randomly permute layers of the network when searching the optimal strategy. This could be helpful since the bit budget used in previous layers may affect the searching space for following layers, while randomly shuffling all layers makes sure that all layers have equal probability of all strategies. Usage Examples In this section, we provide some usage examples to demonstrate how to use UniformQuantLearner under different execution modes and hyper-parameter combinations. To quantize the network, users should first get the model prepared. Users can either use the pre-built models in PocketFlow, or develop their customized nets following the model definition in PocketFlow (for example, resnet_at_cifar10.py ). Once the model is built, the quantization can be easily triggered by directly as follows: To quantize a ResNet-20 model for CIFAR-10 classification task with 4 bits in the local mode, use: # quantize resnet-20 on CIFAR-10 sh ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner=uniform \\ --uql_weight_bits=4 \\ --uql_activation_bits=4 \\ To quantize a ResNet-18 model for ILSVRC_12 classification task with 8 bits in the docker mode with 4 GPUs, and allow to use the channel-wise bucketing, use: # quantize the resnet-18 on ILSVRC-12 sh ./scripts/run_docker.sh nets/resnet_at_ilsvrc12_run.py \\ -n=4 \\ --learner=uniform \\ --uql_weight_bits=8 \\ --uql_activation_bits=8 \\ --uql_use_buckets=True \\ --uql_bucket_type=channel To quantize a MobileNet-v1 model for ILSVRC_12 classification task with 4 bits in the seven mode with 8 GPUs, and allow the RL agent to search for the optimal bit strategy, use: # quantize mobilenet-v1 on ILSVRC-12 sh ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py \\ -n=8 \\ --learner=uniform \\ --uql_enbl_rl_agent=True \\ --uql_equivalent_bits=4 \\ UniformQuantTFLearner PocketFlow also wraps the quantization aware training in TensorFlow. The quantized model can be directly exported to .tflite format via export_quant_tflite_model.py in PocketFlow, and then be easily deployed on Android devices. To configure UniformQuantTFLearner , the hyper-parameters are as follows: Option Description uqtf_save_path UQ-TF: model\\'s save path. Default: ./models_uqtf/model.ckpt . uqtf_save_path_eval UQ-TF: model\\'s save path for evaluation. Default: ./models_uqtf_eval/model.ckpt . uqtf_weight_bits UQ-TF: # of bits for weight quantization. Default: 8 . uqtf_activation_bits UQ-TF: # of bits for activation quantization. Default: 8 . uqtf_quant_delay UQ-TF: # of steps after which weights and activations are quantized. Default: 0 . uqtf_freeze_bn_delay UT-TF: # of steps after which moving mean and variance are frozen. Default: None . uqtf_lrn_rate_dcy UQ-TF: learning rate\\'s decaying factor. Default: 1e-2 . Here, the detailed description (and some analysis) for some above hyper-parameters are listed as follows: uqtf_quant_delay : The number of steps to start fine-tuning on the quantized network. Before the training step reaches uqtf_quant_delay , only full precision weights of the model are updated. uqtf_freeze_bn_delay : The number of steps after which the moving mean and variance of batch normalization layers are frozen and used, instead of the batch statistics during training. uqtf_lrn_rate_dcy : The decay of learning rate for the quantized model. Generally the quantized network needs smaller learning rate comparing to that for the full-precision model. Usage Examples To deploy a quantized network on Android devices, there are generally 3 steps: Quantize the pre-trained network To quantize a MobileNet-v1 model for ILSVRC-12 classification task with 8 bits in the seven mode, use: # quantize MobileNet-v1 on ILSVRC-12 $ ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py -n=8 \\ --learner uniform-tf \\ --nb_epochs_rat 0.2 where --nb_epochs_rat 0.2 specifies that only 20% training epochs to be used, which usually should be enough. Export to .tflite format # load the checkpoints in ./models_uqtf_eval $ python export_quant_tflite_models.py \\ --model_dir ./models_uqtf_eval \\ --enbl_post_quant Note that we enable the enbl_post_quant option to ensure all operations being quantized. On one hand, some operations may not be successfully quantized via TensorFlow's quantization-aware training APIs, so post-training quantization can help remedy this, possibly at the cost of slightly reduced accuracy of the quantized model. On the other hand, users can directly export a full-precision model to its quantized counterpart without going through the UniformQuantTFLearner . This could be helpful when users want to quickly evaluate the inference speed, or there is more tolerance for the performance degradation of quantized model. If the conversion completes without error, then .pb and .tflite files will be saved in ./models_uqtf_eval . Deploy on Mobile Devices The Deployment of a quantized model is very similar to that of a full-precision model, as is shown in the tutorial page . Specifically, users need to do the following modifications: In ImageClassifierQuantizedMobileNet.java L24: rename the class w.r.t. your model. In ImageClassifierQuantizedMobileNet.java L46: replace the model input \"mobilenet_quant_v1_224.tflite\" to your \"*.tflite\" file. In ImageClassifierQuantizedMobileNet.java L51: replace the label file \"labels_mobilenet_quant_v1_224.txt\" to your label files. In Camera2BasicFragment.java L332: change the name of the class accordingly.","title":"Uniform Quantization"},{"location":"uq_learner/#uniform-quantization","text":"","title":"Uniform Quantization"},{"location":"uq_learner/#introduction","text":"Uniform quantization is widely used for model compression and acceleration. Originally the weights in the network are represented by 32-bit floating-point numbers. With uniform quantization, low-precision ( e.g. 4-bit or 8-bit) fixed-point numbers are used to approximate the full-precision network. For k k -bit quantization, the memory saving can be up to 32 / k\u200b 32 / k\u200b . For example, 8-bit quantization can reduce the network size by 4 folds with negligible drop of performance. Currently, PocketFlow supports two types of uniform quantization learners: UniformQuantLearner : a self-developed learner for uniform quantization. The learner is carefully optimized with various extensions and variations supported. UniformQuantTFLearner : a wrapper based on TensorFlow's quantization-aware training training APIs. For now, this wrapper only supports 8-bit quantization, which leads to approximately 4x memory reduction and 3x inference speed-up. A comparison of these two learners are shown below: Features UniformQuantLearner UniformQuantTFLearner Compression Yes Yes Acceleration Yes Fine-tuning Yes Bucketing Yes Hyper-param Optimization Yes","title":"Introduction"},{"location":"uq_learner/#algorithm","text":"","title":"Algorithm"},{"location":"uq_learner/#training-workflow","text":"Both two uniform quantization learners generally follow the training workflow below: Given a pre-defined full-precision model, the learner inserts quantization nodes and operations into the computation graph of the model. With activation quantization enabled, quantization nodes will also be placed after activation operations ( e.g. ReLU). In the training phase, both full-precision and quantized weights are kept. In the forward pass, quantized weights are obtained by applying the quantization function on full-precision weights. To update full-precision weights in the backward pass, since gradients w.r.t. quantized weights are zeros almost everywhere, we use the straight-through estimator (STE, Bengio et al., 2015) to pass gradients of quantized weights directly to full-precision weights for update.","title":"Training Workflow"},{"location":"uq_learner/#quantization-function","text":"Uniform quantization distributes all the quantization points evenly across the range \\left[ w_{min}, w_{max} \\right] \\left[ w_{min}, w_{max} \\right] , where w_{max} w_{max} and w_{min} w_{min} are the maximum and minimum values of weights in each layer (or bucket). The original full-precision weights are then assigned to their closest quantization points. To achieve this, we first normalize the full-precision weights x x to \\left[ 0, 1 \\right] \\left[ 0, 1 \\right] : \\text{sc} \\left( x \\right) = \\frac{ x - \\beta}{\\alpha}, \\text{sc} \\left( x \\right) = \\frac{ x - \\beta}{\\alpha}, where \\alpha = w_{max} - w_{min} \\alpha = w_{max} - w_{min} and \\beta = w_{min} \\beta = w_{min} . Then, we assign \\text{sc} \\left( x \\right) \\text{sc} \\left( x \\right) to its closest quantization point (assuming k k -bit quantization is used): \\hat{x} = \\frac{1}{2^{k} - 1} \\text{round} \\left( \\left( 2^{k} - 1 \\right) \\cdot \\text{sc} \\left( x \\right) \\right), \\hat{x} = \\frac{1}{2^{k} - 1} \\text{round} \\left( \\left( 2^{k} - 1 \\right) \\cdot \\text{sc} \\left( x \\right) \\right), and finally we use inverse linear transformation to recover the quantized weights to the original scale: Q \\left( x \\right) = \\alpha \\cdot \\hat{x} + \\beta. Q \\left( x \\right) = \\alpha \\cdot \\hat{x} + \\beta.","title":"Quantization Function"},{"location":"uq_learner/#uniformquantlearner","text":"UniformQuantLearner is a self-developed learner, which allows a number of customized configurations for uniform quantization. For example, the learner supports bucketing, leading to more fine-grained quantization and better performance. The learner also allows to allocate different bits across layers, in which users can turn on the hyper-parameter optimizer with reinforcement learning to search for the optimal bit allocation strategy.","title":"UniformQuantLearner"},{"location":"uq_learner/#hyper-parameters","text":"To configure UniformQuantLearner , users can pass options via the TensorFlow flag interface. The available options are listed as follows: Option Description uql_weight_bits the number of bits for weights. Default: 4 . uql_activation_bits the number of bits for activation. Default: 32. uql_save_quant_model_path quantized model's save path. Default: ./uql_quant_models/model.ckpt uql_use_buckets the switch to use bucketing. Default: False. uql_bucket_type two bucket type available: [ split , channel ]. Default: channel. uql_bucket_size the number of bucket size for bucket type split . Default: 256 . uql_quantize_all_layers the switch to quantize first and last layers of network. Default: False. uql_quant_epoch the number of epochs for fine-tuning. Default: 60 . uql_enbl_rl_agent the switch to enable RL to learn optimal bit strategy. Default: False . Here, we provide detailed description (and some analysis) for above hyper-parameters: uql_weight_bits : The number of bits for weight quantization. Generally, 8 bit does not hurt the model performance while it can compress the model size by 4 folds. While 2 bit and 4 bit could lead to drop of performance on large datasets such as Imagenet. uql_activation_bits : The number of bits for activation quantization. When both weights and activations are quantized, 8 bit does not lead to apparent drop of performance, and sometimes can even increase the classification accuracy, which is probably due to better generalization ability. Nevertheless, the performance will be more challenged when both weights and activations are quantized to lower bits, comparing to weight-only quantization. uql_save_quant_mode_path : the path to save the quantized model. Quantization nodes have already been inserted into the graph. uql_use_buckets : the switch to turn on the bucket. With bucketing, weights are split into multiple pieces, while the \\alpha \\alpha and \\beta \\beta are calculated individually for each piece. Therefore, turning on the bucketing can lead to more fine-grained quantization. uql_bucket_type : the type of bucketing. Currently two types are supported: [ split , channel ]. split refers to that the weights of a layer are first concatenated into a long vector, and then cut it into pieces according to uql_bucket_size . The remaining last piece will be padded and taken as a new piece. After quantization of each piece, the vectors are then folded back to the original shape as the quantized weights. channel refers to that weights with shape [k, k, cin, cout] in a convolutional layer are cut into cout buckets, where each bucket has the size of k * k * cin . For weights with shape [m, n] in fully connected layers, they are cut into n buckets, each of size m . In practice, bucketing with type channel can be calculated more efficiently comparing to type split since there are less buckets and less computation to iterate through all of them. uql_bucket_size : the size of buckets when using bucket type split . Generally, smaller bucket size can lead to more fine grained quantization, while more storage are required since full precision statistics ( \\alpha \\alpha and \\beta \\beta ) of each bucket need to be kept. uql_quantize_all_layers : the switch to quantize the first and last layers. The first and last layers of the network are connected directly with the input and output, and are arguably more sensitive to quantization. Keeping them un-quantized can slightly increase the performance, nevertheless, if you want to accelerate the inference speed, all layers are supposed to be quantized. uql_quant_epoch : the epochs for fine-tuning a quantized network. uql_enbl_rl_agent : the switch to turn on the RL agent as hyper parameter optimizer. Details about the RL agent and its configurations are described below.","title":"Hyper-parameters"},{"location":"uq_learner/#configure-the-rl-agent","text":"Once the hyper parameter optimizer is turned on, i.e., uql_enbl_rl_agent==True , the RL agent will automatically search for the optimal bit allocation strategy for each layer. In order to search efficiently, the agent need to be configured properly. While here we list all the configurable hyper parameters for the agent, users can just keep the default value for most parameters, while modify only a few of them if necessary. Option Description uql_equivalent_bits the number of re-allocated bits that is equivalent to uniform allocation of bits. Default: 4 . uql_nb_rlouts the number of roll outs for training the RL agent. Default: 200 . uql_w_bit_min the minimal number of bits for each layer. Default: 2 . uql_w_bit_max the maximal number of bits for each layer. Default: 8 . uql_enbl_rl_global_tune the switch to fine-tune all layers of the network. Default: True . uql_enbl_rl_layerwise_tune the switch to fine-tune the network layer by layer. Default: False . uql_tune_layerwise_steps the number of steps for layer-wise fine-tuning. Default: 300 . uql_tune_global_steps the number of steps for global fine-tuning. Default: 2000 . uql_tune_disp_steps the display steps to show the fine-tuning progress. Default: 100 . uql_enbl_random_layers the switch to randomly permute layers during RL agent training. Default: True . Detailed description and usages for above hyper-parameters are listed below: uql_equivalent_bits : the total number of bits used in the optimal strategy will not exceed n_{param}* n_{param}* uql_equivalent_bits . For example, by setting uql_equivalent_bits =4, the RL agent will try to find the best quantization strategy with the same compression ratio to that each layer is quantized by 4 bits. The following parameters can be kept in default value in most cases. Users can also modify them when using their customized models if necessary. uql_nb_rlouts : the number of roll-out for training the RL agent. Generally we will use the first quarter of uql_nb_rlouts for collection of the training buffer, and last three quarters for the training of the agent. The larger the uql_nb_rlouts , the slower the search for the hyper-parameter optimizer. uql_w_bit_min : the minimum number of quantization bit for a layer. This is used to constrain the searching space and avoid extreme strategies that crash the entire performance of the compressed model. uql_w_bit_max : the maximum number of quantization bit for a layer. This is used to constrain the searching space and avoid that one layer may use too much unnecessary bits. uql_enbl_rl_global_tune : the switch to globally fine-tune the network in each roll-out, which is done by updating the full-precision weights for all layers via the STE estimator. The aim of the fine-tune is to obtain effective reward from the current strategy. uql_enbl_rl_layerwise_tune : the switch to layer-wise fine-tune the network in each roll-out, which is done by minimizing the l2-norm between the quantized layer and full-precision layer. uql_tune_layerwise_steps : the number of steps for layer-wise fine-tuning. Generally, the larger the value, the more precise the reward and thereon the better the strategy. uql_tune_global_steps : the number of steps for global fine-tuning. Generally, the larger the value, the more precise the reward and thereon the better the strategy. uql_tune_disp_steps : the intervals to display the global training process in each roll-out. uql_enbl_random_layers : the switch to randomly permute layers of the network when searching the optimal strategy. This could be helpful since the bit budget used in previous layers may affect the searching space for following layers, while randomly shuffling all layers makes sure that all layers have equal probability of all strategies.","title":"Configure the RL Agent"},{"location":"uq_learner/#usage-examples","text":"In this section, we provide some usage examples to demonstrate how to use UniformQuantLearner under different execution modes and hyper-parameter combinations. To quantize the network, users should first get the model prepared. Users can either use the pre-built models in PocketFlow, or develop their customized nets following the model definition in PocketFlow (for example, resnet_at_cifar10.py ). Once the model is built, the quantization can be easily triggered by directly as follows: To quantize a ResNet-20 model for CIFAR-10 classification task with 4 bits in the local mode, use: # quantize resnet-20 on CIFAR-10 sh ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner=uniform \\ --uql_weight_bits=4 \\ --uql_activation_bits=4 \\ To quantize a ResNet-18 model for ILSVRC_12 classification task with 8 bits in the docker mode with 4 GPUs, and allow to use the channel-wise bucketing, use: # quantize the resnet-18 on ILSVRC-12 sh ./scripts/run_docker.sh nets/resnet_at_ilsvrc12_run.py \\ -n=4 \\ --learner=uniform \\ --uql_weight_bits=8 \\ --uql_activation_bits=8 \\ --uql_use_buckets=True \\ --uql_bucket_type=channel To quantize a MobileNet-v1 model for ILSVRC_12 classification task with 4 bits in the seven mode with 8 GPUs, and allow the RL agent to search for the optimal bit strategy, use: # quantize mobilenet-v1 on ILSVRC-12 sh ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py \\ -n=8 \\ --learner=uniform \\ --uql_enbl_rl_agent=True \\ --uql_equivalent_bits=4 \\","title":"Usage Examples"},{"location":"uq_learner/#uniformquanttflearner","text":"PocketFlow also wraps the quantization aware training in TensorFlow. The quantized model can be directly exported to .tflite format via export_quant_tflite_model.py in PocketFlow, and then be easily deployed on Android devices. To configure UniformQuantTFLearner , the hyper-parameters are as follows: Option Description uqtf_save_path UQ-TF: model\\'s save path. Default: ./models_uqtf/model.ckpt . uqtf_save_path_eval UQ-TF: model\\'s save path for evaluation. Default: ./models_uqtf_eval/model.ckpt . uqtf_weight_bits UQ-TF: # of bits for weight quantization. Default: 8 . uqtf_activation_bits UQ-TF: # of bits for activation quantization. Default: 8 . uqtf_quant_delay UQ-TF: # of steps after which weights and activations are quantized. Default: 0 . uqtf_freeze_bn_delay UT-TF: # of steps after which moving mean and variance are frozen. Default: None . uqtf_lrn_rate_dcy UQ-TF: learning rate\\'s decaying factor. Default: 1e-2 . Here, the detailed description (and some analysis) for some above hyper-parameters are listed as follows: uqtf_quant_delay : The number of steps to start fine-tuning on the quantized network. Before the training step reaches uqtf_quant_delay , only full precision weights of the model are updated. uqtf_freeze_bn_delay : The number of steps after which the moving mean and variance of batch normalization layers are frozen and used, instead of the batch statistics during training. uqtf_lrn_rate_dcy : The decay of learning rate for the quantized model. Generally the quantized network needs smaller learning rate comparing to that for the full-precision model.","title":"UniformQuantTFLearner"},{"location":"uq_learner/#usage-examples_1","text":"To deploy a quantized network on Android devices, there are generally 3 steps:","title":"Usage Examples"},{"location":"uq_learner/#quantize-the-pre-trained-network","text":"To quantize a MobileNet-v1 model for ILSVRC-12 classification task with 8 bits in the seven mode, use: # quantize MobileNet-v1 on ILSVRC-12 $ ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py -n=8 \\ --learner uniform-tf \\ --nb_epochs_rat 0.2 where --nb_epochs_rat 0.2 specifies that only 20% training epochs to be used, which usually should be enough.","title":"Quantize the pre-trained network"},{"location":"uq_learner/#export-to-tflite-format","text":"# load the checkpoints in ./models_uqtf_eval $ python export_quant_tflite_models.py \\ --model_dir ./models_uqtf_eval \\ --enbl_post_quant Note that we enable the enbl_post_quant option to ensure all operations being quantized. On one hand, some operations may not be successfully quantized via TensorFlow's quantization-aware training APIs, so post-training quantization can help remedy this, possibly at the cost of slightly reduced accuracy of the quantized model. On the other hand, users can directly export a full-precision model to its quantized counterpart without going through the UniformQuantTFLearner . This could be helpful when users want to quickly evaluate the inference speed, or there is more tolerance for the performance degradation of quantized model. If the conversion completes without error, then .pb and .tflite files will be saved in ./models_uqtf_eval .","title":"Export to .tflite format"},{"location":"uq_learner/#deploy-on-mobile-devices","text":"The Deployment of a quantized model is very similar to that of a full-precision model, as is shown in the tutorial page . Specifically, users need to do the following modifications: In ImageClassifierQuantizedMobileNet.java L24: rename the class w.r.t. your model. In ImageClassifierQuantizedMobileNet.java L46: replace the model input \"mobilenet_quant_v1_224.tflite\" to your \"*.tflite\" file. In ImageClassifierQuantizedMobileNet.java L51: replace the label file \"labels_mobilenet_quant_v1_224.txt\" to your label files. In Camera2BasicFragment.java L332: change the name of the class accordingly.","title":"Deploy on Mobile Devices"},{"location":"ws_learner/","text":"Weight Sparsification Introduction By imposing sparsity constraints on convolutional and fully-connected layers, the number of non-zero weights can be dramatically reduced, which leads to smaller model size and lower FLOPS for inference (actual acceleration depends on efficient implementation for sparse operations). Directly training a network with fixed sparsity degree may encounter some optimization difficulties and takes longer time to converge. To overcome this, Zhu & Gupta proposed a dynamic pruning schedule to gradually remove network weights to simplify the optimization process (Zhu & Gupta, 2017). Note: in this documentation, we will use both \"sparsity\" and \"pruning ratio\" to denote the ratio of zero-valued weights over all weights. Algorithm Description For each convolutional kernel (for convolutional layer) or weighting matrix (for fully-connected layer), we create a binary mask of the same size to impose the sparsity constraint. During the forward pass, the convolutional kernel (or weighting matrix) is multiplied with the binary mask, so that some weights will not participate in the computation and also will not be updated via gradients. The binary mask is computed based on absolute values of weights: weight with the smallest absolute value will be masked-out until the desired sparsity is reached. During the training process, the sparsity is gradually increased to improve the overall optimization behavior. The dynamic pruning schedule is defined as: s_{t} = s_{f} - s_{f} \\cdot \\left( 1 - \\frac{t - t_{b}}{t_{e} - t_{b}} \\right)^{\\alpha}, t \\in \\left[ t_{b}, t_{e} \\right] s_{t} = s_{f} - s_{f} \\cdot \\left( 1 - \\frac{t - t_{b}}{t_{e} - t_{b}} \\right)^{\\alpha}, t \\in \\left[ t_{b}, t_{e} \\right] where s_{t} s_{t} is the sparsity at iteration # t t , s_{f} s_{f} is the target sparsity, t_{b} t_{b} and t_{e} t_{e} are the iteration indices where the sparsity begins and stops increasing, and \\alpha \\alpha is the exponent term. In the actual implementation, the binary mask is not updated at each iteration. Instead, it is updated every \\Delta t \\Delta t iterations so as to stabilize the training process. We visualize the dynamic pruning schedule in the figure below. Most networks consist of multiple layers, and the weight redundancy may differ from one layer to another. In order to maximally exploit the weight redundancy, we incorporate a reinforcement learning controller to automatically determine the optimal sparsity (or pruning ratio) for each layer. In each roll-out, the RL agent sequentially determine the sparsity for each layer, and then the network is pruned and re-trained for a few iterations using layer-wise regression & global fine-tuning. The reward function's value is computed based on the re-trained network's accuracy (and computation efficiency), and then used update model parameters of RL agent. For more details, please refer to the documentation named \"Hyper-parameter Optimizer - Reinforcement Learning\". Hyper-parameters Below is the full list of hyper-parameters used in the weight sparsification learner: Name Description ws_save_path model's save path ws_prune_ratio target pruning ratio ws_prune_ratio_prtl pruning ratio protocol: 'uniform' / 'heurist' / 'optimal' ws_nb_rlouts number of roll-outs for the RL agent ws_nb_rlouts_min minimal number of roll-outs for the RL agent to start training ws_reward_type reward type: 'single-obj' / 'multi-obj' ws_lrn_rate_rg learning rate for layer-wise regression ws_nb_iters_rg number of iterations for layer-wise regression ws_lrn_rate_ft learning rate for global fine-tuning ws_nb_iters_ft number of iterations for global fine-tuning ws_nb_iters_feval number of iterations for fast evaluation ws_prune_ratio_exp pruning ratio's exponent term ws_iter_ratio_beg iteration ratio at which the pruning ratio begins increasing ws_iter_ratio_end iteration ratio at which the pruning ratio stops increasing ws_mask_update_step step size for updating the pruning mask Here, we provide detailed description (and some analysis) for above hyper-parameters: ws_save_path : save path for model created in the training graph. The resulting checkpoint files can be used to resume training from a previous run and compute model's loss function's value and some other evaluation metrics. ws_prune_ratio : target pruning ratio for convolutional & fully-connected layers. The larger ws_prune_ratio is, the more weights will be pruned. If ws_prune_ratio equals 0, then no weights will be pruned and model remains the same; if ws_prune_ratio equals 1, then all weights are pruned. ws_prune_ratio_prtl : pruning ratio protocol. Possible options include: 1) uniform: all layers use the same pruning ratio; 2) heurist: the more weights in one layer, the higher pruning ratio will be; 3) optimal: each layer's pruning ratio is determined by reinforcement learning. ws_nb_rlouts : number of roll-outs for training the reinforcement learning agent. A roll-out refers to: use the RL agent to determine the pruning ratio for each layer; fine-tune the weight sparsified network; evaluate the fine-tuned network to obtain the reward value. ws_nb_rlouts_min : minimal number of roll-outs for the RL agent to start training. The RL agent requires a few roll-outs for random exploration before actual training starts. We recommend to set this to be a quarter of ws_nb_rlouts . ws_reward_type : reward function's type for the RL agent. Possible options include: 1) single-obj: the reward function only depends on the compressed model's accuracy (the sparsity constraint is imposed during roll-out); 2) multi-obj: the reward function depends on both the compressed model's accuracy and the actual sparsity. ws_lrn_rate_rg : learning rate for layer-wise regression. ws_nb_iters_rg : number of iterations for layer-wise regression. This should be set to some value that the layer-wise regression can almost converge and the loss function's value does not decrease much even if more iterations are used. ws_lrn_rate_ft : learning rate for global fine-tuning. ws_nb_iters_ft : number of iterations for global fine-tuning. This should be set to some value that the global fine-tuning can almost converge and the loss function's value does not decrease much even if more iterations are used. ws_nb_iters_feval : number of iterations for fast evaluation. In each roll-out, the re-trained network is evaluated on a subset of evaluation data to save time. ws_prune_ratio_exp : pruning ratio's exponent term as defined in the dynamic pruning schedule above. ws_iter_ratio_beg : iteration ratio at which the pruning ratio begins increasing. In the dynamic pruning schedule defined above, t_{b} t_{b} equals to the total number of training iterations multiplied with ws_iter_ratio_beg . ws_iter_ratio_end : iteration ratio at which the pruning ratio stops increasing. In the dynamic pruning schedule defined above, t_{e} t_{e} equals to the total number of training iterations multiplied with ws_iter_ratio_end . ws_mask_update_step : step size for updating the pruning mask. By increasing ws_mask_update_step , binary masks for weight pruning are less frequently updated, which will speed-up the training but the difference between pre-update and post-update sparsity will be larger. Usage Examples In this section, we provide some usage examples to demonstrate how to use WeightSparseLearner under different execution modes and hyper-parameter combinations: To compress a ResNet-20 model for CIFAR-10 classification task in the local mode, use: # set the target pruning ratio to 0.75 ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner weight-sparse \\ --ws_prune_ratio 0.75 To compress a ResNet-34 model for ILSVRC-12 classification task in the docker mode with 4 GPUs, use: # set the pruning ratio protocol to \"heurist\" ./scripts/run_docker.sh nets/resnet_at_ilsvrc12_run.py -n=4 \\ --learner weight-sparse \\ --resnet_size 34 \\ --ws_prune_ratio_prtl heurist To compress a MobileNet-v2 model for ILSVRC-12 classification task in the seven mode with 8 GPUs, use: # enable training with distillation loss ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py -n=8 \\ --learner weight-sparse \\ --mobilenet_version 2 \\ --enbl_dst","title":"Weight Sparsification"},{"location":"ws_learner/#weight-sparsification","text":"","title":"Weight Sparsification"},{"location":"ws_learner/#introduction","text":"By imposing sparsity constraints on convolutional and fully-connected layers, the number of non-zero weights can be dramatically reduced, which leads to smaller model size and lower FLOPS for inference (actual acceleration depends on efficient implementation for sparse operations). Directly training a network with fixed sparsity degree may encounter some optimization difficulties and takes longer time to converge. To overcome this, Zhu & Gupta proposed a dynamic pruning schedule to gradually remove network weights to simplify the optimization process (Zhu & Gupta, 2017). Note: in this documentation, we will use both \"sparsity\" and \"pruning ratio\" to denote the ratio of zero-valued weights over all weights.","title":"Introduction"},{"location":"ws_learner/#algorithm-description","text":"For each convolutional kernel (for convolutional layer) or weighting matrix (for fully-connected layer), we create a binary mask of the same size to impose the sparsity constraint. During the forward pass, the convolutional kernel (or weighting matrix) is multiplied with the binary mask, so that some weights will not participate in the computation and also will not be updated via gradients. The binary mask is computed based on absolute values of weights: weight with the smallest absolute value will be masked-out until the desired sparsity is reached. During the training process, the sparsity is gradually increased to improve the overall optimization behavior. The dynamic pruning schedule is defined as: s_{t} = s_{f} - s_{f} \\cdot \\left( 1 - \\frac{t - t_{b}}{t_{e} - t_{b}} \\right)^{\\alpha}, t \\in \\left[ t_{b}, t_{e} \\right] s_{t} = s_{f} - s_{f} \\cdot \\left( 1 - \\frac{t - t_{b}}{t_{e} - t_{b}} \\right)^{\\alpha}, t \\in \\left[ t_{b}, t_{e} \\right] where s_{t} s_{t} is the sparsity at iteration # t t , s_{f} s_{f} is the target sparsity, t_{b} t_{b} and t_{e} t_{e} are the iteration indices where the sparsity begins and stops increasing, and \\alpha \\alpha is the exponent term. In the actual implementation, the binary mask is not updated at each iteration. Instead, it is updated every \\Delta t \\Delta t iterations so as to stabilize the training process. We visualize the dynamic pruning schedule in the figure below. Most networks consist of multiple layers, and the weight redundancy may differ from one layer to another. In order to maximally exploit the weight redundancy, we incorporate a reinforcement learning controller to automatically determine the optimal sparsity (or pruning ratio) for each layer. In each roll-out, the RL agent sequentially determine the sparsity for each layer, and then the network is pruned and re-trained for a few iterations using layer-wise regression & global fine-tuning. The reward function's value is computed based on the re-trained network's accuracy (and computation efficiency), and then used update model parameters of RL agent. For more details, please refer to the documentation named \"Hyper-parameter Optimizer - Reinforcement Learning\".","title":"Algorithm Description"},{"location":"ws_learner/#hyper-parameters","text":"Below is the full list of hyper-parameters used in the weight sparsification learner: Name Description ws_save_path model's save path ws_prune_ratio target pruning ratio ws_prune_ratio_prtl pruning ratio protocol: 'uniform' / 'heurist' / 'optimal' ws_nb_rlouts number of roll-outs for the RL agent ws_nb_rlouts_min minimal number of roll-outs for the RL agent to start training ws_reward_type reward type: 'single-obj' / 'multi-obj' ws_lrn_rate_rg learning rate for layer-wise regression ws_nb_iters_rg number of iterations for layer-wise regression ws_lrn_rate_ft learning rate for global fine-tuning ws_nb_iters_ft number of iterations for global fine-tuning ws_nb_iters_feval number of iterations for fast evaluation ws_prune_ratio_exp pruning ratio's exponent term ws_iter_ratio_beg iteration ratio at which the pruning ratio begins increasing ws_iter_ratio_end iteration ratio at which the pruning ratio stops increasing ws_mask_update_step step size for updating the pruning mask Here, we provide detailed description (and some analysis) for above hyper-parameters: ws_save_path : save path for model created in the training graph. The resulting checkpoint files can be used to resume training from a previous run and compute model's loss function's value and some other evaluation metrics. ws_prune_ratio : target pruning ratio for convolutional & fully-connected layers. The larger ws_prune_ratio is, the more weights will be pruned. If ws_prune_ratio equals 0, then no weights will be pruned and model remains the same; if ws_prune_ratio equals 1, then all weights are pruned. ws_prune_ratio_prtl : pruning ratio protocol. Possible options include: 1) uniform: all layers use the same pruning ratio; 2) heurist: the more weights in one layer, the higher pruning ratio will be; 3) optimal: each layer's pruning ratio is determined by reinforcement learning. ws_nb_rlouts : number of roll-outs for training the reinforcement learning agent. A roll-out refers to: use the RL agent to determine the pruning ratio for each layer; fine-tune the weight sparsified network; evaluate the fine-tuned network to obtain the reward value. ws_nb_rlouts_min : minimal number of roll-outs for the RL agent to start training. The RL agent requires a few roll-outs for random exploration before actual training starts. We recommend to set this to be a quarter of ws_nb_rlouts . ws_reward_type : reward function's type for the RL agent. Possible options include: 1) single-obj: the reward function only depends on the compressed model's accuracy (the sparsity constraint is imposed during roll-out); 2) multi-obj: the reward function depends on both the compressed model's accuracy and the actual sparsity. ws_lrn_rate_rg : learning rate for layer-wise regression. ws_nb_iters_rg : number of iterations for layer-wise regression. This should be set to some value that the layer-wise regression can almost converge and the loss function's value does not decrease much even if more iterations are used. ws_lrn_rate_ft : learning rate for global fine-tuning. ws_nb_iters_ft : number of iterations for global fine-tuning. This should be set to some value that the global fine-tuning can almost converge and the loss function's value does not decrease much even if more iterations are used. ws_nb_iters_feval : number of iterations for fast evaluation. In each roll-out, the re-trained network is evaluated on a subset of evaluation data to save time. ws_prune_ratio_exp : pruning ratio's exponent term as defined in the dynamic pruning schedule above. ws_iter_ratio_beg : iteration ratio at which the pruning ratio begins increasing. In the dynamic pruning schedule defined above, t_{b} t_{b} equals to the total number of training iterations multiplied with ws_iter_ratio_beg . ws_iter_ratio_end : iteration ratio at which the pruning ratio stops increasing. In the dynamic pruning schedule defined above, t_{e} t_{e} equals to the total number of training iterations multiplied with ws_iter_ratio_end . ws_mask_update_step : step size for updating the pruning mask. By increasing ws_mask_update_step , binary masks for weight pruning are less frequently updated, which will speed-up the training but the difference between pre-update and post-update sparsity will be larger.","title":"Hyper-parameters"},{"location":"ws_learner/#usage-examples","text":"In this section, we provide some usage examples to demonstrate how to use WeightSparseLearner under different execution modes and hyper-parameter combinations: To compress a ResNet-20 model for CIFAR-10 classification task in the local mode, use: # set the target pruning ratio to 0.75 ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner weight-sparse \\ --ws_prune_ratio 0.75 To compress a ResNet-34 model for ILSVRC-12 classification task in the docker mode with 4 GPUs, use: # set the pruning ratio protocol to \"heurist\" ./scripts/run_docker.sh nets/resnet_at_ilsvrc12_run.py -n=4 \\ --learner weight-sparse \\ --resnet_size 34 \\ --ws_prune_ratio_prtl heurist To compress a MobileNet-v2 model for ILSVRC-12 classification task in the seven mode with 8 GPUs, use: # enable training with distillation loss ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py -n=8 \\ --learner weight-sparse \\ --mobilenet_version 2 \\ --enbl_dst","title":"Usage Examples"}]}