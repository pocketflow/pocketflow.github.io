{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PocketFlow PocketFlow is an open-source framework for compressing and accelerating deep learning models with minimal human effort. Deep learning is widely used in various areas, such as computer vision, speech recognition, and natural language translation. However, deep learning models are often computational expensive, which limits further applications on mobile devices with limited computational resources. PocketFlow aims at providing an easy-to-use toolkit for developers to improve the inference efficiency with little or no performance degradation. Developers only needs to specify the desired compression and/or acceleration ratios and then PocketFlow will automatically choose proper hyper-parameters to generate a highly efficient compressed model for deployment. Framework The proposed framework mainly consists of two categories of algorithm components, \\ie learners and hyper-parameter optimizers, as depicted in the figure below. Given an uncompressed original model, the learner module generates a candidate compressed model using some randomly chosen hyper-parameter combination. The candidate model's accuracy and computation efficiency is then evaluated and used by hyper-parameter optimizer module as the feedback signal to determine the next hyper-parameter combination to be explored by the learner module. After a few iterations, the best one of all the candidate models is output as the final compressed model. Learners A learner refers to some model compression algorithm augmented with several training techniques as shown in the figure above. Below is a list of model compression algorithms supported in PocketFlow: Name Description ChannelPrunedLearner channel pruning with LASSO-based channel selection (He et al., 2017) DisChnPrunedLearner discrimination-aware channel pruning (Zhuang et al., 2018) WeightSparseLearner weight sparsification with dynamic pruning schedule (Zhu Gupta, 2017) UniformQuantLearner weight quantization with uniform reconstruction levels (Jacob et al., 2018) UniformQuantTFLearner weight quantization with uniform reconstruction levels and TensorFlow APIs NonUniformQuantLearner weight quantization with non-uniform reconstruction levels (Han et al., 2016) All the above model compression algorithms can trained with fast fine-tuning, which is to directly derive a compressed model from the original one by applying either pruning masks or quantization functions. The resulting model can be fine-tuned with a few iterations to recover the accuracy to some extent. Alternatively, the compressed model can be re-trained with the full training data, which leads to higher accuracy but usually takes longer to complete. To further reduce the compressed model's performance degradation, we adopt network distillation to augment its training process with an extra loss term, using the original uncompressed model's outputs as soft labels. Additionally, multi-GPU distributed training is enabled for all learners to speed-up the time-consuming training process. Hyper-parameter Optimizers For model compression algorithms, there are several hyper-parameters that may have a large impact on the final compressed model's performance. It can be quite difficult to manually determine proper values for these hyper-parameters, especially for developers that are not very familiar with algorithm details. Recently, several AutoML systems, e.g. Cloud AutoML from Google, have been developed to train high-quality machine learning models with minimal human effort. Particularly, the AMC algorithm (He et al., 2018) presents promising results for adopting reinforcement learning for automated model compression with channel pruning and fine-grained pruning. In PocketFlow, we introduce the hyper-parameter optimizer module to iteratively search for the optimal hyper-parameter setting. We provide several implementations of hyper-parameter optimizer, based on models including Gaussian Processes (GP, Mockus, 1975), Tree-structured Parzen Estimator (TPE, Bergstra et al., 2013), and Deterministic Deep Policy Gradients (DDPG, Lillicrap et al., 2016). The hyper-parameter setting is optimized through an iterative process. In each iteration, the hyper-parameter optimizer chooses a combination of hyper-parameter values, and the learner generates a candidate model with fast fast-tuning. The candidate model is evaluated to calculate the reward of the current hyper-parameter setting. After that, the hyper-parameter optimizer updates its model to improve its estimation on the hyper-parameter space. Finally, when the best candidate model (and corresponding hyper-parameter setting) is selected after some iterations, this model can be re-trained with full data to further reduce the performance loss.","title":"Home"},{"location":"#pocketflow","text":"PocketFlow is an open-source framework for compressing and accelerating deep learning models with minimal human effort. Deep learning is widely used in various areas, such as computer vision, speech recognition, and natural language translation. However, deep learning models are often computational expensive, which limits further applications on mobile devices with limited computational resources. PocketFlow aims at providing an easy-to-use toolkit for developers to improve the inference efficiency with little or no performance degradation. Developers only needs to specify the desired compression and/or acceleration ratios and then PocketFlow will automatically choose proper hyper-parameters to generate a highly efficient compressed model for deployment.","title":"PocketFlow"},{"location":"#framework","text":"The proposed framework mainly consists of two categories of algorithm components, \\ie learners and hyper-parameter optimizers, as depicted in the figure below. Given an uncompressed original model, the learner module generates a candidate compressed model using some randomly chosen hyper-parameter combination. The candidate model's accuracy and computation efficiency is then evaluated and used by hyper-parameter optimizer module as the feedback signal to determine the next hyper-parameter combination to be explored by the learner module. After a few iterations, the best one of all the candidate models is output as the final compressed model.","title":"Framework"},{"location":"#learners","text":"A learner refers to some model compression algorithm augmented with several training techniques as shown in the figure above. Below is a list of model compression algorithms supported in PocketFlow: Name Description ChannelPrunedLearner channel pruning with LASSO-based channel selection (He et al., 2017) DisChnPrunedLearner discrimination-aware channel pruning (Zhuang et al., 2018) WeightSparseLearner weight sparsification with dynamic pruning schedule (Zhu Gupta, 2017) UniformQuantLearner weight quantization with uniform reconstruction levels (Jacob et al., 2018) UniformQuantTFLearner weight quantization with uniform reconstruction levels and TensorFlow APIs NonUniformQuantLearner weight quantization with non-uniform reconstruction levels (Han et al., 2016) All the above model compression algorithms can trained with fast fine-tuning, which is to directly derive a compressed model from the original one by applying either pruning masks or quantization functions. The resulting model can be fine-tuned with a few iterations to recover the accuracy to some extent. Alternatively, the compressed model can be re-trained with the full training data, which leads to higher accuracy but usually takes longer to complete. To further reduce the compressed model's performance degradation, we adopt network distillation to augment its training process with an extra loss term, using the original uncompressed model's outputs as soft labels. Additionally, multi-GPU distributed training is enabled for all learners to speed-up the time-consuming training process.","title":"Learners"},{"location":"#hyper-parameter-optimizers","text":"For model compression algorithms, there are several hyper-parameters that may have a large impact on the final compressed model's performance. It can be quite difficult to manually determine proper values for these hyper-parameters, especially for developers that are not very familiar with algorithm details. Recently, several AutoML systems, e.g. Cloud AutoML from Google, have been developed to train high-quality machine learning models with minimal human effort. Particularly, the AMC algorithm (He et al., 2018) presents promising results for adopting reinforcement learning for automated model compression with channel pruning and fine-grained pruning. In PocketFlow, we introduce the hyper-parameter optimizer module to iteratively search for the optimal hyper-parameter setting. We provide several implementations of hyper-parameter optimizer, based on models including Gaussian Processes (GP, Mockus, 1975), Tree-structured Parzen Estimator (TPE, Bergstra et al., 2013), and Deterministic Deep Policy Gradients (DDPG, Lillicrap et al., 2016). The hyper-parameter setting is optimized through an iterative process. In each iteration, the hyper-parameter optimizer chooses a combination of hyper-parameter values, and the learner generates a candidate model with fast fast-tuning. The candidate model is evaluated to calculate the reward of the current hyper-parameter setting. After that, the hyper-parameter optimizer updates its model to improve its estimation on the hyper-parameter space. Finally, when the best candidate model (and corresponding hyper-parameter setting) is selected after some iterations, this model can be re-trained with full data to further reduce the performance loss.","title":"Hyper-parameter Optimizers"},{"location":"automl_based_methods/","text":"AutoML-based Methods Under construction ...","title":"AutoML-based Methods"},{"location":"automl_based_methods/#automl-based-methods","text":"Under construction ...","title":"AutoML-based Methods"},{"location":"cp_learner/","text":"Channel Pruning Introduction Channel pruning is a kind of structural model compression approach which can not only compress the model size, but accelerate the inference speed directly. PocketFlow uses the Yihui. et. al[1] 's channel pruning algorithm to pruning each channel of convolution layers with a certain ratio, and for details please refer to the channel pruning paper . For better performance and more robust, we modify some parts of the algorithm to achieve better result. In order to achieve a better performance, PocketFlow can take advantages of reinforcement learning to search a better compression ratio[2]. User can also use the distilling [3] and group tuning function to improve the accuracy after compression. Group tuning means setting a certain number of layers as group and then pruning and finetuning/retraining each group sequentially. For example we can set each 3 layers as a group and then pruning the first 3 layers. After that finetune/retraine the whole model and prune the next 3 layers and so on. Distilling and group tuning are experimentally proved as effective approaches to achieve higher accuracy at a certain compression ratio in most situations. Pruning Option The code of channel pruning are located at directory ./learners/channel_pruning . To use channel pruning. users can set --learners to channel . The Channel pruning supports 3 kinds of pruning setup by prune_option option. Uniform Channel Pruning One is the uniform layer pruning, which means the user can set each convolution layer pruned with an uniform pruning ratio by --prune_option=uniform and set the ratio (eg. making the ratio 0.5) by --uniform_preserve_ratio=0.5 . Note that for a layer, if both of pruning ratio of the layer and its previous layer are 0.5, the real preserved FLOPs are 1/4 of original FLOPs. Because channel pruning only prune the c_out channels of the convolution and c_in channels of the next convolution, if both c_in and c_out channels are pruned by 0.5, it will preserve only 1/4 of original computation cost. For a layer by layer convolution networks without residual blocks, if the user set uniform_preserve_ratio to 0.5 , the whole model will be the 0.25 computation of the original model. However for the residual networks, some convolutions can only prune their c_in or c_out channels, which means the total preseved computation ratio may be much greater than 0.25. Example: ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --uniform_preserve_ratio=0.5 \\ --learner=channel\\ --prune_option=uniform\\ --batch_size_eval=64 \\ --resnet_size=20 List Channel Pruning Another pruning option is pruning the corresponding layer with ratios listed in a named ratio.list file, the file name of which can be set by --prune_list_file option. the ratio value must be separated by a comma. User can set --prune_option=list to prune the model by list ratios. Example: Add list 1.0, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 1.0, 0.25, 1.0, 0.25, 0.21875, 0.21875, 0.21875, 1.0, 0.5625, 1.0, 0.546875, 0.546875, 0.546875, 1 in ./ratio.list ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --learner=channel \\ --prune_option=list \\ --batch_size_eval=64 \\ --prune_list_file=./ratio.list \\ --resnet_size=20 Automatic Channel Pruning The last one pruning option is searching better pruning ratios by reinforcement learning and you only need to give a value which represents what the ratio of total FLOPs/Computation you wants the compressed model preserve. You can set --prune_option=auto and set a preserve ratio number such as --preserve_ratio=0.5 . User can also use cp_nb_rlouts_min to control reinforcement learning warm up iterations, which means the RL agent start to learn after the iterations, the default value is 50 . User can also use cp_nb_rlouts_min to control the total iteration RL agent to search, the default value is 200 . If the user want to control other parameters of the agents, please refer to the reinforcement component page. Example: ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --preserve_ratio=0.5 \\ --learner=channel\\ --prune_option=auto \\ --batch_size_eval=64 \\ --resnet_size=20 Channel pruning parameters The implementation of the channel pruning use Lasso algorithm to do channel selection and linear regression to do feature map reconstruction. During these two phases, sampling is done on the feature map to reduce computation cost. The users can use --nb_points_per_layer to set how many sampling points on each layer are taken, the default value is 10 . For some dataset, if the images contain too many zero pixels (eg. black color), the value should be greater. The users can also set using how many batches to do channel selection and feature reconstruction by nb_batches , the default value is 60 . Small value of nb_batches may cause over-fitting and large value may slow down the solving speed, so a good value depends on the nets and dataset. For more practical usage, user may consider make the channel number of each layer is the quadruple for fast inference of mobile devices. In this case, user can set --quadruple to True to make the compressed model have a quadruple number of channels. Distilling Distilling is an effective approach to improve the final accuracy of compressed model with PocketFlow in most situations of classification. User can set --enbl_dst=True to enable distillling. Group Tuning As introduced above, group tuning was proposed by the PocketFlow team and finding it is very useful to improve the performance of model compression. In PocketFlow, users can set --finetune=True to enable group finetuning and set the group number by --list_group , the default value is 1000 . There is a trade-off between the small value and large value, because if the value is 1 , Pocketflow will prune convolution and finetune/retrain by each layer, which may have better effect but be more time-consuming. If we set the value large, the function will be less effective. User can also set the number of iterations to finetune by setting nb_iters_ft_ratio which mean the ratio the total iterations to be used in finetuning. The learning rate of finetuning can be set by lrn_rate_ft . [1] He, Y., Zhang, X. and Sun, J., 2017, October. Channel pruning for accelerating very deep neural networks. In International Conference on Computer Vision (ICCV) (Vol. 2, No. 6). [2] He, Y., Lin, J., Liu, Z., Wang, H., Li, L.J. and Han, S., 2018, September. Amc: Automl for model compression and acceleration on mobile devices. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 784-800). [3] Hinton, G., Vinyals, O. and Dean, J., 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 .","title":"Channel Pruning"},{"location":"cp_learner/#channel-pruning","text":"","title":"Channel Pruning"},{"location":"cp_learner/#introduction","text":"Channel pruning is a kind of structural model compression approach which can not only compress the model size, but accelerate the inference speed directly. PocketFlow uses the Yihui. et. al[1] 's channel pruning algorithm to pruning each channel of convolution layers with a certain ratio, and for details please refer to the channel pruning paper . For better performance and more robust, we modify some parts of the algorithm to achieve better result. In order to achieve a better performance, PocketFlow can take advantages of reinforcement learning to search a better compression ratio[2]. User can also use the distilling [3] and group tuning function to improve the accuracy after compression. Group tuning means setting a certain number of layers as group and then pruning and finetuning/retraining each group sequentially. For example we can set each 3 layers as a group and then pruning the first 3 layers. After that finetune/retraine the whole model and prune the next 3 layers and so on. Distilling and group tuning are experimentally proved as effective approaches to achieve higher accuracy at a certain compression ratio in most situations.","title":"Introduction"},{"location":"cp_learner/#pruning-option","text":"The code of channel pruning are located at directory ./learners/channel_pruning . To use channel pruning. users can set --learners to channel . The Channel pruning supports 3 kinds of pruning setup by prune_option option.","title":"Pruning Option"},{"location":"cp_learner/#uniform-channel-pruning","text":"One is the uniform layer pruning, which means the user can set each convolution layer pruned with an uniform pruning ratio by --prune_option=uniform and set the ratio (eg. making the ratio 0.5) by --uniform_preserve_ratio=0.5 . Note that for a layer, if both of pruning ratio of the layer and its previous layer are 0.5, the real preserved FLOPs are 1/4 of original FLOPs. Because channel pruning only prune the c_out channels of the convolution and c_in channels of the next convolution, if both c_in and c_out channels are pruned by 0.5, it will preserve only 1/4 of original computation cost. For a layer by layer convolution networks without residual blocks, if the user set uniform_preserve_ratio to 0.5 , the whole model will be the 0.25 computation of the original model. However for the residual networks, some convolutions can only prune their c_in or c_out channels, which means the total preseved computation ratio may be much greater than 0.25. Example: ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --uniform_preserve_ratio=0.5 \\ --learner=channel\\ --prune_option=uniform\\ --batch_size_eval=64 \\ --resnet_size=20","title":"Uniform Channel Pruning"},{"location":"cp_learner/#list-channel-pruning","text":"Another pruning option is pruning the corresponding layer with ratios listed in a named ratio.list file, the file name of which can be set by --prune_list_file option. the ratio value must be separated by a comma. User can set --prune_option=list to prune the model by list ratios. Example: Add list 1.0, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 1.0, 0.25, 1.0, 0.25, 0.21875, 0.21875, 0.21875, 1.0, 0.5625, 1.0, 0.546875, 0.546875, 0.546875, 1 in ./ratio.list ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --learner=channel \\ --prune_option=list \\ --batch_size_eval=64 \\ --prune_list_file=./ratio.list \\ --resnet_size=20","title":"List Channel Pruning"},{"location":"cp_learner/#automatic-channel-pruning","text":"The last one pruning option is searching better pruning ratios by reinforcement learning and you only need to give a value which represents what the ratio of total FLOPs/Computation you wants the compressed model preserve. You can set --prune_option=auto and set a preserve ratio number such as --preserve_ratio=0.5 . User can also use cp_nb_rlouts_min to control reinforcement learning warm up iterations, which means the RL agent start to learn after the iterations, the default value is 50 . User can also use cp_nb_rlouts_min to control the total iteration RL agent to search, the default value is 200 . If the user want to control other parameters of the agents, please refer to the reinforcement component page. Example: ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --preserve_ratio=0.5 \\ --learner=channel\\ --prune_option=auto \\ --batch_size_eval=64 \\ --resnet_size=20","title":"Automatic Channel Pruning"},{"location":"cp_learner/#channel-pruning-parameters","text":"The implementation of the channel pruning use Lasso algorithm to do channel selection and linear regression to do feature map reconstruction. During these two phases, sampling is done on the feature map to reduce computation cost. The users can use --nb_points_per_layer to set how many sampling points on each layer are taken, the default value is 10 . For some dataset, if the images contain too many zero pixels (eg. black color), the value should be greater. The users can also set using how many batches to do channel selection and feature reconstruction by nb_batches , the default value is 60 . Small value of nb_batches may cause over-fitting and large value may slow down the solving speed, so a good value depends on the nets and dataset. For more practical usage, user may consider make the channel number of each layer is the quadruple for fast inference of mobile devices. In this case, user can set --quadruple to True to make the compressed model have a quadruple number of channels.","title":"Channel pruning parameters"},{"location":"cp_learner/#distilling","text":"Distilling is an effective approach to improve the final accuracy of compressed model with PocketFlow in most situations of classification. User can set --enbl_dst=True to enable distillling.","title":"Distilling"},{"location":"cp_learner/#group-tuning","text":"As introduced above, group tuning was proposed by the PocketFlow team and finding it is very useful to improve the performance of model compression. In PocketFlow, users can set --finetune=True to enable group finetuning and set the group number by --list_group , the default value is 1000 . There is a trade-off between the small value and large value, because if the value is 1 , Pocketflow will prune convolution and finetune/retrain by each layer, which may have better effect but be more time-consuming. If we set the value large, the function will be less effective. User can also set the number of iterations to finetune by setting nb_iters_ft_ratio which mean the ratio the total iterations to be used in finetuning. The learning rate of finetuning can be set by lrn_rate_ft . [1] He, Y., Zhang, X. and Sun, J., 2017, October. Channel pruning for accelerating very deep neural networks. In International Conference on Computer Vision (ICCV) (Vol. 2, No. 6). [2] He, Y., Lin, J., Liu, Z., Wang, H., Li, L.J. and Han, S., 2018, September. Amc: Automl for model compression and acceleration on mobile devices. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 784-800). [3] Hinton, G., Vinyals, O. and Dean, J., 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 .","title":"Group Tuning"},{"location":"dcp_learner/","text":"Discrimination-aware Channel Pruning Introduction Discrimination-aware channel pruning (DCP, Zhuang et al., 2018) introduces a group of additional discriminative losses into the network to be pruned, to find out which channels are really contributing to the discriminative power and should be preserved. After channel pruning, the number of input channels of each convolutional layer is reduced, so that the model becomes smaller and the inference speed can be improved. Algorithm Description For a convolutional layer, we denote its input feature map as \\mathbf{X} \\in \\mathbb{R}^{N \\times c_{i} \\times h_{i} \\times w_{i}} \\mathbf{X} \\in \\mathbb{R}^{N \\times c_{i} \\times h_{i} \\times w_{i}} , where N N is the batch size, c_{i} c_{i} is the number of inputs channels, and h_{i} h_{i} and w_{i} w_{i} are the spatial height and width. The convolutional kernel is denoted as \\mathbf{W} \\in \\mathbb{R}^{c_{o} \\times c_{i} \\times k \\times k} \\mathbf{W} \\in \\mathbb{R}^{c_{o} \\times c_{i} \\times k \\times k} , where c_{o} c_{o} is the number of output channels and k k is the kernel size. The resulting output feature map is given by \\mathbf{Y} = f \\left( \\mathbf{X}; \\mathbf{W} \\right) \\mathbf{Y} = f \\left( \\mathbf{X}; \\mathbf{W} \\right) , where f \\left( \\cdot \\right) f \\left( \\cdot \\right) represents the convolutional operation. The idea of channel pruning is to impose the sparsity constraint on the convolutional kernel, so that some of its input channels only contains all-zero weights and can be safely removed. For instance, if the convolutional kernel satisifies: \\left\\| \\left\\| \\mathbf{W}_{:, j, :, :} \\right\\|_{F}^{2} \\right\\|_{0} = c'_{i}, \\left\\| \\left\\| \\mathbf{W}_{:, j, :, :} \\right\\|_{F}^{2} \\right\\|_{0} = c'_{i}, where c'_{i} \\lt c_{i} c'_{i} \\lt c_{i} , then the convolutional layer simplified to with c'_{i} c'_{i} input channels only, and the computational complexity is reduced by a ratio of \\frac{c_{i} - c'_{i}}{c_{i}} \\frac{c_{i} - c'_{i}}{c_{i}} . In order to reduce the performance degradation caused by channel pruning, the DCP algorithm introduces a novel channel selection algorithm by incorporating additional discrimination-aware and reconstruction loss terms, as shown below. Source: Zhuang et al., Discrimination-aware Channel Pruning for Deep Neural Networks . NIPS '18. The network is evenly divided into \\left( P + 1 \\right) \\left( P + 1 \\right) blocks. For each of the first P P blocks, an extra branch is derived from the output feature map of this block's last layer. The output feature map is then passed through batch normalization ReLU average pooling softmax layers to produce predictions, from which a discrimination-aware loss is constructed, denoted as L_{p} L_{p} . For the last block, the final loss of whole network, denoted as L L , is used as its discrimination-aware loss. Additionally, for each layer in the channel pruned network, a reconstruction loss is introduced to force it to re-produce the corresponding output feature map in the original network. We denote the q q -th layer's reconstruction loss as L_{q}^{( R )} L_{q}^{( R )} . Based on a pre-trained model, the DCP algorithm performs channel pruning with \\left( P + 1 \\right) \\left( P + 1 \\right) stages. During the p p -th stage, the network is fine-tuned with the p p -th discrimination-aware loss L_{p} L_{p} plus the final loss L L . After the block-wise fine-tuning, we sequentially perform channel pruning for each convolutional layer within the block. For channel pruning, we compute each input channel's gradients w.r.t. the reconstruction loss L_{q}^{( R )} L_{q}^{( R )} plus the discrimination-aware loss L_{p} L_{p} , and remove the input channel with the minimal Frobenius norm of gradients. After that, this layer is fine-tuned with the remaining input channels only to (partially) recover the discriminative power. We repeat this process until the target pruning ratio is reached. After all convolutional layers have been pruned, the resulting network can be further fine-tuned for a few epochs to further reduce the performance loss. Hyper-parameters Below is the full list of hyper-parameters used in the discrimination-aware channel pruning learner: Name Description dcp_save_path model's save path dcp_save_path_eval model's save path for evaluation dcp_prune_ratio target pruning ratio dcp_nb_stages number of channel pruning stages dcp_lrn_rate_adam Adam's learning rate for block-wise layer-wise fine-tuning dcp_nb_iters_block number of iterations for block-wise fine-tuning dcp_nb_iters_layer number of iterations for layer-wise fine-tuning Here, we provide detailed description (and some analysis) for above hyper-parameters: dcp_save_path : save path for model created in the training graph. The resulting checkpoint files can be used to resume training from a previous run and compute model's loss function's value and some other evaluation metrics. dcp_save_path_eval : save path for model created in the evaluation graph. The resulting checkpoint files can be used to export GraphDef TensorFlow Lite model files. dcp_prune_ratio : target pruning ratio for input channels of each convolutional layer. The larger dcp_prune_ratio is, the more input channels will be pruned. If dcp_prune_ratio equals 0, then no input channels will be pruned and model remains the same; if dcp_prune_ratio equals 1, then all input channels will be pruned. dcp_nb_stages : number of channel pruning stages / number of discrimination-aware losses. The training process of DCP algorithm is divided into multiple stages. For each discrimination-aware loss, a channel pruning stage is involved to select channels within corresponding layers. The final classification loss corresponds to a pseudo channel pruning stage, which is not counted in dcp_nb_stages .The larger dcp_nb_stages is, the slower the training process will be. dcp_lrn_rate_adam : Adam's learning rate for block-wise layer-wise fine-tuning. If dcp_lrn_rate_adam is too large, then the fine-tuning process may become unstable; if dcp_lrn_rate_adam is too small, then the fine-tuning process may take long time to converge. dcp_nb_iters_block : number of iterations for block-wise fine-tuning. This should be set to some value that the block-wise fine-tuning can almost converge and the loss function's value does not decrease much even if more iterations are used. dcp_nb_iters_layer : number of iterations for layer-wise fine-tuning. This should be set to some value that the layer-wise fine-tuning can almost converge and the loss function's value does not decrease much even if more iterations are used. Usage Examples In this section, we provide some usage examples to demonstrate how to use DisChnPrunedLearner under different execution modes and hyper-parameter combinations: To compress a ResNet-20 model for CIFAR-10 classification task in the local mode, use: # set the target pruning ratio to 0.75 ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner dis-chn-pruned \\ --dcp_prune_ratio 0.75 To compress a ResNet-34 model for ILSVRC-12 classification task in the docker mode with 4 GPUs, use: # set the number of channel pruning stages to 4 ./scripts/run_docker.sh nets/resnet_at_ilsvrc12_run.py -n=4 \\ --learner dis-chn-pruned \\ --data_disk docker \\ --resnet_size 34 \\ --dcp_nb_stages 4 To compress a MobileNet-v2 model for ILSVRC-12 classification task in the seven mode with 8 GPUs, use: # enable training with distillation loss ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py -n=8 \\ --learner dis-chn-pruned \\ --data_disk seven \\ --mobilenet_version 2 \\ --enbl_dst","title":"Discrimination-aware Channel Pruning"},{"location":"dcp_learner/#discrimination-aware-channel-pruning","text":"","title":"Discrimination-aware Channel Pruning"},{"location":"dcp_learner/#introduction","text":"Discrimination-aware channel pruning (DCP, Zhuang et al., 2018) introduces a group of additional discriminative losses into the network to be pruned, to find out which channels are really contributing to the discriminative power and should be preserved. After channel pruning, the number of input channels of each convolutional layer is reduced, so that the model becomes smaller and the inference speed can be improved.","title":"Introduction"},{"location":"dcp_learner/#algorithm-description","text":"For a convolutional layer, we denote its input feature map as \\mathbf{X} \\in \\mathbb{R}^{N \\times c_{i} \\times h_{i} \\times w_{i}} \\mathbf{X} \\in \\mathbb{R}^{N \\times c_{i} \\times h_{i} \\times w_{i}} , where N N is the batch size, c_{i} c_{i} is the number of inputs channels, and h_{i} h_{i} and w_{i} w_{i} are the spatial height and width. The convolutional kernel is denoted as \\mathbf{W} \\in \\mathbb{R}^{c_{o} \\times c_{i} \\times k \\times k} \\mathbf{W} \\in \\mathbb{R}^{c_{o} \\times c_{i} \\times k \\times k} , where c_{o} c_{o} is the number of output channels and k k is the kernel size. The resulting output feature map is given by \\mathbf{Y} = f \\left( \\mathbf{X}; \\mathbf{W} \\right) \\mathbf{Y} = f \\left( \\mathbf{X}; \\mathbf{W} \\right) , where f \\left( \\cdot \\right) f \\left( \\cdot \\right) represents the convolutional operation. The idea of channel pruning is to impose the sparsity constraint on the convolutional kernel, so that some of its input channels only contains all-zero weights and can be safely removed. For instance, if the convolutional kernel satisifies: \\left\\| \\left\\| \\mathbf{W}_{:, j, :, :} \\right\\|_{F}^{2} \\right\\|_{0} = c'_{i}, \\left\\| \\left\\| \\mathbf{W}_{:, j, :, :} \\right\\|_{F}^{2} \\right\\|_{0} = c'_{i}, where c'_{i} \\lt c_{i} c'_{i} \\lt c_{i} , then the convolutional layer simplified to with c'_{i} c'_{i} input channels only, and the computational complexity is reduced by a ratio of \\frac{c_{i} - c'_{i}}{c_{i}} \\frac{c_{i} - c'_{i}}{c_{i}} . In order to reduce the performance degradation caused by channel pruning, the DCP algorithm introduces a novel channel selection algorithm by incorporating additional discrimination-aware and reconstruction loss terms, as shown below. Source: Zhuang et al., Discrimination-aware Channel Pruning for Deep Neural Networks . NIPS '18. The network is evenly divided into \\left( P + 1 \\right) \\left( P + 1 \\right) blocks. For each of the first P P blocks, an extra branch is derived from the output feature map of this block's last layer. The output feature map is then passed through batch normalization ReLU average pooling softmax layers to produce predictions, from which a discrimination-aware loss is constructed, denoted as L_{p} L_{p} . For the last block, the final loss of whole network, denoted as L L , is used as its discrimination-aware loss. Additionally, for each layer in the channel pruned network, a reconstruction loss is introduced to force it to re-produce the corresponding output feature map in the original network. We denote the q q -th layer's reconstruction loss as L_{q}^{( R )} L_{q}^{( R )} . Based on a pre-trained model, the DCP algorithm performs channel pruning with \\left( P + 1 \\right) \\left( P + 1 \\right) stages. During the p p -th stage, the network is fine-tuned with the p p -th discrimination-aware loss L_{p} L_{p} plus the final loss L L . After the block-wise fine-tuning, we sequentially perform channel pruning for each convolutional layer within the block. For channel pruning, we compute each input channel's gradients w.r.t. the reconstruction loss L_{q}^{( R )} L_{q}^{( R )} plus the discrimination-aware loss L_{p} L_{p} , and remove the input channel with the minimal Frobenius norm of gradients. After that, this layer is fine-tuned with the remaining input channels only to (partially) recover the discriminative power. We repeat this process until the target pruning ratio is reached. After all convolutional layers have been pruned, the resulting network can be further fine-tuned for a few epochs to further reduce the performance loss.","title":"Algorithm Description"},{"location":"dcp_learner/#hyper-parameters","text":"Below is the full list of hyper-parameters used in the discrimination-aware channel pruning learner: Name Description dcp_save_path model's save path dcp_save_path_eval model's save path for evaluation dcp_prune_ratio target pruning ratio dcp_nb_stages number of channel pruning stages dcp_lrn_rate_adam Adam's learning rate for block-wise layer-wise fine-tuning dcp_nb_iters_block number of iterations for block-wise fine-tuning dcp_nb_iters_layer number of iterations for layer-wise fine-tuning Here, we provide detailed description (and some analysis) for above hyper-parameters: dcp_save_path : save path for model created in the training graph. The resulting checkpoint files can be used to resume training from a previous run and compute model's loss function's value and some other evaluation metrics. dcp_save_path_eval : save path for model created in the evaluation graph. The resulting checkpoint files can be used to export GraphDef TensorFlow Lite model files. dcp_prune_ratio : target pruning ratio for input channels of each convolutional layer. The larger dcp_prune_ratio is, the more input channels will be pruned. If dcp_prune_ratio equals 0, then no input channels will be pruned and model remains the same; if dcp_prune_ratio equals 1, then all input channels will be pruned. dcp_nb_stages : number of channel pruning stages / number of discrimination-aware losses. The training process of DCP algorithm is divided into multiple stages. For each discrimination-aware loss, a channel pruning stage is involved to select channels within corresponding layers. The final classification loss corresponds to a pseudo channel pruning stage, which is not counted in dcp_nb_stages .The larger dcp_nb_stages is, the slower the training process will be. dcp_lrn_rate_adam : Adam's learning rate for block-wise layer-wise fine-tuning. If dcp_lrn_rate_adam is too large, then the fine-tuning process may become unstable; if dcp_lrn_rate_adam is too small, then the fine-tuning process may take long time to converge. dcp_nb_iters_block : number of iterations for block-wise fine-tuning. This should be set to some value that the block-wise fine-tuning can almost converge and the loss function's value does not decrease much even if more iterations are used. dcp_nb_iters_layer : number of iterations for layer-wise fine-tuning. This should be set to some value that the layer-wise fine-tuning can almost converge and the loss function's value does not decrease much even if more iterations are used.","title":"Hyper-parameters"},{"location":"dcp_learner/#usage-examples","text":"In this section, we provide some usage examples to demonstrate how to use DisChnPrunedLearner under different execution modes and hyper-parameter combinations: To compress a ResNet-20 model for CIFAR-10 classification task in the local mode, use: # set the target pruning ratio to 0.75 ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner dis-chn-pruned \\ --dcp_prune_ratio 0.75 To compress a ResNet-34 model for ILSVRC-12 classification task in the docker mode with 4 GPUs, use: # set the number of channel pruning stages to 4 ./scripts/run_docker.sh nets/resnet_at_ilsvrc12_run.py -n=4 \\ --learner dis-chn-pruned \\ --data_disk docker \\ --resnet_size 34 \\ --dcp_nb_stages 4 To compress a MobileNet-v2 model for ILSVRC-12 classification task in the seven mode with 8 GPUs, use: # enable training with distillation loss ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py -n=8 \\ --learner dis-chn-pruned \\ --data_disk seven \\ --mobilenet_version 2 \\ --enbl_dst","title":"Usage Examples"},{"location":"distillation/","text":"Knowledge Distilation wledge Distillation Knowledge Distillation is a kind of model compression approach in which a pre-trained large model teaches a smaller model to achieve the similar prediction capacity. It often named 'teacher-student' training, where the large model is the teacher and the smaller model is the student. The implementation used in PocketFlow is based on Hinton et al., 2015 . With distillation, knowledge can be transferred from the teacher model to the student by minimizing a loss function where the target is the distribution of class probabilities predicted by the teacher model. In most situations, the probability of the correct class predicted by the teacher model is very high, and the probabilities of other classes are closed to 0, which may provide very limited information beyond the ground truth labels. We can use the class probabilities produced by the cumbersome model as \"soft targets\" for training the small model. The solution is to raise the temperature of the final softmax until the cumbersome model produces a suitably soft set of targets. The probability q_i q_i of class i i is calculated from the logits z_i z_i : q_i = \\frac{exp(z_i / T)}{\\sum_j{exp(z_j/T)}} q_i = \\frac{exp(z_i / T)}{\\sum_j{exp(z_j/T)}} where T T is the temperature. As T T grows, the probability distribution is more softer, providing more information as to which classes the cumbersome model more similar to the predicted class. It is better to include the standard loss ( T=1 T=1 ) between the predicted class probabilities and the ground-truth labels. The overall loss function is: L(x;W)=H(y,\\sigma(z_s;T=1))+\\alpha*H(\\sigma(z_t;T=\\tau),\\sigma(z_s,T=\\tau)) L(x;W)=H(y,\\sigma(z_s;T=1))+\\alpha*H(\\sigma(z_t;T=\\tau),\\sigma(z_s,T=\\tau)) where x x is the input, W W are parameters of the distilled small model and y y is the ground truth label, \\sigma \\sigma is the softmax parameterized by temperature T T , H H is the cross entropy loss. \\alpha \\alpha is the coefficient of the distillation loss. User can set \\alpha \\alpha by loss_w_dst and set temperature \\tau \\tau by --tempr_dst . Compression with Other Model Compression Approaches For other model model compression techniques such as channel pruning, weight pruning and quantization can be combined with the knowledge distilling. To use it, just enable it by setting --enbl_dst=True . [1] Hinton, G., Vinyals, O. and Dean, J., 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 .","title":"Distillation"},{"location":"distillation/#knowledge-distilation","text":"wledge Distillation Knowledge Distillation is a kind of model compression approach in which a pre-trained large model teaches a smaller model to achieve the similar prediction capacity. It often named 'teacher-student' training, where the large model is the teacher and the smaller model is the student. The implementation used in PocketFlow is based on Hinton et al., 2015 . With distillation, knowledge can be transferred from the teacher model to the student by minimizing a loss function where the target is the distribution of class probabilities predicted by the teacher model. In most situations, the probability of the correct class predicted by the teacher model is very high, and the probabilities of other classes are closed to 0, which may provide very limited information beyond the ground truth labels. We can use the class probabilities produced by the cumbersome model as \"soft targets\" for training the small model. The solution is to raise the temperature of the final softmax until the cumbersome model produces a suitably soft set of targets. The probability q_i q_i of class i i is calculated from the logits z_i z_i : q_i = \\frac{exp(z_i / T)}{\\sum_j{exp(z_j/T)}} q_i = \\frac{exp(z_i / T)}{\\sum_j{exp(z_j/T)}} where T T is the temperature. As T T grows, the probability distribution is more softer, providing more information as to which classes the cumbersome model more similar to the predicted class. It is better to include the standard loss ( T=1 T=1 ) between the predicted class probabilities and the ground-truth labels. The overall loss function is: L(x;W)=H(y,\\sigma(z_s;T=1))+\\alpha*H(\\sigma(z_t;T=\\tau),\\sigma(z_s,T=\\tau)) L(x;W)=H(y,\\sigma(z_s;T=1))+\\alpha*H(\\sigma(z_t;T=\\tau),\\sigma(z_s,T=\\tau)) where x x is the input, W W are parameters of the distilled small model and y y is the ground truth label, \\sigma \\sigma is the softmax parameterized by temperature T T , H H is the cross entropy loss. \\alpha \\alpha is the coefficient of the distillation loss. User can set \\alpha \\alpha by loss_w_dst and set temperature \\tau \\tau by --tempr_dst .","title":"Knowledge Distilation"},{"location":"distillation/#compression-with-other-model-compression-approaches","text":"For other model model compression techniques such as channel pruning, weight pruning and quantization can be combined with the knowledge distilling. To use it, just enable it by setting --enbl_dst=True . [1] Hinton, G., Vinyals, O. and Dean, J., 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 .","title":"Compression with Other Model Compression Approaches"},{"location":"faq/","text":"Frequently Asked Questions Q: Under construction ... A: Under construction ...","title":"Frequently Asked Questions"},{"location":"faq/#frequently-asked-questions","text":"Q: Under construction ... A: Under construction ...","title":"Frequently Asked Questions"},{"location":"installation/","text":"Installation PocketFlow is developed and tested on Linux, using Python 3.6 and TensorFlow 1.10.0. We support the following three execution modes for PocketFlow: Local mode: run PocketFlow on the local machine. Docker mode: run PocketFlow within a docker image. Seven mode: run PocketFlow on the seven cluster (only available within Tencent). Clone PocketFlow To make a local copy of the PocketFlow repository, use: $ git clone http://git.code.oa.com/ml/PocketFlow.git Create a Path Configuration File PocketFlow requires a path configuration file, named path.conf , to setup directory paths to data sets and pre-trained models under different execution modes, as well as HDFS / HTTP connection parameters. We have provided a template file to help you create your own path configuration file. You can find it in the PocketFlow repository, named path.conf.template , which contains more detailed descriptions on how to customize path configurations. For instance, if you want to use CIFAR-10 and ImageNet data sets stored on the local machine, then the path configuration file should look like this: # data files data_hdfs_host = None data_dir_local_cifar10 = /home/user_name/datasets/cifar-10-batches-bin # this line has been edited! data_dir_hdfs_cifar10 = None data_dir_seven_cifar10 = None data_dir_docker_cifar10 = /opt/ml/data # DO NOT EDIT data_dir_local_ilsvrc12 = /home/user_name/datasets/imagenet_tfrecord # this line has been edited! data_dir_hdfs_ilsvrc12 = None data_dir_seven_ilsvrc12 = None data_dir_docker_ilsvrc12 = /opt/ml/data # DO NOT EDIT # model files model_http_url = https://api.ai.tencent.com/pocketflow In short, you need to replace \"None\" in the template file with the actual path (or HDFS / HTTP connection parameters) if available, or leave it unchanged otherwise. Prepare for the Local Mode We recommend to use Anaconda as the Python environment, which has many essential packages built-in. The Anaconda installer can be downloaded from here . To install, use the following command: # install Anaconda; replace the installer's file name if needed $ bash Anaconda3-5.2.0-Linux-x86_64.sh # activate Anaconda's Python path $ source ~/.bashrc For Anaconda 5.3.0 or later, the default Python version is 3.7, which does not support installing TensorFlow with pip directly. Therefore, you need to manually switch to Python 3.6 once Anaconda is installed: # install Python 3.6 $ conda install python=3.6 To install TensorFlow, you may refer to TensorFlow's official documentation for detailed instructions. Specially, if GPU-based training is required, then you need to follow the GPU support guide to set up a CUDA-enabled GPU card in prior to installation. After that, install TensorFlow with: # TensorFlow with GPU support; use tensorflow if GPU is not available $ pip install tensorflow-gpu # verify the install $ python -c import tensorflow as tf; print(tf.__version__) To run PocketFlow in the local mode, e.g. to train a full-precision ResNet-20 model for the CIFAR-10 classification task, use the following command: $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py Prepare for the Docker Mode Docker offers an alternative way to run PocketFlow within an isolated container, so that your local Python environment remains untouched. We recommend you to use the horovod docker provided by Uber, which enables multi-GPU distributed training for TensorFlow with only a few lines modification. Once docker is installed, the docker image can be obtained via: # obtain the docker image $ docker pull uber/horovod To run PocketFlow in the docker mode, e.g. to train a full-precision ResNet-20 model for the CIFAR-10 classification task, use the following command: $ ./scripts/run_docker.sh nets/resnet_at_cifar10_run.py Prepare for the Seven Mode Seven is a distributed learning platform built for both CPU and GPU clusters. Users can submit tasks to the seven cluster, using built-in data sets and docker images seamlessly. For detailed introduction, please refer to its documentation site . To run PocketFlow in the seven mode, e.g. to train a full-precision ResNet-20 model for the CIFAR-10 classification task, use the following command: $ ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py","title":"Installation"},{"location":"installation/#installation","text":"PocketFlow is developed and tested on Linux, using Python 3.6 and TensorFlow 1.10.0. We support the following three execution modes for PocketFlow: Local mode: run PocketFlow on the local machine. Docker mode: run PocketFlow within a docker image. Seven mode: run PocketFlow on the seven cluster (only available within Tencent).","title":"Installation"},{"location":"installation/#clone-pocketflow","text":"To make a local copy of the PocketFlow repository, use: $ git clone http://git.code.oa.com/ml/PocketFlow.git","title":"Clone PocketFlow"},{"location":"installation/#create-a-path-configuration-file","text":"PocketFlow requires a path configuration file, named path.conf , to setup directory paths to data sets and pre-trained models under different execution modes, as well as HDFS / HTTP connection parameters. We have provided a template file to help you create your own path configuration file. You can find it in the PocketFlow repository, named path.conf.template , which contains more detailed descriptions on how to customize path configurations. For instance, if you want to use CIFAR-10 and ImageNet data sets stored on the local machine, then the path configuration file should look like this: # data files data_hdfs_host = None data_dir_local_cifar10 = /home/user_name/datasets/cifar-10-batches-bin # this line has been edited! data_dir_hdfs_cifar10 = None data_dir_seven_cifar10 = None data_dir_docker_cifar10 = /opt/ml/data # DO NOT EDIT data_dir_local_ilsvrc12 = /home/user_name/datasets/imagenet_tfrecord # this line has been edited! data_dir_hdfs_ilsvrc12 = None data_dir_seven_ilsvrc12 = None data_dir_docker_ilsvrc12 = /opt/ml/data # DO NOT EDIT # model files model_http_url = https://api.ai.tencent.com/pocketflow In short, you need to replace \"None\" in the template file with the actual path (or HDFS / HTTP connection parameters) if available, or leave it unchanged otherwise.","title":"Create a Path Configuration File"},{"location":"installation/#prepare-for-the-local-mode","text":"We recommend to use Anaconda as the Python environment, which has many essential packages built-in. The Anaconda installer can be downloaded from here . To install, use the following command: # install Anaconda; replace the installer's file name if needed $ bash Anaconda3-5.2.0-Linux-x86_64.sh # activate Anaconda's Python path $ source ~/.bashrc For Anaconda 5.3.0 or later, the default Python version is 3.7, which does not support installing TensorFlow with pip directly. Therefore, you need to manually switch to Python 3.6 once Anaconda is installed: # install Python 3.6 $ conda install python=3.6 To install TensorFlow, you may refer to TensorFlow's official documentation for detailed instructions. Specially, if GPU-based training is required, then you need to follow the GPU support guide to set up a CUDA-enabled GPU card in prior to installation. After that, install TensorFlow with: # TensorFlow with GPU support; use tensorflow if GPU is not available $ pip install tensorflow-gpu # verify the install $ python -c import tensorflow as tf; print(tf.__version__) To run PocketFlow in the local mode, e.g. to train a full-precision ResNet-20 model for the CIFAR-10 classification task, use the following command: $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py","title":"Prepare for the Local Mode"},{"location":"installation/#prepare-for-the-docker-mode","text":"Docker offers an alternative way to run PocketFlow within an isolated container, so that your local Python environment remains untouched. We recommend you to use the horovod docker provided by Uber, which enables multi-GPU distributed training for TensorFlow with only a few lines modification. Once docker is installed, the docker image can be obtained via: # obtain the docker image $ docker pull uber/horovod To run PocketFlow in the docker mode, e.g. to train a full-precision ResNet-20 model for the CIFAR-10 classification task, use the following command: $ ./scripts/run_docker.sh nets/resnet_at_cifar10_run.py","title":"Prepare for the Docker Mode"},{"location":"installation/#prepare-for-the-seven-mode","text":"Seven is a distributed learning platform built for both CPU and GPU clusters. Users can submit tasks to the seven cluster, using built-in data sets and docker images seamlessly. For detailed introduction, please refer to its documentation site . To run PocketFlow in the seven mode, e.g. to train a full-precision ResNet-20 model for the CIFAR-10 classification task, use the following command: $ ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py","title":"Prepare for the Seven Mode"},{"location":"multi_gpu_training/","text":"Multi-GPU Training Due to the high computational complexity, it often takes hours or even days to fully train deep learning models using a single GPU. In PocketFlow, we adopt multi-GPU training to speed-up this time-consuming training process. Our implementation is compatible with: Horovod : a distributed training framework for TensorFlow, Keras, and PyTorch. TF-Plus : an optimized framework for TensorFlow-based distributed training (only available within Tencent). We have provide a wrapper class, MultiGpuWrapper , to seamlessly switch between the above two frameworks. It will sequentially check whether Horovod and TF-Plus can be used, and use the first available one as the underlying framework for multi-GPU training. The main reason that using Horovod or TF-Plus instead TensorFlow's original distributed training routine is that these framewors provide many easy-to-use APIs and require far less code changes to change from single-GPU to multi-GPU training, as we shall see later. From Single-GPU to Multi-GPU To extend a single-GPU based training script to the multi-GPU scenario, at most 7 steps are needed: Import the Horovod or TF-Plus module. from utils.multi_gpu_wrapper import MultiGpuWrapper as mgw Initialize the multi-GPU training framework, as early as possible. mgw.init() For each worker, create a session with a distinct GPU device. config = tf.ConfigProto() config.gpu_options.visible_device_list = str(mgw.local_rank()) sess = tf.Session(config=config) (Optional) Let each worker use a distinct subset of training data. filenames = tf.data.Dataset.list_files(file_pattern, shuffle=True) filenames = filenames.shard(mgw.size(), mgw.rank()) Wrapper the optimizer for distributed gradient communication. optimizer = tf.train.AdamOptimizer(learning_rate=lrn_rate) optimizer = mgw.DistributedOptimizer(optimizer) train_op = optimizer.minimize(loss) Synchronize master's parameters to all the other workers. bcast_op = mgw.broadcast_global_variables(0) sess.run(tf.global_variables_initializer()) sess.run(bcast_op) (Optional) Save checkpoint files at the master node periodically. if mgw.rank() == 0: saver.save(sess, save_path, global_step) Usage Example Here, we provide a code snippet to demonstrate how to use multi-GPU training to speed-up training. Please note that many implementation details are omitted for clarity. import tensorflow as tf from utils.multi_gpu_wrapper import MultiGpuWrapper as mgw # initialization mgw.init() # create the training graph with tf.Graph().as_default(): # create a TensorFlow session config = tf.ConfigProto() config.gpu_options.visible_device_list = str(mgw.local_rank()) sess = tf.Session(config=config) # use tf.data.Dataset() to traverse images and labels filenames = tf.data.Dataset.list_files(file_pattern, shuffle=True) filenames = filenames.shard(mgw.size(), mgw.rank()) images, labels = get_images_n_labels(filenames) # define the network and its loss function logits = forward_pass(images) loss = calc_loss(labels, logits) # create an optimizer and setup training-related operations global_step = tf.train.get_or_create_global_step() optimizer = tf.train.AdamOptimizer(learning_rate=lrn_rate) optimizer = mgw.DistributedOptimizer(optimizer) train_op = optimizer.minimize(loss, global_step=global_step) bcast_op = mgw.broadcast_global_variables(0) # multi-GPU training sess.run(tf.global_variables_initializer()) sess.run(bcast_op) for idx_iter in range(nb_iters): sess.run(train_op) if mgw.rank() == 0 and (idx_iter + 1) % save_step == 0: saver.save(sess, save_path, global_step)","title":"Multi-GPU Training"},{"location":"multi_gpu_training/#multi-gpu-training","text":"Due to the high computational complexity, it often takes hours or even days to fully train deep learning models using a single GPU. In PocketFlow, we adopt multi-GPU training to speed-up this time-consuming training process. Our implementation is compatible with: Horovod : a distributed training framework for TensorFlow, Keras, and PyTorch. TF-Plus : an optimized framework for TensorFlow-based distributed training (only available within Tencent). We have provide a wrapper class, MultiGpuWrapper , to seamlessly switch between the above two frameworks. It will sequentially check whether Horovod and TF-Plus can be used, and use the first available one as the underlying framework for multi-GPU training. The main reason that using Horovod or TF-Plus instead TensorFlow's original distributed training routine is that these framewors provide many easy-to-use APIs and require far less code changes to change from single-GPU to multi-GPU training, as we shall see later.","title":"Multi-GPU Training"},{"location":"multi_gpu_training/#from-single-gpu-to-multi-gpu","text":"To extend a single-GPU based training script to the multi-GPU scenario, at most 7 steps are needed: Import the Horovod or TF-Plus module. from utils.multi_gpu_wrapper import MultiGpuWrapper as mgw Initialize the multi-GPU training framework, as early as possible. mgw.init() For each worker, create a session with a distinct GPU device. config = tf.ConfigProto() config.gpu_options.visible_device_list = str(mgw.local_rank()) sess = tf.Session(config=config) (Optional) Let each worker use a distinct subset of training data. filenames = tf.data.Dataset.list_files(file_pattern, shuffle=True) filenames = filenames.shard(mgw.size(), mgw.rank()) Wrapper the optimizer for distributed gradient communication. optimizer = tf.train.AdamOptimizer(learning_rate=lrn_rate) optimizer = mgw.DistributedOptimizer(optimizer) train_op = optimizer.minimize(loss) Synchronize master's parameters to all the other workers. bcast_op = mgw.broadcast_global_variables(0) sess.run(tf.global_variables_initializer()) sess.run(bcast_op) (Optional) Save checkpoint files at the master node periodically. if mgw.rank() == 0: saver.save(sess, save_path, global_step)","title":"From Single-GPU to Multi-GPU"},{"location":"multi_gpu_training/#usage-example","text":"Here, we provide a code snippet to demonstrate how to use multi-GPU training to speed-up training. Please note that many implementation details are omitted for clarity. import tensorflow as tf from utils.multi_gpu_wrapper import MultiGpuWrapper as mgw # initialization mgw.init() # create the training graph with tf.Graph().as_default(): # create a TensorFlow session config = tf.ConfigProto() config.gpu_options.visible_device_list = str(mgw.local_rank()) sess = tf.Session(config=config) # use tf.data.Dataset() to traverse images and labels filenames = tf.data.Dataset.list_files(file_pattern, shuffle=True) filenames = filenames.shard(mgw.size(), mgw.rank()) images, labels = get_images_n_labels(filenames) # define the network and its loss function logits = forward_pass(images) loss = calc_loss(labels, logits) # create an optimizer and setup training-related operations global_step = tf.train.get_or_create_global_step() optimizer = tf.train.AdamOptimizer(learning_rate=lrn_rate) optimizer = mgw.DistributedOptimizer(optimizer) train_op = optimizer.minimize(loss, global_step=global_step) bcast_op = mgw.broadcast_global_variables(0) # multi-GPU training sess.run(tf.global_variables_initializer()) sess.run(bcast_op) for idx_iter in range(nb_iters): sess.run(train_op) if mgw.rank() == 0 and (idx_iter + 1) % save_step == 0: saver.save(sess, save_path, global_step)","title":"Usage Example"},{"location":"nuq_learner/","text":"Non-Uniform Quantization Learner This document describes how to set up the Non-Uniform Quantization Learner in PocketFlow. In non-uniform quantization, the quantization points are not distributed evenly, and can be optimized via the back-propagation of the network gradients. Consequently, with the same number of bits, non-uniform quantization is more expressive to approximate the original full-precision network comparing to uniform quantization. Following a similar pattern in the previous sections, we first show how to configure the Non-Uniform Quantization Learner, followed by the algorithms used in the learner. Prepare the Model Again, users should first get the model prepared. Users can either use the pre-built models in PocketFlow, or develop their custom models according to TODO . Configure the Learner To configure the learner, users can pass the options via the TensorFlow flag interface. The available options are as follows: Options Default Value Description --nuql_opt_mode weight variables to optimize: ['weights', 'clusters', 'both'] --nuql_init_style quantile the initialization of quantization points: ['quantile', 'uniform'] --nuql_weight_bits 4 the number of bits for weight --nuql_activation_bits 32 the number of bits for activation\uff0c by default it remains full precision --nuql_save_quant_mode_path TODO the save path for quantized models --nuql_use_buckets False use bucketing or not --nuql_bucket_type channel two bucket type available: ['split', 'channel'] --nuql_bucket_size 256 quantize the first and last layers of the network or not --nuql_enbl_rl_agent False enable reinforcement learning to learn the optimal bit allocation or not --nuql_quantize_all_layers False quantize the first and last layers of the network or not --nuql_quant_epoch 60 the number of epochs for fine-tuning Note that since non-uniform quantization cannot be accelerated directly, by default we do not quantize the activations. Examples Once the model is built, the Non-Uniform Quantization Learner can be easily triggered by passing the Uniform Quantization Learner in the command line as follows: # quantize resnet-20 on CIFAR-10 # you can also configure the sh ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --data_disk local \\ --data_dir_local ${PF_CIFAR10_LOCAL} \\ --learner=non-uniform \\ --nuql_weight_bits=4 \\ --nuql_activation_bits=4 \\ # quantize the resnet-18 on ILSVRC-12 sh ./scripts/run_local.sh nets/resnet_at_ilsvrc12_run.py \\ --learner=uniform \\ --data_disk local \\ --data_dir_local ${PF_ILSVRC12_LOCAL} \\ --nuql_weight_bits=8 \\ --nuql_activation_bits=8 \\ --nuql_use_buckets=True \\ --nuql_bucket_type=channel To enable the RL agent, one can follow similar patterns as those in the Uniform Quantization Learner: # quantize mobilenet-v1 on ILSVRC-12 sh ./scripts/run_local.sh nets/mobilnet_at_ilsvrc12_run.py \\ --data_disk local \\ --data_dir_local ${PF_CIFAR10_LOCAL} \\ --learner=uniform \\ --nuql_enbl_rl_agent=True \\ --nuql_equivalent_bits=4 \\ --nuql_tune_global_steps=1200 Performance Here we list some of the performance on Cifar-10 using the Non-Uniform Quantization Learner and the built-in models in PocketFlow. The options not displayed remain the default values. Model Weight Bit Activation Bit Acc ResNet-20 32 32 91.96 ResNet-20 2 4 90.31 ResNet-20 4 8 91.70 Model Weight Bit Bucketing Acc ResNet-20 2 channel 90.90 ResNet-20 4 channel 91.97 ResNet-20 2 split 90.02 ResNet-20 4 split 91.56 Model Weight Bit RL search Acc ResNet-20 2 FALSE 90.31 ResNet-20 4 FALSE 91.70 ResNet-20 2 TRUE 90.60 ResNet-20 4 TRUE 91.79 Algorithm Non-Uniform Quantization Learner adopts a similar training and evaluation procedure to the Uniform Quantization. In the training process, the quantized weights are forwarded. In the backward pass, the full precision weights are updated via the STE estimator. The major difference from uniform quantization is that, the location of quantization points are not evenly distributed, but can be optimized and initialized differently. In the following, we introduce the scheme to update and initialize the quantization points. Optimization the quantization points Unlike uniform quantization, non-uniform quantization can optimize the location of quantization points dynamically during the training of the network, and thereon leads to less quantization loss. The location of quantization points can be updated by summing the gradients of weights that fall into the point ( Han et.al 2015 ), i.e.,: $$ \\frac{\\partial \\mathcal{L}}{\\partial c_k} = \\sum_{i,j}\\frac{\\partial\\mathcal{L}}{\\partial w_{ij}}\\frac{\\partial{w_{ij}}}{\\partial c_k}=\\sum_{ij}\\frac{\\partial\\mathcal{L}}{\\partial{w_{ij}}}1(I_{ij}=k) $$ The following figure taken from Han et.al 2015 shows the process of updating the clusters: Initialization of quantization points Aside from optimizing the quantization points, another helpful strategy is to properly initialize the quantization points according to the distribution of weights. PocketFlow currently supports two kinds of initialization: uniform initialization and quantile initialization. Comparing to uniform initialization, quantile initialization uses the quantiles of weights as the initial locations of quantization points. Quantile initialization considers the distribution of weights and can usually lead to better performance. References Han S, Mao H, and Dally W J. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv:1510.00149, 2015","title":"Non-uniform Quantization"},{"location":"nuq_learner/#non-uniform-quantization-learner","text":"This document describes how to set up the Non-Uniform Quantization Learner in PocketFlow. In non-uniform quantization, the quantization points are not distributed evenly, and can be optimized via the back-propagation of the network gradients. Consequently, with the same number of bits, non-uniform quantization is more expressive to approximate the original full-precision network comparing to uniform quantization. Following a similar pattern in the previous sections, we first show how to configure the Non-Uniform Quantization Learner, followed by the algorithms used in the learner.","title":"Non-Uniform Quantization Learner"},{"location":"nuq_learner/#prepare-the-model","text":"Again, users should first get the model prepared. Users can either use the pre-built models in PocketFlow, or develop their custom models according to TODO .","title":"Prepare the Model"},{"location":"nuq_learner/#configure-the-learner","text":"To configure the learner, users can pass the options via the TensorFlow flag interface. The available options are as follows: Options Default Value Description --nuql_opt_mode weight variables to optimize: ['weights', 'clusters', 'both'] --nuql_init_style quantile the initialization of quantization points: ['quantile', 'uniform'] --nuql_weight_bits 4 the number of bits for weight --nuql_activation_bits 32 the number of bits for activation\uff0c by default it remains full precision --nuql_save_quant_mode_path TODO the save path for quantized models --nuql_use_buckets False use bucketing or not --nuql_bucket_type channel two bucket type available: ['split', 'channel'] --nuql_bucket_size 256 quantize the first and last layers of the network or not --nuql_enbl_rl_agent False enable reinforcement learning to learn the optimal bit allocation or not --nuql_quantize_all_layers False quantize the first and last layers of the network or not --nuql_quant_epoch 60 the number of epochs for fine-tuning Note that since non-uniform quantization cannot be accelerated directly, by default we do not quantize the activations.","title":"Configure the Learner"},{"location":"nuq_learner/#examples","text":"Once the model is built, the Non-Uniform Quantization Learner can be easily triggered by passing the Uniform Quantization Learner in the command line as follows: # quantize resnet-20 on CIFAR-10 # you can also configure the sh ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --data_disk local \\ --data_dir_local ${PF_CIFAR10_LOCAL} \\ --learner=non-uniform \\ --nuql_weight_bits=4 \\ --nuql_activation_bits=4 \\ # quantize the resnet-18 on ILSVRC-12 sh ./scripts/run_local.sh nets/resnet_at_ilsvrc12_run.py \\ --learner=uniform \\ --data_disk local \\ --data_dir_local ${PF_ILSVRC12_LOCAL} \\ --nuql_weight_bits=8 \\ --nuql_activation_bits=8 \\ --nuql_use_buckets=True \\ --nuql_bucket_type=channel To enable the RL agent, one can follow similar patterns as those in the Uniform Quantization Learner: # quantize mobilenet-v1 on ILSVRC-12 sh ./scripts/run_local.sh nets/mobilnet_at_ilsvrc12_run.py \\ --data_disk local \\ --data_dir_local ${PF_CIFAR10_LOCAL} \\ --learner=uniform \\ --nuql_enbl_rl_agent=True \\ --nuql_equivalent_bits=4 \\ --nuql_tune_global_steps=1200","title":"Examples"},{"location":"nuq_learner/#performance","text":"Here we list some of the performance on Cifar-10 using the Non-Uniform Quantization Learner and the built-in models in PocketFlow. The options not displayed remain the default values. Model Weight Bit Activation Bit Acc ResNet-20 32 32 91.96 ResNet-20 2 4 90.31 ResNet-20 4 8 91.70 Model Weight Bit Bucketing Acc ResNet-20 2 channel 90.90 ResNet-20 4 channel 91.97 ResNet-20 2 split 90.02 ResNet-20 4 split 91.56 Model Weight Bit RL search Acc ResNet-20 2 FALSE 90.31 ResNet-20 4 FALSE 91.70 ResNet-20 2 TRUE 90.60 ResNet-20 4 TRUE 91.79","title":"Performance"},{"location":"nuq_learner/#algorithm","text":"Non-Uniform Quantization Learner adopts a similar training and evaluation procedure to the Uniform Quantization. In the training process, the quantized weights are forwarded. In the backward pass, the full precision weights are updated via the STE estimator. The major difference from uniform quantization is that, the location of quantization points are not evenly distributed, but can be optimized and initialized differently. In the following, we introduce the scheme to update and initialize the quantization points.","title":"Algorithm"},{"location":"nuq_learner/#optimization-the-quantization-points","text":"Unlike uniform quantization, non-uniform quantization can optimize the location of quantization points dynamically during the training of the network, and thereon leads to less quantization loss. The location of quantization points can be updated by summing the gradients of weights that fall into the point ( Han et.al 2015 ), i.e.,: $$ \\frac{\\partial \\mathcal{L}}{\\partial c_k} = \\sum_{i,j}\\frac{\\partial\\mathcal{L}}{\\partial w_{ij}}\\frac{\\partial{w_{ij}}}{\\partial c_k}=\\sum_{ij}\\frac{\\partial\\mathcal{L}}{\\partial{w_{ij}}}1(I_{ij}=k) $$ The following figure taken from Han et.al 2015 shows the process of updating the clusters:","title":"Optimization the quantization points"},{"location":"nuq_learner/#initialization-of-quantization-points","text":"Aside from optimizing the quantization points, another helpful strategy is to properly initialize the quantization points according to the distribution of weights. PocketFlow currently supports two kinds of initialization: uniform initialization and quantile initialization. Comparing to uniform initialization, quantile initialization uses the quantiles of weights as the initial locations of quantization points. Quantile initialization considers the distribution of weights and can usually lead to better performance.","title":"Initialization of quantization points"},{"location":"nuq_learner/#references","text":"Han S, Mao H, and Dally W J. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv:1510.00149, 2015","title":"References"},{"location":"performance/","text":"Performance Under construction ...","title":"Performance"},{"location":"performance/#performance","text":"Under construction ...","title":"Performance"},{"location":"pre_trained_models/","text":"Pre-trained Models We maintain a list of pre-trained uncompressed models, so that the training process of model compression does not need to start from scratch. For the CIFAR-10 data set, we provide following pre-trained models: Model name Accuracy URL LeNet 81.79% Link ResNet-20 91.93% Link ResNet-32 92.59% Link ResNet-44 92.76% Link ResNet-56 93.23% Link For the ImageNet (ILSVRC-12) data set, we provide following pre-trained models: Model name Top-1 Acc. Top-5 Acc. URL ResNet-18 70.28% 89.38% Link ResNet-34 73.41% 91.27% Link ResNet-50 75.97% 92.88% Link MobileNet-v1 70.89% 89.56% Link MobileNet-v2 71.84% 90.60% Link","title":"Pre-trained Models"},{"location":"pre_trained_models/#pre-trained-models","text":"We maintain a list of pre-trained uncompressed models, so that the training process of model compression does not need to start from scratch. For the CIFAR-10 data set, we provide following pre-trained models: Model name Accuracy URL LeNet 81.79% Link ResNet-20 91.93% Link ResNet-32 92.59% Link ResNet-44 92.76% Link ResNet-56 93.23% Link For the ImageNet (ILSVRC-12) data set, we provide following pre-trained models: Model name Top-1 Acc. Top-5 Acc. URL ResNet-18 70.28% 89.38% Link ResNet-34 73.41% 91.27% Link ResNet-50 75.97% 92.88% Link MobileNet-v1 70.89% 89.56% Link MobileNet-v2 71.84% 90.60% Link","title":"Pre-trained Models"},{"location":"reference/","text":"Reference [ Bergstra et al., 2013 ] J. Bergstra, D. Yamins, and D. D. Cox. Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures . In International Conference on Machine Learning (ICML), pages 115-123, Jun 2013. [ Han et al., 2016 ] Song Han, Huizi Mao, and William J. Dally. Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding . In International Conference on Learning Representations (ICLR), 2016. [ He et al., 2017 ] Yihui He, Xiangyu Zhang, and Jian Sun. Channel Pruning for Accelerating Very Deep Neural Networks . In IEEE International Conference on Computer Vision (ICCV), pages 1389-1397, 2017. [ He et al., 2018 ] Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. AMC: AutoML for Model Compression and Acceleration on Mobile Devices . In European Conference on Computer Vision (ECCV), pages 784-800, 2018. [ Jacob et al., 2018 ] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference . In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2704-2713, 2018. [ Lillicrap et al., 2016 ] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous Control with Deep Reinforcement Learning . In International Conference on Learning Representations (ICLR), 2016. [ Mockus, 1975 ] J. Mockus. On Bayesian Methods for Seeking the Extremum . In Optimization Techniques IFIP Technical Conference, pages 400-404, 1975. [ Zhu Gupta, 2017 ] Michael Zhu and Suyog Gupta. To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression . CoRR, abs/1710.01878, 2017. [ Zhuang et al., 2018 ] Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Jiezhang Cao, Qingyao Wu, Junzhou Huang, and Jinhui Zhu. Discrimination-aware Channel Pruning for Deep Neural Networks . In Annual Conference on Neural Information Processing Systems (NIPS), 2018.","title":"Reference"},{"location":"reference/#reference","text":"[ Bergstra et al., 2013 ] J. Bergstra, D. Yamins, and D. D. Cox. Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures . In International Conference on Machine Learning (ICML), pages 115-123, Jun 2013. [ Han et al., 2016 ] Song Han, Huizi Mao, and William J. Dally. Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding . In International Conference on Learning Representations (ICLR), 2016. [ He et al., 2017 ] Yihui He, Xiangyu Zhang, and Jian Sun. Channel Pruning for Accelerating Very Deep Neural Networks . In IEEE International Conference on Computer Vision (ICCV), pages 1389-1397, 2017. [ He et al., 2018 ] Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. AMC: AutoML for Model Compression and Acceleration on Mobile Devices . In European Conference on Computer Vision (ECCV), pages 784-800, 2018. [ Jacob et al., 2018 ] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference . In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2704-2713, 2018. [ Lillicrap et al., 2016 ] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous Control with Deep Reinforcement Learning . In International Conference on Learning Representations (ICLR), 2016. [ Mockus, 1975 ] J. Mockus. On Bayesian Methods for Seeking the Extremum . In Optimization Techniques IFIP Technical Conference, pages 400-404, 1975. [ Zhu Gupta, 2017 ] Michael Zhu and Suyog Gupta. To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression . CoRR, abs/1710.01878, 2017. [ Zhuang et al., 2018 ] Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Jiezhang Cao, Qingyao Wu, Junzhou Huang, and Jinhui Zhu. Discrimination-aware Channel Pruning for Deep Neural Networks . In Annual Conference on Neural Information Processing Systems (NIPS), 2018.","title":"Reference"},{"location":"reinforcement_learning/","text":"Reinforcement Learning For most deep learning models, the parameter redundancy differs from one layer to another. Some layers may be more robust to model compression algorithms due to larger redundancy, while others may be more sensitive. Therefore, it is often sub-optimal to use a unified pruning ratio or number of quantization bits for all layers, which completely omits the redundancy difference. However, it is also time-consuming or even impractical to manually setup the optimal value of such hyper-parameter for each layer, especially for deep networks with tens or hundreds of layers. To overcome this dilemma, in PocketFlow, we adopt reinforcement learning to automatically determine the optimal pruning ratio or number of quantization bits for each layer. Our approach is innovated from (He et al., 2018), which automatically determines each layer's optimal pruning ratio, and generalize it to hyper-parameter optimization for more model compression methods. In this documentation, we take UniformQuantLearner as an example to explain how the reinforcement learning method is used to iteratively optimize the number of quantization bits for each layer. It is worthy mentioning that this feature is also available for ChannelPrunedLearner , WeightSparseLearner , and NonUniformQuantLearner . Algorithm Description Here, we assume the original model to be compressed consists of T T layers, and denote the t t -th layer's weight tensor as \\mathbf{W}_{t} \\mathbf{W}_{t} and its quantization bit-width as b_{t} b_{t} . In order to maximally exploit the parameter redundancy of each layer, we need to find the optimal combination of layer-wise quantization bit-width that achieves the highest accuracy after compression while satisfying: \\sum_{t = 1}^{T} b_{t} \\left| \\mathbf{W}_{t} \\right| \\le b \\cdot \\sum_{t = 1}^{T} \\left| \\mathbf{W}_{t} \\right| \\sum_{t = 1}^{T} b_{t} \\left| \\mathbf{W}_{t} \\right| \\le b \\cdot \\sum_{t = 1}^{T} \\left| \\mathbf{W}_{t} \\right| where \\left| \\mathbf{W}_{t} \\right| \\left| \\mathbf{W}_{t} \\right| denotes the number of parameters in the weight tensor \\mathbf{W}_{t} \\mathbf{W}_{t} and b b is the whole network's target quantization bit-width. Below, we present the overall workflow of adopting reinforcement learning, or more specifically, the DDPG algorithm (Lillicrap et al., 2016) to search for the optimal combination of layer-wise quantization bit-width: To start with, we initialize an DDPG agent and set the best reward r_{best} r_{best} to negative infinity to track the optimal combination of layer-wise quantization bit-width. The search process consists of multiple roll-outs. In each roll-out, we sequentially traverse each layer in the network to determine its quantization bit-width. For the t t -th layer, we construct its state vector with following information: one-hot embedding of layer index weight tensor's shape number of parameters in the weight tensor number of quantization bits used by previous layers budget of quantization bits for remaining layers Afterwards, we feed this state vector into the DDPG agent to choose an action, which is then converted into the quantization bit-width under certain constraints. A commonly-used constraint is that with the selected quantization bit-width, the budget of quantization bits for remaining layers should be sufficient, e.g. ensuring the minimal quantization bit-width can be satisfied. After obtaining all layer's quantization bit-width, we quantize each layer's weights with the corresponding quantization bit-width, and fine-tune the quantized network for a few iteration (as supported by each learner's \"Fast Fine-tuning\" mode). We then evaluate the fine-tuned network' accuracy and use it as the reward signal r_{n} r_{n} . The reward signal is compared against the best reward discovered so far, and the optimal combination of layer-wise quantization bit-width is updated if the current reward is larger. Finally, we generate a list of transitions from all the \\left( \\mathbf{s}_{t}, a_{t}, r_{t}, \\mathbf{s}_{t + 1} \\right) \\left( \\mathbf{s}_{t}, a_{t}, r_{t}, \\mathbf{s}_{t + 1} \\right) tuples in the roll-out, and store them in the DDPG agent's replay buffer. The DDPG agent is then trained with one or more mini-batches of sampled transitions, so that it can choose better actions in the following roll-outs. After obtaining the optimal combination of layer-wise quantization bit-width, we can optionally use UniformQuantLearner 's \"Re-training with Full Data\" mode (also supported by others learners) for a complete quantization-aware training to further reduce the accuracy loss.","title":"Reinforcement Learning"},{"location":"reinforcement_learning/#reinforcement-learning","text":"For most deep learning models, the parameter redundancy differs from one layer to another. Some layers may be more robust to model compression algorithms due to larger redundancy, while others may be more sensitive. Therefore, it is often sub-optimal to use a unified pruning ratio or number of quantization bits for all layers, which completely omits the redundancy difference. However, it is also time-consuming or even impractical to manually setup the optimal value of such hyper-parameter for each layer, especially for deep networks with tens or hundreds of layers. To overcome this dilemma, in PocketFlow, we adopt reinforcement learning to automatically determine the optimal pruning ratio or number of quantization bits for each layer. Our approach is innovated from (He et al., 2018), which automatically determines each layer's optimal pruning ratio, and generalize it to hyper-parameter optimization for more model compression methods. In this documentation, we take UniformQuantLearner as an example to explain how the reinforcement learning method is used to iteratively optimize the number of quantization bits for each layer. It is worthy mentioning that this feature is also available for ChannelPrunedLearner , WeightSparseLearner , and NonUniformQuantLearner .","title":"Reinforcement Learning"},{"location":"reinforcement_learning/#algorithm-description","text":"Here, we assume the original model to be compressed consists of T T layers, and denote the t t -th layer's weight tensor as \\mathbf{W}_{t} \\mathbf{W}_{t} and its quantization bit-width as b_{t} b_{t} . In order to maximally exploit the parameter redundancy of each layer, we need to find the optimal combination of layer-wise quantization bit-width that achieves the highest accuracy after compression while satisfying: \\sum_{t = 1}^{T} b_{t} \\left| \\mathbf{W}_{t} \\right| \\le b \\cdot \\sum_{t = 1}^{T} \\left| \\mathbf{W}_{t} \\right| \\sum_{t = 1}^{T} b_{t} \\left| \\mathbf{W}_{t} \\right| \\le b \\cdot \\sum_{t = 1}^{T} \\left| \\mathbf{W}_{t} \\right| where \\left| \\mathbf{W}_{t} \\right| \\left| \\mathbf{W}_{t} \\right| denotes the number of parameters in the weight tensor \\mathbf{W}_{t} \\mathbf{W}_{t} and b b is the whole network's target quantization bit-width. Below, we present the overall workflow of adopting reinforcement learning, or more specifically, the DDPG algorithm (Lillicrap et al., 2016) to search for the optimal combination of layer-wise quantization bit-width: To start with, we initialize an DDPG agent and set the best reward r_{best} r_{best} to negative infinity to track the optimal combination of layer-wise quantization bit-width. The search process consists of multiple roll-outs. In each roll-out, we sequentially traverse each layer in the network to determine its quantization bit-width. For the t t -th layer, we construct its state vector with following information: one-hot embedding of layer index weight tensor's shape number of parameters in the weight tensor number of quantization bits used by previous layers budget of quantization bits for remaining layers Afterwards, we feed this state vector into the DDPG agent to choose an action, which is then converted into the quantization bit-width under certain constraints. A commonly-used constraint is that with the selected quantization bit-width, the budget of quantization bits for remaining layers should be sufficient, e.g. ensuring the minimal quantization bit-width can be satisfied. After obtaining all layer's quantization bit-width, we quantize each layer's weights with the corresponding quantization bit-width, and fine-tune the quantized network for a few iteration (as supported by each learner's \"Fast Fine-tuning\" mode). We then evaluate the fine-tuned network' accuracy and use it as the reward signal r_{n} r_{n} . The reward signal is compared against the best reward discovered so far, and the optimal combination of layer-wise quantization bit-width is updated if the current reward is larger. Finally, we generate a list of transitions from all the \\left( \\mathbf{s}_{t}, a_{t}, r_{t}, \\mathbf{s}_{t + 1} \\right) \\left( \\mathbf{s}_{t}, a_{t}, r_{t}, \\mathbf{s}_{t + 1} \\right) tuples in the roll-out, and store them in the DDPG agent's replay buffer. The DDPG agent is then trained with one or more mini-batches of sampled transitions, so that it can choose better actions in the following roll-outs. After obtaining the optimal combination of layer-wise quantization bit-width, we can optionally use UniformQuantLearner 's \"Re-training with Full Data\" mode (also supported by others learners) for a complete quantization-aware training to further reduce the accuracy loss.","title":"Algorithm Description"},{"location":"self_defined_models/","text":"Self-defined Models Under construction ...","title":"Self-defined Models"},{"location":"self_defined_models/#self-defined-models","text":"Under construction ...","title":"Self-defined Models"},{"location":"test_cases/","text":"Test Cases This document contains various test cases to cover different combinations of learners and hyper-parameter settings. Any merge request to the master branch should be able to pass all the test cases to be approved. FP: Full-Precision # local mode $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --enbl_dst $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --data_disk hdfs $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --data_disk hdfs \\ --enbl_dst # seven mode $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py \\ --enbl_dst $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py \\ --data_disk hdfs $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py \\ --data_disk hdfs \\ --enbl_dst # docker mode $ ./scripts/run_docker.sh nets/lenet_at_cifar10_run.py $ ./scripts/run_docker.sh nets/lenet_at_cifar10_run.py \\ --enbl_dst $ ./scripts/run_docker.sh nets/resnet_at_cifar10_run.py $ ./scripts/run_docker.sh nets/resnet_at_cifar10_run.py \\ --enbl_dst CP: Channel Pruning # uniform preserve ratios for all layers $ ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --learner channel \\ --cp_prune_option uniform \\ --cp_uniform_preserve_ratio 0.5 # auto-tuned preserve ratios for each layer $ ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --cp_learner channel \\ --cp_prune_option auto \\ --cp_preserve_ratio 0.3 DCP: Discrimination-aware Channel Pruning # no network distillation $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner dis-chn-pruned \\ --dcp_nb_stages 3 \\ --data_disk hdfs # network distillation $ ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py \\ --learner dis-chn-pruned \\ --enbl_dst \\ --dcp_nb_stages 4 WS: Weight Sparsification # uniform pruning ratios for all layers $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner weight-sparse \\ --ws_prune_ratio_prtl uniform \\ --data_disk hdfs # optimal pruning ratios for each layer $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner weight-sparse \\ --ws_prune_ratio_prtl optimal \\ --data_disk hdfs # heurist pruning ratios for each layer $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py \\ --learner weight-sparse \\ --ws_prune_ratio_prtl heurist # optimal pruning ratios for each layer $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py \\ --learner weight-sparse \\ --ws_prune_ratio_prtl optimal UQ: Uniform Quantization # channel-based bucketing $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner uniform \\ --uql_use_buckets \\ --uql_bucket_type channel \\ --data_disk hdfs # split-based bucketing $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner uniform \\ --uql_use_buckets \\ --uql_bucket_type split \\ --data_disk hdfs # channel-based bucketing + RL $ ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py -n=2 \\ --learner uniform \\ --uql_enbl_rl_agent \\ --uql_use_buckets \\ --uql_bucket_type channel # split-based bucketing + RL $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py -n=2 \\ --learner uniform \\ --uql_enbl_rl_agent \\ --uql_use_buckets \\ --uql_bucket_type split NUQ: Non-uniform Quantization # channel-based bucketing + RL + optimize clusters $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner non-uniform \\ --nuql_enbl_rl_agent \\ --nuql_use_buckets \\ --nuql_bucket_type channel \\ --nuql_opt_mode clusters \\ --data_disk hdfs # split-based bucketing + RL + optimize weights $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner non-uniform \\ --nuql_enbl_rl_agent \\ --nuql_use_buckets \\ --nuql_bucket_type split \\ --nuql_opt_mode weights \\ --data_disk hdfs # channel-based bucketing + RL + optimize weights $ ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py -n=2 \\ --learner non-uniform \\ --nuql_enbl_rl_agent \\ --nuql_use_buckets \\ --nuql_bucket_type channel \\ --nuql_opt_mode weights # split-based bucketing + RL + optimize clusters $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py -n=2 \\ --learner non-uniform \\ --nuql_enbl_rl_agent \\ --nuql_use_buckets \\ --nuql_bucket_type split \\ --nuql_opt_mode clusters","title":"Test Cases"},{"location":"test_cases/#test-cases","text":"This document contains various test cases to cover different combinations of learners and hyper-parameter settings. Any merge request to the master branch should be able to pass all the test cases to be approved.","title":"Test Cases"},{"location":"test_cases/#fp-full-precision","text":"# local mode $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --enbl_dst $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --data_disk hdfs $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --data_disk hdfs \\ --enbl_dst # seven mode $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py \\ --enbl_dst $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py \\ --data_disk hdfs $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py \\ --data_disk hdfs \\ --enbl_dst # docker mode $ ./scripts/run_docker.sh nets/lenet_at_cifar10_run.py $ ./scripts/run_docker.sh nets/lenet_at_cifar10_run.py \\ --enbl_dst $ ./scripts/run_docker.sh nets/resnet_at_cifar10_run.py $ ./scripts/run_docker.sh nets/resnet_at_cifar10_run.py \\ --enbl_dst","title":"FP: Full-Precision"},{"location":"test_cases/#cp-channel-pruning","text":"# uniform preserve ratios for all layers $ ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --learner channel \\ --cp_prune_option uniform \\ --cp_uniform_preserve_ratio 0.5 # auto-tuned preserve ratios for each layer $ ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --cp_learner channel \\ --cp_prune_option auto \\ --cp_preserve_ratio 0.3","title":"CP: Channel Pruning"},{"location":"test_cases/#dcp-discrimination-aware-channel-pruning","text":"# no network distillation $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner dis-chn-pruned \\ --dcp_nb_stages 3 \\ --data_disk hdfs # network distillation $ ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py \\ --learner dis-chn-pruned \\ --enbl_dst \\ --dcp_nb_stages 4","title":"DCP: Discrimination-aware Channel Pruning"},{"location":"test_cases/#ws-weight-sparsification","text":"# uniform pruning ratios for all layers $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner weight-sparse \\ --ws_prune_ratio_prtl uniform \\ --data_disk hdfs # optimal pruning ratios for each layer $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner weight-sparse \\ --ws_prune_ratio_prtl optimal \\ --data_disk hdfs # heurist pruning ratios for each layer $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py \\ --learner weight-sparse \\ --ws_prune_ratio_prtl heurist # optimal pruning ratios for each layer $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py \\ --learner weight-sparse \\ --ws_prune_ratio_prtl optimal","title":"WS: Weight Sparsification"},{"location":"test_cases/#uq-uniform-quantization","text":"# channel-based bucketing $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner uniform \\ --uql_use_buckets \\ --uql_bucket_type channel \\ --data_disk hdfs # split-based bucketing $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner uniform \\ --uql_use_buckets \\ --uql_bucket_type split \\ --data_disk hdfs # channel-based bucketing + RL $ ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py -n=2 \\ --learner uniform \\ --uql_enbl_rl_agent \\ --uql_use_buckets \\ --uql_bucket_type channel # split-based bucketing + RL $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py -n=2 \\ --learner uniform \\ --uql_enbl_rl_agent \\ --uql_use_buckets \\ --uql_bucket_type split","title":"UQ: Uniform Quantization"},{"location":"test_cases/#nuq-non-uniform-quantization","text":"# channel-based bucketing + RL + optimize clusters $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner non-uniform \\ --nuql_enbl_rl_agent \\ --nuql_use_buckets \\ --nuql_bucket_type channel \\ --nuql_opt_mode clusters \\ --data_disk hdfs # split-based bucketing + RL + optimize weights $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner non-uniform \\ --nuql_enbl_rl_agent \\ --nuql_use_buckets \\ --nuql_bucket_type split \\ --nuql_opt_mode weights \\ --data_disk hdfs # channel-based bucketing + RL + optimize weights $ ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py -n=2 \\ --learner non-uniform \\ --nuql_enbl_rl_agent \\ --nuql_use_buckets \\ --nuql_bucket_type channel \\ --nuql_opt_mode weights # split-based bucketing + RL + optimize clusters $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py -n=2 \\ --learner non-uniform \\ --nuql_enbl_rl_agent \\ --nuql_use_buckets \\ --nuql_bucket_type split \\ --nuql_opt_mode clusters","title":"NUQ: Non-uniform Quantization"},{"location":"tutorial/","text":"Quick Start In this tutorial, we demonstrate how to compress a convolutional neural network and export the compressed model into a *.tflite file for deployment on mobile devices. The model we used here is a 18-layer residual network (denoted as \"ResNet-18\") trained for the ImageNet classification task. We will compress it with the discrimination-aware channel pruning algorithm (Zhuang et al., NIPS '18) to reduce the number of convolutional channels used in the network for speed-up. Prepare the Data To start with, we need to convert the ImageNet data set (ILSVRC-12) into TensorFlow's native TFRecord file format. You may follow the data preparation guide here to download the full data set and convert it into TFRecord files. After that, you should be able to find 4,096 training files and 128 validation files in the data directory, like this: # training files train-00000-of-04096 train-00001-of-04096 ... train-04095-of-04096 # validation files validation-00000-of-00128 validation-00001-of-00128 ... validation-00127-of-00128 Prepare the Pre-trained Model The discrimination-aware channel pruning algorithm requires a pre-trained uncompressed model provided in advance, so that a channel-pruned model can be trained with warm-start. You can download a pre-trained model from here , and then unzip files into the models sub-directory. Alternatively, you can train an uncompressed full-precision model from scratch using FullPrecLearner with the following command (choose whatever mode that fits you): # local mode with 1 GPU $ ./scripts/run_local.sh nets/resnet_at_ilsvrc12_run.py # docker mode with 8 GPUs $ ./scripts/run_docker.sh nets/resnet_at_ilsvrc12_run.py -n=8 # seven mode with 8 GPUs $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py -n=8 After the training process, you should be able to find the resulting model files located at the models sub-directory in PocketFlow's home directory. Train the Compressed Model Now, we can train a compressed model with the discrimination-aware channel pruning algorithm, as implemented by DisChnPrunedLearner . Assuming you are now in PocketFlow's home directory, the training process of model compression can be started using the following command (choose whatever mode that fits you): # local mode with 1 GPU $ ./scripts/run_local.sh nets/resnet_at_ilsvrc12_run.py \\ --learner dis-chn-pruned # docker mode with 8 GPUs $ ./scripts/run_docker.sh nets/resnet_at_ilsvrc12_run.py -n=8 \\ --learner dis-chn-pruned # seven mode with 8 GPUs $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py -n=8 \\ --learner dis-chn-pruned Let's take the execution command for the local mode as an example. In this command, run_local.sh is a shell script that executes the specified Python script with user-provided arguments. Here, we ask it to run the Python script named nets/resnet_at_ilsvrc12_run.py , which is the execution script for ResNet models on the ImageNet data set. After that, we use --learner dis-chn-pruned to specify that the DisChnPrunedLearner should be used for model compression. You may also use other learners by specifying the corresponding learner name. Below is a full list of available learners in PocketFlow: Learner name Learner class Note full-prec FullPrecLearner No model compression channel ChannelPruningLearner Channel pruning [2] dis-chn-pruned DisChnPrunedLearner Discrimination-aware channel pruning [1] weight-sparse WeightSparseLearner Weight sparsification [3] uniform UniformQuantizedLearner Uniform weight quantization [] tf-uniform UniformQuantizedLearnerTF Uniform weight quantization in TensorFlow [] non-uniform NonUniformQuantizedLearner Non-uniform weight quantization [] The local mode only uses 1 GPU for the training process, which takes approximately 20-30 hours to complete. This can be accelerated by multi-GPU training in the docker and seven mode, which is enabled by adding -n=x right after the specified Python script, where x is the number of GPUs to be used. Optionally, you can pass some extra arguments to customize the training process. For the discrimination-aware channel pruning algorithm, some of key arguments are: Name Definition Default Value enbl_dst Enable training with distillation loss False dcp_prune_ratio DCP algorithm's pruning ratio 0.5 You may override the default value by appending customized arguments at the end of the execution command. For instance, the following command: $ ./scripts/run_local.sh nets/resnet_at_ilsvrc12_run.py \\ --learner dis-chn-pruned \\ --enbl_dst \\ --dcp_prune_ratio 0.75 requires the DisChnPrunedLearner to achieve an overall pruning ratio of 0.75 and the training process will be carried out with the distillation loss. As a result, the number of channels in each convolutional layer of the compressed model will be one quarter of the original one. After the training process is completed, you should be able to find a sub-directory named models_dcp_eval created in the home directory of PocketFlow. This sub-directory contains all the files that define the compressed model, and we will export them to a TensorFlow Lite formatted model file for deployment in the next section. Export to TensorFlow Lite TensorFlow's checkpoint files cannot be directly used for deployment on mobile devices. Instead, we need to firstly convert them into a single *.tflite file that is supported by the TensorFlow Lite Interpreter. For model compressed with channel-pruning based algorithms, e.g. ChannelPruningLearner and DisChnPrunedLearner , we have prepared a model conversion script, tools/conversion/export_pb_tflite_models.py , to generate a TF-Lite model from TensorFlow's checkpoint files. To convert checkpoint files into a *.tflite file, use the following command: # convert checkpoint files into a *.tflite model $ python tools/conversion/export_pb_tflite_models.py \\ --model_dir models_dcp_eval In the above command, we specify the model directory containing checkpoint files generated in the previous training process. The conversion script automatically detects which channels can be safely pruned, and then produces a light-weighted compressed model. The resulting TensorFlow Lite file is also placed at the models_dcp_eval directory, named as model_transformed.tflite . Deploy on Mobile Devices After exporting the compressed model to the TensorFlow Lite file format, you may follow the official guide for creating an Android demo App from it. Basically, this demo App uses a TensorFlow Lite model to continuously classifies images captured by the camera, and all the computation are performed on mobile devices in real time. To use the model_transformed.tflite model file, you need to place it in the asserts directory and create a Java class named ImageClassifierFloatResNet to use this model for classification. Below is the example code, which is modified from ImageClassifierFloatInception.java used in the official demo project: /* Copyright 2017 The TensorFlow Authors. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the License ); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an AS IS BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. ==============================================================================*/ package com.example.android.tflitecamerademo; import android.app.Activity; import java.io.IOException; /** * This classifier works with the ResNet-18 model. * It applies floating point inference rather than using a quantized model. */ public class ImageClassifierFloatResNet extends ImageClassifier { /** * The ResNet requires additional normalization of the used input. */ private static final float IMAGE_MEAN_RED = 123.58f; private static final float IMAGE_MEAN_GREEN = 116.779f; private static final float IMAGE_MEAN_BLUE = 103.939f; /** * An array to hold inference results, to be feed into Tensorflow Lite as outputs. * This isn't part of the super class, because we need a primitive array here. */ private float[][] labelProbArray = null; /** * Initializes an {@code ImageClassifier}. * * @param activity */ ImageClassifierFloatResNet(Activity activity) throws IOException { super(activity); labelProbArray = new float[1][getNumLabels()]; } @Override protected String getModelPath() { return model_transformed.tflite ; } @Override protected String getLabelPath() { return labels_imagenet_slim.txt ; } @Override protected int getImageSizeX() { return 224; } @Override protected int getImageSizeY() { return 224; } @Override protected int getNumBytesPerChannel() { // a 32bit float value requires 4 bytes return 4; } @Override protected void addPixelValue(int pixelValue) { imgData.putFloat(((pixelValue 16) 0xFF) - IMAGE_MEAN_RED); imgData.putFloat(((pixelValue 8) 0xFF) - IMAGE_MEAN_GREEN); imgData.putFloat((pixelValue 0xFF) - IMAGE_MEAN_BLUE); } @Override protected float getProbability(int labelIndex) { return labelProbArray[0][labelIndex]; } @Override protected void setProbability(int labelIndex, Number value) { labelProbArray[0][labelIndex] = value.floatValue(); } @Override protected float getNormalizedProbability(int labelIndex) { // TODO the following value isn't in [0,1] yet, but may be greater. Why? return getProbability(labelIndex); } @Override protected void runInference() { tflite.run(imgData, labelProbArray); } } After that, you need to change the image classifier class used in Camera2BasicFragment.java . Locate the function named onActivityCreated and change its content as below. Now you will be able to use the compressed ResNet-18 model to classify objects on your mobile phone in real time. /** Load the model and labels. */ @Override public void onActivityCreated(Bundle savedInstanceState) { super.onActivityCreated(savedInstanceState); try { classifier = new ImageClassifierFloatResNet(getActivity()); } catch (IOException e) { Log.e(TAG, Failed to initialize an image classifier. , e); } startBackgroundThread(); } Reference [1] Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Jiezhang Cao, Qingyao Wu, Junzhou Huang, Jinhui Zhu, Discrimination-aware Channel Pruning for Deep Neural Networks , In Proc. of the Annual Conference on Neural Information Processing Systems (NIPS), 2018. [2] Yihui He, Xiangyu Zhang, Jian Sun, Channel Pruning for Accelerating Very Deep Neural Networks , In Proc. of the IEEE International Conference on Computer Vision (ICCV), 2017. [3] Michael Zhu, Suyog Gupta, To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression , CoRR, abs/1710.01878, 2017.","title":"Tutorial"},{"location":"tutorial/#quick-start","text":"In this tutorial, we demonstrate how to compress a convolutional neural network and export the compressed model into a *.tflite file for deployment on mobile devices. The model we used here is a 18-layer residual network (denoted as \"ResNet-18\") trained for the ImageNet classification task. We will compress it with the discrimination-aware channel pruning algorithm (Zhuang et al., NIPS '18) to reduce the number of convolutional channels used in the network for speed-up.","title":"Quick Start"},{"location":"tutorial/#prepare-the-data","text":"To start with, we need to convert the ImageNet data set (ILSVRC-12) into TensorFlow's native TFRecord file format. You may follow the data preparation guide here to download the full data set and convert it into TFRecord files. After that, you should be able to find 4,096 training files and 128 validation files in the data directory, like this: # training files train-00000-of-04096 train-00001-of-04096 ... train-04095-of-04096 # validation files validation-00000-of-00128 validation-00001-of-00128 ... validation-00127-of-00128","title":"Prepare the Data"},{"location":"tutorial/#prepare-the-pre-trained-model","text":"The discrimination-aware channel pruning algorithm requires a pre-trained uncompressed model provided in advance, so that a channel-pruned model can be trained with warm-start. You can download a pre-trained model from here , and then unzip files into the models sub-directory. Alternatively, you can train an uncompressed full-precision model from scratch using FullPrecLearner with the following command (choose whatever mode that fits you): # local mode with 1 GPU $ ./scripts/run_local.sh nets/resnet_at_ilsvrc12_run.py # docker mode with 8 GPUs $ ./scripts/run_docker.sh nets/resnet_at_ilsvrc12_run.py -n=8 # seven mode with 8 GPUs $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py -n=8 After the training process, you should be able to find the resulting model files located at the models sub-directory in PocketFlow's home directory.","title":"Prepare the Pre-trained Model"},{"location":"tutorial/#train-the-compressed-model","text":"Now, we can train a compressed model with the discrimination-aware channel pruning algorithm, as implemented by DisChnPrunedLearner . Assuming you are now in PocketFlow's home directory, the training process of model compression can be started using the following command (choose whatever mode that fits you): # local mode with 1 GPU $ ./scripts/run_local.sh nets/resnet_at_ilsvrc12_run.py \\ --learner dis-chn-pruned # docker mode with 8 GPUs $ ./scripts/run_docker.sh nets/resnet_at_ilsvrc12_run.py -n=8 \\ --learner dis-chn-pruned # seven mode with 8 GPUs $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py -n=8 \\ --learner dis-chn-pruned Let's take the execution command for the local mode as an example. In this command, run_local.sh is a shell script that executes the specified Python script with user-provided arguments. Here, we ask it to run the Python script named nets/resnet_at_ilsvrc12_run.py , which is the execution script for ResNet models on the ImageNet data set. After that, we use --learner dis-chn-pruned to specify that the DisChnPrunedLearner should be used for model compression. You may also use other learners by specifying the corresponding learner name. Below is a full list of available learners in PocketFlow: Learner name Learner class Note full-prec FullPrecLearner No model compression channel ChannelPruningLearner Channel pruning [2] dis-chn-pruned DisChnPrunedLearner Discrimination-aware channel pruning [1] weight-sparse WeightSparseLearner Weight sparsification [3] uniform UniformQuantizedLearner Uniform weight quantization [] tf-uniform UniformQuantizedLearnerTF Uniform weight quantization in TensorFlow [] non-uniform NonUniformQuantizedLearner Non-uniform weight quantization [] The local mode only uses 1 GPU for the training process, which takes approximately 20-30 hours to complete. This can be accelerated by multi-GPU training in the docker and seven mode, which is enabled by adding -n=x right after the specified Python script, where x is the number of GPUs to be used. Optionally, you can pass some extra arguments to customize the training process. For the discrimination-aware channel pruning algorithm, some of key arguments are: Name Definition Default Value enbl_dst Enable training with distillation loss False dcp_prune_ratio DCP algorithm's pruning ratio 0.5 You may override the default value by appending customized arguments at the end of the execution command. For instance, the following command: $ ./scripts/run_local.sh nets/resnet_at_ilsvrc12_run.py \\ --learner dis-chn-pruned \\ --enbl_dst \\ --dcp_prune_ratio 0.75 requires the DisChnPrunedLearner to achieve an overall pruning ratio of 0.75 and the training process will be carried out with the distillation loss. As a result, the number of channels in each convolutional layer of the compressed model will be one quarter of the original one. After the training process is completed, you should be able to find a sub-directory named models_dcp_eval created in the home directory of PocketFlow. This sub-directory contains all the files that define the compressed model, and we will export them to a TensorFlow Lite formatted model file for deployment in the next section.","title":"Train the Compressed Model"},{"location":"tutorial/#export-to-tensorflow-lite","text":"TensorFlow's checkpoint files cannot be directly used for deployment on mobile devices. Instead, we need to firstly convert them into a single *.tflite file that is supported by the TensorFlow Lite Interpreter. For model compressed with channel-pruning based algorithms, e.g. ChannelPruningLearner and DisChnPrunedLearner , we have prepared a model conversion script, tools/conversion/export_pb_tflite_models.py , to generate a TF-Lite model from TensorFlow's checkpoint files. To convert checkpoint files into a *.tflite file, use the following command: # convert checkpoint files into a *.tflite model $ python tools/conversion/export_pb_tflite_models.py \\ --model_dir models_dcp_eval In the above command, we specify the model directory containing checkpoint files generated in the previous training process. The conversion script automatically detects which channels can be safely pruned, and then produces a light-weighted compressed model. The resulting TensorFlow Lite file is also placed at the models_dcp_eval directory, named as model_transformed.tflite .","title":"Export to TensorFlow Lite"},{"location":"tutorial/#deploy-on-mobile-devices","text":"After exporting the compressed model to the TensorFlow Lite file format, you may follow the official guide for creating an Android demo App from it. Basically, this demo App uses a TensorFlow Lite model to continuously classifies images captured by the camera, and all the computation are performed on mobile devices in real time. To use the model_transformed.tflite model file, you need to place it in the asserts directory and create a Java class named ImageClassifierFloatResNet to use this model for classification. Below is the example code, which is modified from ImageClassifierFloatInception.java used in the official demo project: /* Copyright 2017 The TensorFlow Authors. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the License ); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an AS IS BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. ==============================================================================*/ package com.example.android.tflitecamerademo; import android.app.Activity; import java.io.IOException; /** * This classifier works with the ResNet-18 model. * It applies floating point inference rather than using a quantized model. */ public class ImageClassifierFloatResNet extends ImageClassifier { /** * The ResNet requires additional normalization of the used input. */ private static final float IMAGE_MEAN_RED = 123.58f; private static final float IMAGE_MEAN_GREEN = 116.779f; private static final float IMAGE_MEAN_BLUE = 103.939f; /** * An array to hold inference results, to be feed into Tensorflow Lite as outputs. * This isn't part of the super class, because we need a primitive array here. */ private float[][] labelProbArray = null; /** * Initializes an {@code ImageClassifier}. * * @param activity */ ImageClassifierFloatResNet(Activity activity) throws IOException { super(activity); labelProbArray = new float[1][getNumLabels()]; } @Override protected String getModelPath() { return model_transformed.tflite ; } @Override protected String getLabelPath() { return labels_imagenet_slim.txt ; } @Override protected int getImageSizeX() { return 224; } @Override protected int getImageSizeY() { return 224; } @Override protected int getNumBytesPerChannel() { // a 32bit float value requires 4 bytes return 4; } @Override protected void addPixelValue(int pixelValue) { imgData.putFloat(((pixelValue 16) 0xFF) - IMAGE_MEAN_RED); imgData.putFloat(((pixelValue 8) 0xFF) - IMAGE_MEAN_GREEN); imgData.putFloat((pixelValue 0xFF) - IMAGE_MEAN_BLUE); } @Override protected float getProbability(int labelIndex) { return labelProbArray[0][labelIndex]; } @Override protected void setProbability(int labelIndex, Number value) { labelProbArray[0][labelIndex] = value.floatValue(); } @Override protected float getNormalizedProbability(int labelIndex) { // TODO the following value isn't in [0,1] yet, but may be greater. Why? return getProbability(labelIndex); } @Override protected void runInference() { tflite.run(imgData, labelProbArray); } } After that, you need to change the image classifier class used in Camera2BasicFragment.java . Locate the function named onActivityCreated and change its content as below. Now you will be able to use the compressed ResNet-18 model to classify objects on your mobile phone in real time. /** Load the model and labels. */ @Override public void onActivityCreated(Bundle savedInstanceState) { super.onActivityCreated(savedInstanceState); try { classifier = new ImageClassifierFloatResNet(getActivity()); } catch (IOException e) { Log.e(TAG, Failed to initialize an image classifier. , e); } startBackgroundThread(); }","title":"Deploy on Mobile Devices"},{"location":"tutorial/#reference","text":"[1] Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Jiezhang Cao, Qingyao Wu, Junzhou Huang, Jinhui Zhu, Discrimination-aware Channel Pruning for Deep Neural Networks , In Proc. of the Annual Conference on Neural Information Processing Systems (NIPS), 2018. [2] Yihui He, Xiangyu Zhang, Jian Sun, Channel Pruning for Accelerating Very Deep Neural Networks , In Proc. of the IEEE International Conference on Computer Vision (ICCV), 2017. [3] Michael Zhu, Suyog Gupta, To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression , CoRR, abs/1710.01878, 2017.","title":"Reference"},{"location":"uq_learner/","text":"Uniform Quantization This document describes how to set up uniform quantization with PocketFlow. Uniform quantization is widely used for model compression and acceleration. Originally the weights in the network are represented by 32-bit float numbers. With uniform quantization, low-precision (e.g., 4 bit, 8 bit) and evenly distributed float numbers are used to approximate the full precision networks. For 8 bit quantization, the network size can be reduced by 4 folds with little drop of performance. Currently PocketFlow supports two types of uniform quantization: Uniform Quantization Learner: the self-developed learner. Aside from uniform quantization, the learner is carefully optimized with various extensions supported. The detailed algorithm of the Uniform Quantized Learner will be introduced at the end of the algorithm. The algorithm details are presented at the end of the document. TensorFlow Quantization Wrapper: a wrapper based on the post training quantization in TensorFlow. The wrapper currently only supports 8-bit quantization, enjoying 4x reduction of memory and nearly 4x times speed up of inference. A comparison of the two learners are shown below: Features Uniform Quantization Learner TensorFlow Quantization Wrapper Compression yes Yes Acceleration Yes Fine-tuning Yes Bucketing Yes Hyper-param Searching Yes Uniform Quantization Learner The uniform quantization learner supports both weight quantization and activation quantization, where users can manually set up the bits for quantization. The uniform quantization learner also supports bucketing, which leads to more fine-grained quantization and better performance. The users can also turn on the hyper parameter optimizer with reinforcement learning to search for the optimal bit allocation for the learner. Prepare the Model To quantize the network, users should first get the model prepared. Users can either use the pre-built models in PocketFlow, or develop their custom models according to TODO . Configure the Learner To configure the learner, users can pass the options via the TensorFlow flag interface. The available options are as follows: Options Default Value Description --uql_weight_bits 4 the number of bits for weight --uql_activation_bits 32 the number of bits for activation\uff0c by default it remains full precision --uql_save_quant_mode_path TODO the save path for quantized models --uql_use_buckets False use bucketing or not --uql_bucket_type channel two bucket type available: ['split', 'channel'] --uql_bucket_size 256 quantize the first and last layers of the network or not --uql_enbl_rl_agent False enable reinforcement learning to learn the optimal bit allocation or not --uql_quantize_all_layers False quantize the first and last layers of the network or not --uql_quant_epoch 60 the number of epochs for fine-tuning Examples Once the model is built, the quantization can be easily triggered by directly passing the Uniform Quantization Learner in the command line as follows: # quantize resnet-20 on CIFAR-10 # you can also configure the sh ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --data_disk local \\ --data_dir_local ${PF_CIFAR10_LOCAL} \\ --learner=uniform \\ --uql_weight_bits=4 \\ --uql_activation_bits=4 \\ # quantize the resnet-18 on ILSVRC-12 sh ./scripts/run_local.sh nets/resnet_at_ilsvrc12_run.py \\ --learner=uniform \\ --data_disk local \\ --data_dir_local ${PF_ILSVRC12_LOCAL} \\ --uql_weight_bits=8 \\ --uql_activation_bits=8 \\ --uql_use_buckets=True \\ --uql_bucket_type=channel Configure the Hyper Parameter Optimizer Once the hyper parameter optimizer is turned on, i.e., uql_enbl_rl_agent==True , the reinforcement learning agents will search for the optimal allocation of bits to each layers. Before the search, users are supposed set up the bit constraints via --uql_evquivalent_bits , so that the optimal bits searched by the RL agent will not exceed the bit number without RL agent. For example, TODO Users can also configure other options in the RL agent, such as the number of roll-outs, the fine-tuning steps to get the reward, e.t.c.. Full list of options are listed as follows: Options Default Value Description --uql_evquivalent_bits 4 the number of re-allocated bits that is equivalent to uniform quantization without RL agent --uql_nb_rlouts 200 the number of roll outs for training the RL agent --uql_w_bit_min 2 the minimal number of bits for each layer --uql_w_bit_max 8 the maximal number of bits for each layer --uql_enbl_rl_global_tune True enable fine-tuning all layers of the network or not --uql_enbl_rl_layerwise_tune False enable fine-tuning the network layer by layer or not --uql_tune_layerwise_steps 100 the number of steps for layerwise fine-tuning --uql_tune_global_steps 2000 the number of steps for global fine-tuning --uql_tune_disp_steps 300 the display steps to show the fine-tuning progress --uql_enbl_random_layers True randomly permute the layers during RL agent training Examples # quantize mobilenet-v1 on ILSVRC-12 sh ./scripts/run_local.sh nets/mobilnet_at_ilsvrc12_run.py \\ --data_disk local \\ --data_dir_local ${PF_CIFAR10_LOCAL} \\ --learner=uniform \\ --uql_enbl_rl_agent=True \\ --uql_equivalent_bits=4 \\ --uql_tune_global_steps=1200 Performance Here we list some of the performance on Cifar-10 using the Uniform Quantization Learner and the built-in models in PocketFlow. The options not displayed remain the default values. Model Weight Bit Activation Bit Acc ResNet-20 32 32 91.96 ResNet-20 4 4 90.73 ResNet-20 8 8 92.25 Model Weight Bit Bucketing Acc ResNet-20 2 channel 89.67 ResNet-20 4 channel 92.02 ResNet-20 2 split 91.15 ResNet-20 4 split 91.98 Model Weight Bit RL search Acc ResNet-20 2 FALSE 86.17 ResNet-20 4 FALSE 91.76 ResNet-20 2 TRUE 90.30 ResNet-20 4 TRUE 91.88 TensorFlow Quantization Wrapper PocketFlow wraps the post training quantization in Tensorflow, and include all the necessary steps to convert the model to the .tflite format, which can be deployed on Andriod devices. To run the wrapper, users only need to get the checkpoint files ready, and then run the script. Prepare the Checkpoint Files Generally in TensorFlow, the checkpoints of a model include three files: .data, .index, .meta. Users are also supposed to add the input and output to collections, and configure the wrapper to acquire the corresponding collections. An example is as follows: TODO add quantization to the conversion tools # load the checkpoints in ./models, and read the collections of 'inputs' and 'outputs' python export_pb_tflite_models.py \\ --model_dir ./models --input_coll inputs --output_coll outputs --quantize True If successfully transformed, the .pb and .tflite files will be saved in ./models . Deploy on Mobile Devices TODO Algorithms Now we introduce the detailed algorithm in the Uniform Quantization Learner. As is shown in the following graph, given a full precision model, the Uniform Quantization Learner inserts quantization nodes into the computation graph of the model. To enable activation quantization, the quantization nodes shall also be inserted after the activation function. In the training phase, both full-precision and quantized weights are stored. During the forward pass, quantized weights are obtained by applying the quantization functions on the full precision weights. For the backward propagation of gradients, since the gradients w.r.t. the quantized weights are 0 almost everywhere, we use the straight-through estimator (STE) ( Hinton et.al 2012 , Bengio et.al 2013 ) to pass the gradient of quantized weights directly to the full precision weights for update. Uniform Quantization Function Uniform quantization distributed the quantization points evenly across the distribution of weights, and the full precision numbers are then assigned to the closest quantization point. To achieve this, we first normalize the full precision weights x x of one layer to [0, 1] [0, 1] , i.e., $$ sc(x) = \\frac{x-\\beta}{\\alpha}, $$ where \\alpha=\\max{x}-\\min{x} \\alpha=\\max{x}-\\min{x} and \\beta = \\min{x} \\beta = \\min{x} are the scaling factors. Then we assign sc(x) sc(x) to the discrete value by $$ \\hat{x}=\\frac{1}{2^k-1}\\mathrm{round}((2^k-1)\\cdot sc(x)), $$ and finally we do the inverse linear transformation to recover the quantized weights to the original scale, $$ Q(x)=\\alpha\\hat{x}+\\beta. $$ References Bengio Y, L\u00e9onard N, Courville A. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013 Geoffrey Hinton, Nitish Srivastava, Kevin Swersky, Tijmen Tieleman and Abdelrahman Mohamed. Neural Networks for Machine Learning. Coursera, video lectures, 2012","title":"Uniform Quantization"},{"location":"uq_learner/#uniform-quantization","text":"This document describes how to set up uniform quantization with PocketFlow. Uniform quantization is widely used for model compression and acceleration. Originally the weights in the network are represented by 32-bit float numbers. With uniform quantization, low-precision (e.g., 4 bit, 8 bit) and evenly distributed float numbers are used to approximate the full precision networks. For 8 bit quantization, the network size can be reduced by 4 folds with little drop of performance. Currently PocketFlow supports two types of uniform quantization: Uniform Quantization Learner: the self-developed learner. Aside from uniform quantization, the learner is carefully optimized with various extensions supported. The detailed algorithm of the Uniform Quantized Learner will be introduced at the end of the algorithm. The algorithm details are presented at the end of the document. TensorFlow Quantization Wrapper: a wrapper based on the post training quantization in TensorFlow. The wrapper currently only supports 8-bit quantization, enjoying 4x reduction of memory and nearly 4x times speed up of inference. A comparison of the two learners are shown below: Features Uniform Quantization Learner TensorFlow Quantization Wrapper Compression yes Yes Acceleration Yes Fine-tuning Yes Bucketing Yes Hyper-param Searching Yes","title":"Uniform Quantization"},{"location":"uq_learner/#uniform-quantization-learner","text":"The uniform quantization learner supports both weight quantization and activation quantization, where users can manually set up the bits for quantization. The uniform quantization learner also supports bucketing, which leads to more fine-grained quantization and better performance. The users can also turn on the hyper parameter optimizer with reinforcement learning to search for the optimal bit allocation for the learner.","title":"Uniform Quantization Learner"},{"location":"uq_learner/#prepare-the-model","text":"To quantize the network, users should first get the model prepared. Users can either use the pre-built models in PocketFlow, or develop their custom models according to TODO .","title":"Prepare the Model"},{"location":"uq_learner/#configure-the-learner","text":"To configure the learner, users can pass the options via the TensorFlow flag interface. The available options are as follows: Options Default Value Description --uql_weight_bits 4 the number of bits for weight --uql_activation_bits 32 the number of bits for activation\uff0c by default it remains full precision --uql_save_quant_mode_path TODO the save path for quantized models --uql_use_buckets False use bucketing or not --uql_bucket_type channel two bucket type available: ['split', 'channel'] --uql_bucket_size 256 quantize the first and last layers of the network or not --uql_enbl_rl_agent False enable reinforcement learning to learn the optimal bit allocation or not --uql_quantize_all_layers False quantize the first and last layers of the network or not --uql_quant_epoch 60 the number of epochs for fine-tuning","title":"Configure the Learner"},{"location":"uq_learner/#examples","text":"Once the model is built, the quantization can be easily triggered by directly passing the Uniform Quantization Learner in the command line as follows: # quantize resnet-20 on CIFAR-10 # you can also configure the sh ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --data_disk local \\ --data_dir_local ${PF_CIFAR10_LOCAL} \\ --learner=uniform \\ --uql_weight_bits=4 \\ --uql_activation_bits=4 \\ # quantize the resnet-18 on ILSVRC-12 sh ./scripts/run_local.sh nets/resnet_at_ilsvrc12_run.py \\ --learner=uniform \\ --data_disk local \\ --data_dir_local ${PF_ILSVRC12_LOCAL} \\ --uql_weight_bits=8 \\ --uql_activation_bits=8 \\ --uql_use_buckets=True \\ --uql_bucket_type=channel","title":"Examples"},{"location":"uq_learner/#configure-the-hyper-parameter-optimizer","text":"Once the hyper parameter optimizer is turned on, i.e., uql_enbl_rl_agent==True , the reinforcement learning agents will search for the optimal allocation of bits to each layers. Before the search, users are supposed set up the bit constraints via --uql_evquivalent_bits , so that the optimal bits searched by the RL agent will not exceed the bit number without RL agent. For example, TODO Users can also configure other options in the RL agent, such as the number of roll-outs, the fine-tuning steps to get the reward, e.t.c.. Full list of options are listed as follows: Options Default Value Description --uql_evquivalent_bits 4 the number of re-allocated bits that is equivalent to uniform quantization without RL agent --uql_nb_rlouts 200 the number of roll outs for training the RL agent --uql_w_bit_min 2 the minimal number of bits for each layer --uql_w_bit_max 8 the maximal number of bits for each layer --uql_enbl_rl_global_tune True enable fine-tuning all layers of the network or not --uql_enbl_rl_layerwise_tune False enable fine-tuning the network layer by layer or not --uql_tune_layerwise_steps 100 the number of steps for layerwise fine-tuning --uql_tune_global_steps 2000 the number of steps for global fine-tuning --uql_tune_disp_steps 300 the display steps to show the fine-tuning progress --uql_enbl_random_layers True randomly permute the layers during RL agent training","title":"Configure the Hyper Parameter Optimizer"},{"location":"uq_learner/#examples_1","text":"# quantize mobilenet-v1 on ILSVRC-12 sh ./scripts/run_local.sh nets/mobilnet_at_ilsvrc12_run.py \\ --data_disk local \\ --data_dir_local ${PF_CIFAR10_LOCAL} \\ --learner=uniform \\ --uql_enbl_rl_agent=True \\ --uql_equivalent_bits=4 \\ --uql_tune_global_steps=1200","title":"Examples"},{"location":"uq_learner/#performance","text":"Here we list some of the performance on Cifar-10 using the Uniform Quantization Learner and the built-in models in PocketFlow. The options not displayed remain the default values. Model Weight Bit Activation Bit Acc ResNet-20 32 32 91.96 ResNet-20 4 4 90.73 ResNet-20 8 8 92.25 Model Weight Bit Bucketing Acc ResNet-20 2 channel 89.67 ResNet-20 4 channel 92.02 ResNet-20 2 split 91.15 ResNet-20 4 split 91.98 Model Weight Bit RL search Acc ResNet-20 2 FALSE 86.17 ResNet-20 4 FALSE 91.76 ResNet-20 2 TRUE 90.30 ResNet-20 4 TRUE 91.88","title":"Performance"},{"location":"uq_learner/#tensorflow-quantization-wrapper","text":"PocketFlow wraps the post training quantization in Tensorflow, and include all the necessary steps to convert the model to the .tflite format, which can be deployed on Andriod devices. To run the wrapper, users only need to get the checkpoint files ready, and then run the script.","title":"TensorFlow Quantization Wrapper"},{"location":"uq_learner/#prepare-the-checkpoint-files","text":"Generally in TensorFlow, the checkpoints of a model include three files: .data, .index, .meta. Users are also supposed to add the input and output to collections, and configure the wrapper to acquire the corresponding collections. An example is as follows: TODO add quantization to the conversion tools # load the checkpoints in ./models, and read the collections of 'inputs' and 'outputs' python export_pb_tflite_models.py \\ --model_dir ./models --input_coll inputs --output_coll outputs --quantize True If successfully transformed, the .pb and .tflite files will be saved in ./models .","title":"Prepare the Checkpoint Files"},{"location":"uq_learner/#deploy-on-mobile-devices","text":"TODO","title":"Deploy on Mobile Devices"},{"location":"uq_learner/#algorithms","text":"Now we introduce the detailed algorithm in the Uniform Quantization Learner. As is shown in the following graph, given a full precision model, the Uniform Quantization Learner inserts quantization nodes into the computation graph of the model. To enable activation quantization, the quantization nodes shall also be inserted after the activation function. In the training phase, both full-precision and quantized weights are stored. During the forward pass, quantized weights are obtained by applying the quantization functions on the full precision weights. For the backward propagation of gradients, since the gradients w.r.t. the quantized weights are 0 almost everywhere, we use the straight-through estimator (STE) ( Hinton et.al 2012 , Bengio et.al 2013 ) to pass the gradient of quantized weights directly to the full precision weights for update.","title":"Algorithms"},{"location":"uq_learner/#uniform-quantization-function","text":"Uniform quantization distributed the quantization points evenly across the distribution of weights, and the full precision numbers are then assigned to the closest quantization point. To achieve this, we first normalize the full precision weights x x of one layer to [0, 1] [0, 1] , i.e., $$ sc(x) = \\frac{x-\\beta}{\\alpha}, $$ where \\alpha=\\max{x}-\\min{x} \\alpha=\\max{x}-\\min{x} and \\beta = \\min{x} \\beta = \\min{x} are the scaling factors. Then we assign sc(x) sc(x) to the discrete value by $$ \\hat{x}=\\frac{1}{2^k-1}\\mathrm{round}((2^k-1)\\cdot sc(x)), $$ and finally we do the inverse linear transformation to recover the quantized weights to the original scale, $$ Q(x)=\\alpha\\hat{x}+\\beta. $$","title":"Uniform Quantization Function"},{"location":"uq_learner/#references","text":"Bengio Y, L\u00e9onard N, Courville A. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013 Geoffrey Hinton, Nitish Srivastava, Kevin Swersky, Tijmen Tieleman and Abdelrahman Mohamed. Neural Networks for Machine Learning. Coursera, video lectures, 2012","title":"References"},{"location":"ws_learner/","text":"Weight Sparsification Introduction By imposing sparsity constraints on convolutional and fully-connected layers, the number of non-zero weights can be dramatically reduced, which leads to smaller model size and lower FLOPS for inference (actual acceleration depends on efficient implementation for sparse operations). Directly training a network with fixed sparsity degree may encounter some optimization difficulties and takes longer time to converge. To overcome this, Zhu Gupta proposed a dynamic pruning schedule to gradually remove network weights to simplify the optimization process (Zhu Gupta, 2017). Note: in this documentation, we will use both \"sparsity\" and \"pruning ratio\" to denote the ratio of zero-valued weights over all weights. Algorithm Description For each convolutional kernel (for convolutional layer) or weighting matrix (for fully-connected layer), we create a binary mask of the same size to impose the sparsity constraint. During the forward pass, the convolutional kernel (or weighting matrix) is multiplied with the binary mask, so that some weights will not participate in the computation and also will not be updated via gradients. The binary mask is computed based on absolute values of weights: weight with the smallest absolute value will be masked-out until the desired sparsity is reached. During the training process, the sparsity is gradually increased to improve the overall optimization behaviour. The dynamic pruning schedule is defined as: s_{t} = s_{f} - s_{f} \\cdot \\left( 1 - \\frac{t - t_{b}}{t_{e} - t_{b}} \\right)^{\\alpha}, t \\in \\left[ t_{b}, t_{e} \\right] s_{t} = s_{f} - s_{f} \\cdot \\left( 1 - \\frac{t - t_{b}}{t_{e} - t_{b}} \\right)^{\\alpha}, t \\in \\left[ t_{b}, t_{e} \\right] where s_{t} s_{t} is the sparsity at iteration # t t , s_{f} s_{f} is the target sparsity, t_{b} t_{b} and t_{e} t_{e} are the iteration indices where the sparsity begins and stops increasing, and \\alpha \\alpha is the exponent term. In the actual implementation, the binary mask is not updated at each iteration. Instead, it is updated every \\Delta t \\Delta t iterations so as to stabilize the training process. We visualize the dynamic pruning schedule in the figure below. Most networks consist of multiple layers, and the weight redundancy may differ from one layer to another. In order to maximally exploit the weight redundancy, we incorporate a reinforcement learning controller to automatically determine the optimal sparsity (or pruning ratio) for each layer. In each roll-out, the RL agent sequentially determine the sparsity for each layer, and then the network is pruned and re-trained for a few iterations using layer-wise regression global fine-tuning. The reward function's value is computed based on the re-trained network's accuracy (and computation efficiency), and then used update model parameters of RL agent. For more details, please refer to the documentation named \"Hyper-parameter Optimizer - Reinforcement Learning\". Hyper-parameters Below is the full list of hyper-parameters used in the weight sparsification learner: Name Description ws_save_path model's save path ws_prune_ratio target pruning ratio ws_prune_ratio_prtl pruning ratio protocol: 'uniform' / 'heurist' / 'optimal' ws_nb_rlouts number of roll-outs for the RL agent ws_nb_rlouts_min minimal number of roll-outs for the RL agent to start training ws_reward_type reward type: 'single-obj' / 'multi-obj' ws_lrn_rate_rg learning rate for layer-wise regression ws_nb_iters_rg number of iterations for layer-wise regression ws_lrn_rate_ft learning rate for global fine-tuning ws_nb_iters_ft number of iterations for global fine-tuning ws_nb_iters_feval number of iterations for fast evaluation ws_prune_ratio_exp pruning ratio's exponent term ws_iter_ratio_beg iteration ratio at which the pruning ratio begins increasing ws_iter_ratio_end iteration ratio at which the pruning ratio stops increasing ws_mask_update_step step size for updating the pruning mask Here, we provide detailed description (and some analysis) for above hyper-parameters: ws_save_path : save path for model created in the training graph. The resulting checkpoint files can be used to resume training from a previous run and compute model's loss function's value and some other evaluation metrics. ws_prune_ratio : target pruning ratio for convolutional fully-connected layers. The larger ws_prune_ratio is, the more weights will be pruned. If ws_prune_ratio equals 0, then no weights will be pruned and model remains the same; if ws_prune_ratio equals 1, then all weights are pruned. ws_prune_ratio_prtl : pruning ratio protocol. Possible options include: 1) uniform: all layers use the same pruning ratio; 2) heurist: the more weights in one layer, the higher pruning ratio will be; 3) optimal: each layer's pruning ratio is determined by reinforcement learning. ws_nb_rlouts : number of roll-outs for training the reinforcement learning agent. A roll-out refers to: use the RL agent to determine the pruning ratio for each layer; fine-tune the weight sparsified network; evaluate the fine-tuned network to obtain the reward value. ws_nb_rlouts_min : minimal number of roll-outs for the RL agent to start training. The RL agent requires a few roll-outs for random exploration before actual training starts. We recommend to set this to be a quarter of ws_nb_rlouts . ws_reward_type : reward function's type for the RL agent. Possible options include: 1) single-obj: the reward function only depends on the compressed model's accuracy (the sparsity constraint is imposed during roll-out); 2) multi-obj: the reward function depends on both the compressed model's accuracy and the actual sparsity. ws_lrn_rate_rg : learning rate for layer-wise regression. ws_nb_iters_rg : number of iterations for layer-wise regression. This should be set to some value that the layer-wise regression can almost converge and the loss function's value does not decrease much even if more iterations are used. ws_lrn_rate_ft : learning rate for global fine-tuning. ws_nb_iters_ft : number of iterations for global fine-tuning. This should be set to some value that the global fine-tuning can almost converge and the loss function's value does not decrease much even if more iterations are used. ws_nb_iters_feval : number of iterations for fast evaluation. In each roll-out, the re-trained network is evaluated on a subset of evaluation data to save time. ws_prune_ratio_exp : pruning ratio's exponent term as defined in the dynamic pruning schedule above. ws_iter_ratio_beg : iteration ratio at which the pruning ratio begins increasing. In the dynamic pruning schedule defined above, t_{b} t_{b} equals to the total number of training iterations multiplied with ws_iter_ratio_beg . ws_iter_ratio_end : iteration ratio at which the pruning ratio stops increasing. In the dynamic pruning schedule defined above, t_{e} t_{e} equals to the total number of training iterations multiplied with ws_iter_ratio_end . ws_mask_update_step : step size for updating the pruning mask. By increasing ws_mask_update_step , binary masks for weight pruning are less frequently updated, which will speed-up the training but the difference between pre-update and post-update sparsity will be larger. Usage Examples In this section, we provide some usage examples to demonstrate how to use WeightSparseLearner under different execution modes and hyper-parameter combinations: To compress a ResNet-20 model for CIFAR-10 classification task in the local mode, use: # set the target pruning ratio to 0.75 ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner weight-sparse \\ --ws_prune_ratio 0.75 To compress a ResNet-34 model for ILSVRC-12 classification task in the docker mode with 4 GPUs, use: # set the number of channel pruning stages to 4 ./scripts/run_docker.sh nets/resnet_at_ilsvrc12_run.py -n=4 \\ --learner weight-sparse \\ --data_disk docker \\ --resnet_size 34 \\ --dcp_nb_stages 4 To compress a MobileNet-v2 model for ILSVRC-12 classification task in the seven mode with 8 GPUs, use: # enable training with distillation loss ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py -n=8 \\ --learner weight-sparse \\ --data_disk seven \\ --mobilenet_version 2 \\ --enbl_dst","title":"Weight Sparsification"},{"location":"ws_learner/#weight-sparsification","text":"","title":"Weight Sparsification"},{"location":"ws_learner/#introduction","text":"By imposing sparsity constraints on convolutional and fully-connected layers, the number of non-zero weights can be dramatically reduced, which leads to smaller model size and lower FLOPS for inference (actual acceleration depends on efficient implementation for sparse operations). Directly training a network with fixed sparsity degree may encounter some optimization difficulties and takes longer time to converge. To overcome this, Zhu Gupta proposed a dynamic pruning schedule to gradually remove network weights to simplify the optimization process (Zhu Gupta, 2017). Note: in this documentation, we will use both \"sparsity\" and \"pruning ratio\" to denote the ratio of zero-valued weights over all weights.","title":"Introduction"},{"location":"ws_learner/#algorithm-description","text":"For each convolutional kernel (for convolutional layer) or weighting matrix (for fully-connected layer), we create a binary mask of the same size to impose the sparsity constraint. During the forward pass, the convolutional kernel (or weighting matrix) is multiplied with the binary mask, so that some weights will not participate in the computation and also will not be updated via gradients. The binary mask is computed based on absolute values of weights: weight with the smallest absolute value will be masked-out until the desired sparsity is reached. During the training process, the sparsity is gradually increased to improve the overall optimization behaviour. The dynamic pruning schedule is defined as: s_{t} = s_{f} - s_{f} \\cdot \\left( 1 - \\frac{t - t_{b}}{t_{e} - t_{b}} \\right)^{\\alpha}, t \\in \\left[ t_{b}, t_{e} \\right] s_{t} = s_{f} - s_{f} \\cdot \\left( 1 - \\frac{t - t_{b}}{t_{e} - t_{b}} \\right)^{\\alpha}, t \\in \\left[ t_{b}, t_{e} \\right] where s_{t} s_{t} is the sparsity at iteration # t t , s_{f} s_{f} is the target sparsity, t_{b} t_{b} and t_{e} t_{e} are the iteration indices where the sparsity begins and stops increasing, and \\alpha \\alpha is the exponent term. In the actual implementation, the binary mask is not updated at each iteration. Instead, it is updated every \\Delta t \\Delta t iterations so as to stabilize the training process. We visualize the dynamic pruning schedule in the figure below. Most networks consist of multiple layers, and the weight redundancy may differ from one layer to another. In order to maximally exploit the weight redundancy, we incorporate a reinforcement learning controller to automatically determine the optimal sparsity (or pruning ratio) for each layer. In each roll-out, the RL agent sequentially determine the sparsity for each layer, and then the network is pruned and re-trained for a few iterations using layer-wise regression global fine-tuning. The reward function's value is computed based on the re-trained network's accuracy (and computation efficiency), and then used update model parameters of RL agent. For more details, please refer to the documentation named \"Hyper-parameter Optimizer - Reinforcement Learning\".","title":"Algorithm Description"},{"location":"ws_learner/#hyper-parameters","text":"Below is the full list of hyper-parameters used in the weight sparsification learner: Name Description ws_save_path model's save path ws_prune_ratio target pruning ratio ws_prune_ratio_prtl pruning ratio protocol: 'uniform' / 'heurist' / 'optimal' ws_nb_rlouts number of roll-outs for the RL agent ws_nb_rlouts_min minimal number of roll-outs for the RL agent to start training ws_reward_type reward type: 'single-obj' / 'multi-obj' ws_lrn_rate_rg learning rate for layer-wise regression ws_nb_iters_rg number of iterations for layer-wise regression ws_lrn_rate_ft learning rate for global fine-tuning ws_nb_iters_ft number of iterations for global fine-tuning ws_nb_iters_feval number of iterations for fast evaluation ws_prune_ratio_exp pruning ratio's exponent term ws_iter_ratio_beg iteration ratio at which the pruning ratio begins increasing ws_iter_ratio_end iteration ratio at which the pruning ratio stops increasing ws_mask_update_step step size for updating the pruning mask Here, we provide detailed description (and some analysis) for above hyper-parameters: ws_save_path : save path for model created in the training graph. The resulting checkpoint files can be used to resume training from a previous run and compute model's loss function's value and some other evaluation metrics. ws_prune_ratio : target pruning ratio for convolutional fully-connected layers. The larger ws_prune_ratio is, the more weights will be pruned. If ws_prune_ratio equals 0, then no weights will be pruned and model remains the same; if ws_prune_ratio equals 1, then all weights are pruned. ws_prune_ratio_prtl : pruning ratio protocol. Possible options include: 1) uniform: all layers use the same pruning ratio; 2) heurist: the more weights in one layer, the higher pruning ratio will be; 3) optimal: each layer's pruning ratio is determined by reinforcement learning. ws_nb_rlouts : number of roll-outs for training the reinforcement learning agent. A roll-out refers to: use the RL agent to determine the pruning ratio for each layer; fine-tune the weight sparsified network; evaluate the fine-tuned network to obtain the reward value. ws_nb_rlouts_min : minimal number of roll-outs for the RL agent to start training. The RL agent requires a few roll-outs for random exploration before actual training starts. We recommend to set this to be a quarter of ws_nb_rlouts . ws_reward_type : reward function's type for the RL agent. Possible options include: 1) single-obj: the reward function only depends on the compressed model's accuracy (the sparsity constraint is imposed during roll-out); 2) multi-obj: the reward function depends on both the compressed model's accuracy and the actual sparsity. ws_lrn_rate_rg : learning rate for layer-wise regression. ws_nb_iters_rg : number of iterations for layer-wise regression. This should be set to some value that the layer-wise regression can almost converge and the loss function's value does not decrease much even if more iterations are used. ws_lrn_rate_ft : learning rate for global fine-tuning. ws_nb_iters_ft : number of iterations for global fine-tuning. This should be set to some value that the global fine-tuning can almost converge and the loss function's value does not decrease much even if more iterations are used. ws_nb_iters_feval : number of iterations for fast evaluation. In each roll-out, the re-trained network is evaluated on a subset of evaluation data to save time. ws_prune_ratio_exp : pruning ratio's exponent term as defined in the dynamic pruning schedule above. ws_iter_ratio_beg : iteration ratio at which the pruning ratio begins increasing. In the dynamic pruning schedule defined above, t_{b} t_{b} equals to the total number of training iterations multiplied with ws_iter_ratio_beg . ws_iter_ratio_end : iteration ratio at which the pruning ratio stops increasing. In the dynamic pruning schedule defined above, t_{e} t_{e} equals to the total number of training iterations multiplied with ws_iter_ratio_end . ws_mask_update_step : step size for updating the pruning mask. By increasing ws_mask_update_step , binary masks for weight pruning are less frequently updated, which will speed-up the training but the difference between pre-update and post-update sparsity will be larger.","title":"Hyper-parameters"},{"location":"ws_learner/#usage-examples","text":"In this section, we provide some usage examples to demonstrate how to use WeightSparseLearner under different execution modes and hyper-parameter combinations: To compress a ResNet-20 model for CIFAR-10 classification task in the local mode, use: # set the target pruning ratio to 0.75 ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner weight-sparse \\ --ws_prune_ratio 0.75 To compress a ResNet-34 model for ILSVRC-12 classification task in the docker mode with 4 GPUs, use: # set the number of channel pruning stages to 4 ./scripts/run_docker.sh nets/resnet_at_ilsvrc12_run.py -n=4 \\ --learner weight-sparse \\ --data_disk docker \\ --resnet_size 34 \\ --dcp_nb_stages 4 To compress a MobileNet-v2 model for ILSVRC-12 classification task in the seven mode with 8 GPUs, use: # enable training with distillation loss ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py -n=8 \\ --learner weight-sparse \\ --data_disk seven \\ --mobilenet_version 2 \\ --enbl_dst","title":"Usage Examples"}]}