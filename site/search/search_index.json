{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PocketFlow PocketFlow is an open-source framework for compressing and accelerating deep learning models with minimal human effort. Deep learning is widely used in various areas, including face verification, speech recognition, and natural language translation. However, deep learning models are often computational expensive, which limits their further applications on mobile devices. PocketFlow aims at providing an easy-to-use toolkit for developers to improve the inference efficiency with little or no performance degradation. Developers only needs to specify the desired compression and/or acceleration ratios and then PocketFlow will automatically choose proper hyper-parameters to generate a highly efficient compressed model for deployment. Framework PocketFlow mainly consists of two categories of algorithm components: Learner : model compression / acceleration method that takes an uncompressed original model as input and generates a compressed candidate model using certain training methods. Hyper-parameter Optimizer : an external controller unit that evaluates the effectiveness of candidate models and searches for the optimal hyper-parameter combination for learners. Currently, PocketFlow supports following model compression / accelerating methods: Channel Pruning : Weight Sparsification : Weight Quantization : All the above model compression / acceleration methods support two training methods: Fast Training with No / Partial data : The compressed model is directly derived from the original model, by applying either pruning masks or quantization functions. The resulting model can be fine-tuned with a few iterations to recover the accuracy to some extent. Re-Training with Full Data (+Distillation) : The compressed model is","title":"Home"},{"location":"#pocketflow","text":"PocketFlow is an open-source framework for compressing and accelerating deep learning models with minimal human effort. Deep learning is widely used in various areas, including face verification, speech recognition, and natural language translation. However, deep learning models are often computational expensive, which limits their further applications on mobile devices. PocketFlow aims at providing an easy-to-use toolkit for developers to improve the inference efficiency with little or no performance degradation. Developers only needs to specify the desired compression and/or acceleration ratios and then PocketFlow will automatically choose proper hyper-parameters to generate a highly efficient compressed model for deployment.","title":"PocketFlow"},{"location":"#framework","text":"PocketFlow mainly consists of two categories of algorithm components: Learner : model compression / acceleration method that takes an uncompressed original model as input and generates a compressed candidate model using certain training methods. Hyper-parameter Optimizer : an external controller unit that evaluates the effectiveness of candidate models and searches for the optimal hyper-parameter combination for learners. Currently, PocketFlow supports following model compression / accelerating methods: Channel Pruning : Weight Sparsification : Weight Quantization : All the above model compression / acceleration methods support two training methods: Fast Training with No / Partial data : The compressed model is directly derived from the original model, by applying either pruning masks or quantization functions. The resulting model can be fine-tuned with a few iterations to recover the accuracy to some extent. Re-Training with Full Data (+Distillation) : The compressed model is","title":"Framework"},{"location":"cp_learner/","text":"Channel Pruning Introduction Channel pruning is a kind of structural model compression approach which can not only compress the model size, but accelerate the inference speed directly. PocketFlow uses the Yihui. et. al[1] 's channel pruning algorithm to pruning each channel of convolution layers with a certain ratio, and for details please refer to the channel pruning paper . For better performance and more robust, we modify some parts of the algorithm to achieve better result. In order to achieve a better performance, PocketFlow can take advantages of reinforcement learning to search a better compression ratio[2]. User can also use the distilling [3] and group tuning function to improve the accuracy after compression. Group tuning means setting a certain number of layers as group and then pruning and finetuning/retraining each group sequentially. For example we can set each 3 layers as a group and then pruning the first 3 layers. After that finetune/retraine the whole model and prune the next 3 layers and so on. Distilling and group tuning are experimentally proved as effective approaches to achieve higher accuracy at a certain compression ratio in most situations. Pruning Option The code of channel pruning are located at directory ./learners/channel_pruning . To use channel pruning. users can set --learners to channel . The Channel pruning supports 3 kinds of pruning setup by prune_option option. Uniform Channel Pruning One is the uniform layer pruning, which means the user can set each layer has a uniform pruning ratio by --prune_option=uniform and set the ratio (eg. making the ratio 0.5) by --uniform_preserve_ratio=0.5 . Example: ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --uniform_preserve_ratio=0.5 \\ --learner=channel\\ --prune_option=uniform\\ --batch_size_eval=64 \\ --resnet_size=20 List Channel Pruning Another pruning option is pruning the corresponding layer with ratios listed in a named ratio.list file, the file name of which can be set by --prune_list_file option. the ratio value must be separated by a comma. User can set --prune_option=list to prune the model by list ratios. Example: Add list 1.0, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 1.0, 0.25, 1.0, 0.25, 0.21875, 0.21875, 0.21875, 1.0, 0.5625, 1.0, 0.546875, 0.546875, 0.546875, 1 in ./ratio.list ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --learner=channel\\ --prune_option=list \\ --batch_size_eval=64 \\ --resnet_size=20 Automatic Channel Pruning The last one pruning option is searching better pruning ratios by reinforcement learning and you only need to give a value which represents what the ratio of total FLOPs/Computation you wants the compressed model preserve. You can set --prune_option=auto and set a preserve ratio number such as --preserve_ratio=0.5 . User can also use cp_nb_rlouts_min to control reinforcement learning warm up iterations, which means the RL agent start to learn after the iterations, the default value is 50 . User can also use cp_nb_rlouts_min to control the total iteration RL agent to search, the default value is 200 . If the user want to control other parameters of the agents, please refer to the reinforcement component page. Example: ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --preserve_ratio=0.5 \\ --learner=channel\\ --prune_option=auto \\ --batch_size_eval=64 \\ --resnet_size=20 Channel pruning parameters The implementation of the channel pruning use Lasso algorithm to do channel selection and linear regression to do feature map reconstruction. During these two phases, sampling is done on the feature map to reduce computation cost. The users can use --nb_points_per_layer to set how many sampling points on each layer are taken, the default value is 10 . For some dataset, if the images contain too many zero pixels (eg. black color), the value should be greater. The users can also set using how many batches to do channel selection and feature reconstruction by nb_batches , the default value is 60 . Small value of nb_batches may cause over-fitting and large value may slow down the solving speed, so a good value depends on the nets and dataset. For more practical usage, user may consider make the channel number of each layer is the quadruple for fast inference of mobile devices. In this case, user can set --quadruple to True to make the compressed model have a quadruple number of channels. Distilling Distilling is an effective approach to improve the final accuracy of compressed model with PocketFlow in most situations of classification. User can set --enbl_dst=True to enable distillling. Group Tuning As introduced above, group tuning was proposed by the PocketFlow team and finding it is very useful to improve the performance of model compression. In PocketFlow, users can set the group number by --list_group , the default value is 1000 . User can also set the number of iterations to finetune by setting nb_iters_ft_ratio which mean the ratio the total iterations to be used in finetuning. The learning rate of finetuning can be set by lrn_rate_ft . [1] He, Y., Zhang, X. and Sun, J., 2017, October. Channel pruning for accelerating very deep neural networks. In International Conference on Computer Vision (ICCV) (Vol. 2, No. 6). [2] He, Y., Lin, J., Liu, Z., Wang, H., Li, L.J. and Han, S., 2018, September. Amc: Automl for model compression and acceleration on mobile devices. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 784-800). [3] Hinton, G., Vinyals, O. and Dean, J., 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 .","title":"Channel Pruning"},{"location":"cp_learner/#channel-pruning","text":"","title":"Channel Pruning"},{"location":"cp_learner/#introduction","text":"Channel pruning is a kind of structural model compression approach which can not only compress the model size, but accelerate the inference speed directly. PocketFlow uses the Yihui. et. al[1] 's channel pruning algorithm to pruning each channel of convolution layers with a certain ratio, and for details please refer to the channel pruning paper . For better performance and more robust, we modify some parts of the algorithm to achieve better result. In order to achieve a better performance, PocketFlow can take advantages of reinforcement learning to search a better compression ratio[2]. User can also use the distilling [3] and group tuning function to improve the accuracy after compression. Group tuning means setting a certain number of layers as group and then pruning and finetuning/retraining each group sequentially. For example we can set each 3 layers as a group and then pruning the first 3 layers. After that finetune/retraine the whole model and prune the next 3 layers and so on. Distilling and group tuning are experimentally proved as effective approaches to achieve higher accuracy at a certain compression ratio in most situations.","title":"Introduction"},{"location":"cp_learner/#pruning-option","text":"The code of channel pruning are located at directory ./learners/channel_pruning . To use channel pruning. users can set --learners to channel . The Channel pruning supports 3 kinds of pruning setup by prune_option option.","title":"Pruning Option"},{"location":"cp_learner/#uniform-channel-pruning","text":"One is the uniform layer pruning, which means the user can set each layer has a uniform pruning ratio by --prune_option=uniform and set the ratio (eg. making the ratio 0.5) by --uniform_preserve_ratio=0.5 . Example: ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --uniform_preserve_ratio=0.5 \\ --learner=channel\\ --prune_option=uniform\\ --batch_size_eval=64 \\ --resnet_size=20","title":"Uniform Channel Pruning"},{"location":"cp_learner/#list-channel-pruning","text":"Another pruning option is pruning the corresponding layer with ratios listed in a named ratio.list file, the file name of which can be set by --prune_list_file option. the ratio value must be separated by a comma. User can set --prune_option=list to prune the model by list ratios. Example: Add list 1.0, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 0.1875, 1.0, 0.25, 1.0, 0.25, 0.21875, 0.21875, 0.21875, 1.0, 0.5625, 1.0, 0.546875, 0.546875, 0.546875, 1 in ./ratio.list ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --learner=channel\\ --prune_option=list \\ --batch_size_eval=64 \\ --resnet_size=20","title":"List Channel Pruning"},{"location":"cp_learner/#automatic-channel-pruning","text":"The last one pruning option is searching better pruning ratios by reinforcement learning and you only need to give a value which represents what the ratio of total FLOPs/Computation you wants the compressed model preserve. You can set --prune_option=auto and set a preserve ratio number such as --preserve_ratio=0.5 . User can also use cp_nb_rlouts_min to control reinforcement learning warm up iterations, which means the RL agent start to learn after the iterations, the default value is 50 . User can also use cp_nb_rlouts_min to control the total iteration RL agent to search, the default value is 200 . If the user want to control other parameters of the agents, please refer to the reinforcement component page. Example: ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --preserve_ratio=0.5 \\ --learner=channel\\ --prune_option=auto \\ --batch_size_eval=64 \\ --resnet_size=20","title":"Automatic Channel Pruning"},{"location":"cp_learner/#channel-pruning-parameters","text":"The implementation of the channel pruning use Lasso algorithm to do channel selection and linear regression to do feature map reconstruction. During these two phases, sampling is done on the feature map to reduce computation cost. The users can use --nb_points_per_layer to set how many sampling points on each layer are taken, the default value is 10 . For some dataset, if the images contain too many zero pixels (eg. black color), the value should be greater. The users can also set using how many batches to do channel selection and feature reconstruction by nb_batches , the default value is 60 . Small value of nb_batches may cause over-fitting and large value may slow down the solving speed, so a good value depends on the nets and dataset. For more practical usage, user may consider make the channel number of each layer is the quadruple for fast inference of mobile devices. In this case, user can set --quadruple to True to make the compressed model have a quadruple number of channels.","title":"Channel pruning parameters"},{"location":"cp_learner/#distilling","text":"Distilling is an effective approach to improve the final accuracy of compressed model with PocketFlow in most situations of classification. User can set --enbl_dst=True to enable distillling.","title":"Distilling"},{"location":"cp_learner/#group-tuning","text":"As introduced above, group tuning was proposed by the PocketFlow team and finding it is very useful to improve the performance of model compression. In PocketFlow, users can set the group number by --list_group , the default value is 1000 . User can also set the number of iterations to finetune by setting nb_iters_ft_ratio which mean the ratio the total iterations to be used in finetuning. The learning rate of finetuning can be set by lrn_rate_ft . [1] He, Y., Zhang, X. and Sun, J., 2017, October. Channel pruning for accelerating very deep neural networks. In International Conference on Computer Vision (ICCV) (Vol. 2, No. 6). [2] He, Y., Lin, J., Liu, Z., Wang, H., Li, L.J. and Han, S., 2018, September. Amc: Automl for model compression and acceleration on mobile devices. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 784-800). [3] Hinton, G., Vinyals, O. and Dean, J., 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 .","title":"Group Tuning"},{"location":"dcp_learner/","text":"","title":"Discrimination-aware Channel Pruning"},{"location":"distillation/","text":"","title":"Distillation"},{"location":"faq/","text":"Frequently Asked Questions Q: Under construction ... A: Under construction ...","title":"Frequently Asked Questions"},{"location":"faq/#frequently-asked-questions","text":"Q: Under construction ... A: Under construction ...","title":"Frequently Asked Questions"},{"location":"installation/","text":"Installation PocketFlow is developed and tested on Linux, using Python 3.x and TensorFlow 1.10.0. We support the following three execution modes for PocketFlow: Local mode: run PocketFlow on the local machine. Docker mode: run PocketFlow within a docker image. Seven mode: run PocketFlow on the seven cluster. We recommend to use Anaconda as the Python environment, and then install TensorFlow with pip. Alternatively, you may download our pre-built docker image, which integrates TensorFlow and Horovod , an open-source distributed training framework if multi-GPU training is favored. Clone PocketFlow To make a local copy of the PocketFlow repository, use: $ git clone http://git.code.oa.com/ml/PocketFlow.git Prepare for the Local Mode We recommend to use Anaconda as the Python environment, which has many essential packages built-in. The Anaconda installer can be downloaded from here . To install, use the following command: # install Anaconda; replace the installer's file name if needed $ bash Anaconda3-5.2.0-Linux-x86_64.sh # activate Anaconda's Python path $ source ~/.bashrc For Anaconda 5.3.0 or later, the default Python version is 3.7, which does not support installing TensorFlow with pip directly. Therefore, you need to manually switch to Python 3.6 once Anaconda is installed: # install Python 3.6 $ conda install python=3.6 To install TensorFlow, you may refer to TensorFlow's official documentation for detailed instructions. Specially, if GPU-based training is required, then you need to follow the GPU support guide to set up a CUDA-enabled GPU card in prior to installation. After that, install TensorFlow with: # TensorFlow with GPU support; use tensorflow if GPU is not available $ pip install tensorflow-gpu # verify the install $ python -c import tensorflow as tf; print(tf.__version__) To run PocketFlow in the local mode, e.g. to train a full-precision ResNet-20 model for the CIFAR-10 classification task, use the following command: # DATA_DIR_LOCAL: directory path to the CIFAR-10 data set on the local machine # HDFS_URL: IP address of HDFS namenode, prefixed with protocol, followed by WebHDFS port $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner full-prec \\ --data_disk local \\ --data_dir_local ${DATA_DIR_LOCAL} \\ --hdfs_url ${HDFS_URL} Prepare for the Docker Mode Docker offers an alternative way to run PocketFlow within an isolated container, so that your local Python environment remains untouched. We recommend you to use the horovod docker provided by Uber, which enables multi-GPU distributed training for TensorFlow with only a few lines modification. Once docker is installed, the docker image can be obtained via: # obtain the docker image $ docker pull uber/horovod To run PocketFlow in the docker mode, e.g. to train a full-precision ResNet-20 model for the CIFAR-10 classification task, use the following command: # DATA_DIR_LOCAL: directory path to the CIFAR-10 data set on the local machine # HDFS_URL: IP address of HDFS namenode, prefixed with protocol, followed by WebHDFS port $ ./scripts/run_docker.sh nets/resnet_at_cifar10_run.py ${DATA_DIR_LOCAL} \\ --learner full-prec \\ --data_disk local \\ --data_dir_local /opt/ml/data \\ --hdfs_url ${HDFS_URL} Prepare for the Seven Mode Seven is a distributed learning platform built for both CPU and GPU clusters. Users can submit tasks to the seven cluster, using built-in data sets and docker images seamlessly. For detailed introduction, please refer to its documentation site . To run PocketFlow in the seven mode, e.g. to train a full-precision ResNet-20 model for the CIFAR-10 classification task, use the following command: # DATA_DIR_SEVEN: directory path to the CIFAR-10 data set on the seven cluster # HDFS_URL: IP address of HDFS namenode, prefixed with protocol, followed by WebHDFS port $ ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --learner full-prec \\ --data_disk seven \\ --data_dir_seven ${DATA_DIR_SEVEN} \\ --hdfs_url ${HDFS_URL}","title":"Installation"},{"location":"installation/#installation","text":"PocketFlow is developed and tested on Linux, using Python 3.x and TensorFlow 1.10.0. We support the following three execution modes for PocketFlow: Local mode: run PocketFlow on the local machine. Docker mode: run PocketFlow within a docker image. Seven mode: run PocketFlow on the seven cluster. We recommend to use Anaconda as the Python environment, and then install TensorFlow with pip. Alternatively, you may download our pre-built docker image, which integrates TensorFlow and Horovod , an open-source distributed training framework if multi-GPU training is favored.","title":"Installation"},{"location":"installation/#clone-pocketflow","text":"To make a local copy of the PocketFlow repository, use: $ git clone http://git.code.oa.com/ml/PocketFlow.git","title":"Clone PocketFlow"},{"location":"installation/#prepare-for-the-local-mode","text":"We recommend to use Anaconda as the Python environment, which has many essential packages built-in. The Anaconda installer can be downloaded from here . To install, use the following command: # install Anaconda; replace the installer's file name if needed $ bash Anaconda3-5.2.0-Linux-x86_64.sh # activate Anaconda's Python path $ source ~/.bashrc For Anaconda 5.3.0 or later, the default Python version is 3.7, which does not support installing TensorFlow with pip directly. Therefore, you need to manually switch to Python 3.6 once Anaconda is installed: # install Python 3.6 $ conda install python=3.6 To install TensorFlow, you may refer to TensorFlow's official documentation for detailed instructions. Specially, if GPU-based training is required, then you need to follow the GPU support guide to set up a CUDA-enabled GPU card in prior to installation. After that, install TensorFlow with: # TensorFlow with GPU support; use tensorflow if GPU is not available $ pip install tensorflow-gpu # verify the install $ python -c import tensorflow as tf; print(tf.__version__) To run PocketFlow in the local mode, e.g. to train a full-precision ResNet-20 model for the CIFAR-10 classification task, use the following command: # DATA_DIR_LOCAL: directory path to the CIFAR-10 data set on the local machine # HDFS_URL: IP address of HDFS namenode, prefixed with protocol, followed by WebHDFS port $ ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner full-prec \\ --data_disk local \\ --data_dir_local ${DATA_DIR_LOCAL} \\ --hdfs_url ${HDFS_URL}","title":"Prepare for the Local Mode"},{"location":"installation/#prepare-for-the-docker-mode","text":"Docker offers an alternative way to run PocketFlow within an isolated container, so that your local Python environment remains untouched. We recommend you to use the horovod docker provided by Uber, which enables multi-GPU distributed training for TensorFlow with only a few lines modification. Once docker is installed, the docker image can be obtained via: # obtain the docker image $ docker pull uber/horovod To run PocketFlow in the docker mode, e.g. to train a full-precision ResNet-20 model for the CIFAR-10 classification task, use the following command: # DATA_DIR_LOCAL: directory path to the CIFAR-10 data set on the local machine # HDFS_URL: IP address of HDFS namenode, prefixed with protocol, followed by WebHDFS port $ ./scripts/run_docker.sh nets/resnet_at_cifar10_run.py ${DATA_DIR_LOCAL} \\ --learner full-prec \\ --data_disk local \\ --data_dir_local /opt/ml/data \\ --hdfs_url ${HDFS_URL}","title":"Prepare for the Docker Mode"},{"location":"installation/#prepare-for-the-seven-mode","text":"Seven is a distributed learning platform built for both CPU and GPU clusters. Users can submit tasks to the seven cluster, using built-in data sets and docker images seamlessly. For detailed introduction, please refer to its documentation site . To run PocketFlow in the seven mode, e.g. to train a full-precision ResNet-20 model for the CIFAR-10 classification task, use the following command: # DATA_DIR_SEVEN: directory path to the CIFAR-10 data set on the seven cluster # HDFS_URL: IP address of HDFS namenode, prefixed with protocol, followed by WebHDFS port $ ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --learner full-prec \\ --data_disk seven \\ --data_dir_seven ${DATA_DIR_SEVEN} \\ --hdfs_url ${HDFS_URL}","title":"Prepare for the Seven Mode"},{"location":"multi_gpu/","text":"","title":"Multi-GPU Training"},{"location":"nuq_learner/","text":"Non-Uniform Quantization Learner This document describes how to set up the Non-Uniform Quantization Learner in PocketFlow. In non-uniform quantization, the quantization points are not distributed evenly, and can be optimized via the back-propagation of the network gradients. Consequently, with the same number of bits, non-uniform quantization is more expressive to approximate the original full-precision network comparing to uniform quantization. Following a similar pattern in the previous sections, we first show how to configure the Non-Uniform Quantization Learner, followed by the algorithms used in the learner. Prepare the Model Again, users should first get the model prepared. Users can either use the pre-built models in PocketFlow, or develop their custom models according to TODO . Configure the Learner To configure the learner, users can pass the options via the TensorFlow flag interface. The available options are as follows: Options Default Value Description --nuql_opt_mode weight variables to optimize: ['weights', 'clusters', 'both'] --nuql_init_style quantile the initialization of quantization points: ['quantile', 'uniform'] --nuql_weight_bits 4 the number of bits for weight --nuql_activation_bits 32 the number of bits for activation\uff0c by default it remains full precision --nuql_save_quant_mode_path TODO the save path for quantized models --nuql_use_buckets False use bucketing or not --nuql_bucket_type channel two bucket type available: ['split', 'channel'] --nuql_bucket_size 256 quantize the first and last layers of the network or not --nuql_enbl_rl_agent False enable reinforcement learning to learn the optimal bit allocation or not --nuql_quantize_all_layers False quantize the first and last layers of the network or not --nuql_quant_epoch 60 the number of epochs for fine-tuning Note that since non-uniform quantization cannot be accelerated directly, by default we do not quantize the activations. Examples Once the model is built, the Non-Uniform Quantization Learner can be easily triggered by passing the Uniform Quantization Learner in the command line as follows: # quantize resnet-20 on CIFAR-10 # you can also configure the sh ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --data_disk local \\ --data_dir_local ${PF_CIFAR10_LOCAL} \\ --learner=non-uniform \\ --nuql_weight_bits=4 \\ --nuql_activation_bits=4 \\ # quantize the resnet-18 on ILSVRC-12 sh ./scripts/run_local.sh nets/resnet_at_ilsvrc12_run.py \\ --learner=uniform \\ --data_disk local \\ --data_dir_local ${PF_ILSVRC12_LOCAL} \\ --nuql_weight_bits=8 \\ --nuql_activation_bits=8 \\ --nuql_use_buckets=True \\ --nuql_bucket_type=channel To enable the RL agent, one can follow similar patterns as those in the Uniform Quantization Learner: # quantize mobilenet-v1 on ILSVRC-12 sh ./scripts/run_local.sh nets/mobilnet_at_ilsvrc12_run.py \\ --data_disk local \\ --data_dir_local ${PF_CIFAR10_LOCAL} \\ --learner=uniform \\ --nuql_enbl_rl_agent=True \\ --nuql_equivalent_bits=4 \\ --nuql_tune_global_steps=1200 Performance Here we list some of the performance on Cifar-10 using the Non-Uniform Quantization Learner and the built-in models in PocketFlow. The options not displayed remain the default values. Model Weight Bit Activation Bit Acc ResNet-20 32 32 91.96 ResNet-20 2 4 90.31 ResNet-20 4 8 91.70 Model Weight Bit Bucketing Acc ResNet-20 2 channel 90.90 ResNet-20 4 channel 91.97 ResNet-20 2 split 90.02 ResNet-20 4 split 91.56 Model Weight Bit RL search Acc ResNet-20 2 FALSE 90.31 ResNet-20 4 FALSE 91.70 ResNet-20 2 TRUE 90.60 ResNet-20 4 TRUE 91.79 Algorithm Non-Uniform Quantization Learner adopts a similar training and evaluation procedure to the Uniform Quantization. In the training process, the quantized weights are forwarded. In the backward pass, the full precision weights are updated via the STE estimator. The major difference from uniform quantization is that, the location of quantization points are not evenly distributed, but can be optimized and initialized differently. In the following, we introduce the scheme to update and initialize the quantization points. Optimization the quantization points Unlike uniform quantization, non-uniform quantization can optimize the location of quantization points dynamically during the training of the network, and thereon leads to less quantization loss. The location of quantization points can be updated by summing the gradients of weights that fall into the point ( Han et.al 2015 ), i.e.,: $$ \\frac{\\partial \\mathcal{L}}{\\partial c_k} = \\sum_{i,j}\\frac{\\partial\\mathcal{L}}{\\partial w_{ij}}\\frac{\\partial{w_{ij}}}{\\partial c_k}=\\sum_{ij}\\frac{\\partial\\mathcal{L}}{\\partial{w_{ij}}}1(I_{ij}=k) $$ The following figure taken from Han et.al 2015 shows the process of updating the clusters: Initialization of quantization points Aside from optimizing the quantization points, another helpful strategy is to properly initialize the quantization points according to the distribution of weights. PocketFlow currently supports two kinds of initialization: uniform initialization and quantile initialization. Comparing to uniform initialization, quantile initialization uses the quantiles of weights as the initial locations of quantization points. Quantile initialization considers the distribution of weights and can usually lead to better performance. References Han S, Mao H, and Dally W J. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv:1510.00149, 2015","title":"Non-uniform Quantization"},{"location":"nuq_learner/#non-uniform-quantization-learner","text":"This document describes how to set up the Non-Uniform Quantization Learner in PocketFlow. In non-uniform quantization, the quantization points are not distributed evenly, and can be optimized via the back-propagation of the network gradients. Consequently, with the same number of bits, non-uniform quantization is more expressive to approximate the original full-precision network comparing to uniform quantization. Following a similar pattern in the previous sections, we first show how to configure the Non-Uniform Quantization Learner, followed by the algorithms used in the learner.","title":"Non-Uniform Quantization Learner"},{"location":"nuq_learner/#prepare-the-model","text":"Again, users should first get the model prepared. Users can either use the pre-built models in PocketFlow, or develop their custom models according to TODO .","title":"Prepare the Model"},{"location":"nuq_learner/#configure-the-learner","text":"To configure the learner, users can pass the options via the TensorFlow flag interface. The available options are as follows: Options Default Value Description --nuql_opt_mode weight variables to optimize: ['weights', 'clusters', 'both'] --nuql_init_style quantile the initialization of quantization points: ['quantile', 'uniform'] --nuql_weight_bits 4 the number of bits for weight --nuql_activation_bits 32 the number of bits for activation\uff0c by default it remains full precision --nuql_save_quant_mode_path TODO the save path for quantized models --nuql_use_buckets False use bucketing or not --nuql_bucket_type channel two bucket type available: ['split', 'channel'] --nuql_bucket_size 256 quantize the first and last layers of the network or not --nuql_enbl_rl_agent False enable reinforcement learning to learn the optimal bit allocation or not --nuql_quantize_all_layers False quantize the first and last layers of the network or not --nuql_quant_epoch 60 the number of epochs for fine-tuning Note that since non-uniform quantization cannot be accelerated directly, by default we do not quantize the activations.","title":"Configure the Learner"},{"location":"nuq_learner/#examples","text":"Once the model is built, the Non-Uniform Quantization Learner can be easily triggered by passing the Uniform Quantization Learner in the command line as follows: # quantize resnet-20 on CIFAR-10 # you can also configure the sh ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --data_disk local \\ --data_dir_local ${PF_CIFAR10_LOCAL} \\ --learner=non-uniform \\ --nuql_weight_bits=4 \\ --nuql_activation_bits=4 \\ # quantize the resnet-18 on ILSVRC-12 sh ./scripts/run_local.sh nets/resnet_at_ilsvrc12_run.py \\ --learner=uniform \\ --data_disk local \\ --data_dir_local ${PF_ILSVRC12_LOCAL} \\ --nuql_weight_bits=8 \\ --nuql_activation_bits=8 \\ --nuql_use_buckets=True \\ --nuql_bucket_type=channel To enable the RL agent, one can follow similar patterns as those in the Uniform Quantization Learner: # quantize mobilenet-v1 on ILSVRC-12 sh ./scripts/run_local.sh nets/mobilnet_at_ilsvrc12_run.py \\ --data_disk local \\ --data_dir_local ${PF_CIFAR10_LOCAL} \\ --learner=uniform \\ --nuql_enbl_rl_agent=True \\ --nuql_equivalent_bits=4 \\ --nuql_tune_global_steps=1200","title":"Examples"},{"location":"nuq_learner/#performance","text":"Here we list some of the performance on Cifar-10 using the Non-Uniform Quantization Learner and the built-in models in PocketFlow. The options not displayed remain the default values. Model Weight Bit Activation Bit Acc ResNet-20 32 32 91.96 ResNet-20 2 4 90.31 ResNet-20 4 8 91.70 Model Weight Bit Bucketing Acc ResNet-20 2 channel 90.90 ResNet-20 4 channel 91.97 ResNet-20 2 split 90.02 ResNet-20 4 split 91.56 Model Weight Bit RL search Acc ResNet-20 2 FALSE 90.31 ResNet-20 4 FALSE 91.70 ResNet-20 2 TRUE 90.60 ResNet-20 4 TRUE 91.79","title":"Performance"},{"location":"nuq_learner/#algorithm","text":"Non-Uniform Quantization Learner adopts a similar training and evaluation procedure to the Uniform Quantization. In the training process, the quantized weights are forwarded. In the backward pass, the full precision weights are updated via the STE estimator. The major difference from uniform quantization is that, the location of quantization points are not evenly distributed, but can be optimized and initialized differently. In the following, we introduce the scheme to update and initialize the quantization points.","title":"Algorithm"},{"location":"nuq_learner/#optimization-the-quantization-points","text":"Unlike uniform quantization, non-uniform quantization can optimize the location of quantization points dynamically during the training of the network, and thereon leads to less quantization loss. The location of quantization points can be updated by summing the gradients of weights that fall into the point ( Han et.al 2015 ), i.e.,: $$ \\frac{\\partial \\mathcal{L}}{\\partial c_k} = \\sum_{i,j}\\frac{\\partial\\mathcal{L}}{\\partial w_{ij}}\\frac{\\partial{w_{ij}}}{\\partial c_k}=\\sum_{ij}\\frac{\\partial\\mathcal{L}}{\\partial{w_{ij}}}1(I_{ij}=k) $$ The following figure taken from Han et.al 2015 shows the process of updating the clusters:","title":"Optimization the quantization points"},{"location":"nuq_learner/#initialization-of-quantization-points","text":"Aside from optimizing the quantization points, another helpful strategy is to properly initialize the quantization points according to the distribution of weights. PocketFlow currently supports two kinds of initialization: uniform initialization and quantile initialization. Comparing to uniform initialization, quantile initialization uses the quantiles of weights as the initial locations of quantization points. Quantile initialization considers the distribution of weights and can usually lead to better performance.","title":"Initialization of quantization points"},{"location":"nuq_learner/#references","text":"Han S, Mao H, and Dally W J. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv:1510.00149, 2015","title":"References"},{"location":"performance/","text":"","title":"Performance"},{"location":"pre_trained_models/","text":"Pre-trained Models We maintain a list of pre-trained uncompressed models, so that the training process of model compression does not need to start from scratch. For the CIFAR-10 data set, we provide following pre-trained models: Model name Accuracy URL LeNet 81.79% Link ResNet-20 91.93% Link ResNet-32 92.59% Link ResNet-44 92.76% Link ResNet-56 93.23% Link For the ImageNet (ILSVRC-12) data set, we provide following pre-trained models: Model name Top-1 Acc. Top-5 Acc. URL ResNet-18 70.28% 89.38% Link ResNet-34 73.41% 91.27% Link ResNet-50 75.97% 92.88% Link MobileNet-v1 70.89% 89.56% Link MobileNet-v2 71.84% 90.60% Link","title":"Pre-trained Models"},{"location":"pre_trained_models/#pre-trained-models","text":"We maintain a list of pre-trained uncompressed models, so that the training process of model compression does not need to start from scratch. For the CIFAR-10 data set, we provide following pre-trained models: Model name Accuracy URL LeNet 81.79% Link ResNet-20 91.93% Link ResNet-32 92.59% Link ResNet-44 92.76% Link ResNet-56 93.23% Link For the ImageNet (ILSVRC-12) data set, we provide following pre-trained models: Model name Top-1 Acc. Top-5 Acc. URL ResNet-18 70.28% 89.38% Link ResNet-34 73.41% 91.27% Link ResNet-50 75.97% 92.88% Link MobileNet-v1 70.89% 89.56% Link MobileNet-v2 71.84% 90.60% Link","title":"Pre-trained Models"},{"location":"quick_start/","text":"Quick Start In this tutorial, we will demonstrate how to compress a convolutional neural network and export the compressed model into a *.tflite file for deployment on mobile devices. The model we used here is a 18-layer residual network (denoted as \"ResNet-18\") trained for the ImageNet classification task. We will compress it with the discrimination-aware channel pruning algorithm (Zhuang et al., NIPS '18) to reduce the number of convolutional channels used in the network for speed-up. Prepare the Data To start with, we need to convert the ImageNet data set (ILSVRC-12) into TensorFlow's native TFRecord file format. You may follow the data preparation guide here to download the full data set and convert it into TFRecord files. After that, you should be able to find 4,096 training files and 128 validation files in the data directory, like this: # training files train-00000-of-04096 train-00001-of-04096 ... train-04095-of-04096 # validation files validation-00000-of-00128 validation-00001-of-00128 ... validation-00127-of-00128 Prepare the Pre-trained Model The discrimination-aware channel pruning algorithm requires a pre-trained uncompressed model provided in advance, so that a channel-pruned model can be trained with warm-start. You can download a pre-trained model from here , and then unzip files into the models sub-directory. Alternatively, you can train an uncompressed full-precision model from scratch using FullPrecLearner with the following command (choose whatever mode that fits you): # local mode with 1 GPU $ ./scripts/run_local.sh nets/resnet_at_ilsvrc12_run.py \\ --learner full-prec \\ --data_disk local \\ --data_dir_local ${DATA_DIR_LOCAL} \\ --hdfs_url ${HDFS_URL} # docker mode with 8 GPUs $ ./scripts/run_docker.sh nets/resnet_at_ilsvrc12_run.py ${DATA_DIR_LOCAL} -n=8 \\ --learner full-prec \\ --data_disk local \\ --data_dir_local /opt/ml/data \\ --hdfs_url ${HDFS_URL} # seven mode with 8 GPUs $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py -n=8 \\ --learner full-prec \\ --data_disk seven \\ --data_dir_seven ${DATA_DIR_SEVEN} \\ --hdfs_url ${HDFS_URL} After the training process, you should be able to find the resulting model files located at the models sub-directory in PocketFlow's home directory. Train the Compressed Model Now, we can train a compressed model with the discrimination-aware channel pruning algorithm, as implemented by DisChnPrunedLearner . Assuming you are now in PocketFlow's home directory, the training process of model compression can be started using the following command (choose whatever mode that fits you): # local mode with 1 GPU $ ./scripts/run_local.sh nets/resnet_at_ilsvrc12_run.py \\ --learner dis-chn-pruned \\ --data_disk local \\ --data_dir_local ${DATA_DIR_LOCAL} \\ --hdfs_url ${HDFS_URL} # docker mode with 8 GPUs $ ./scripts/run_docker.sh nets/resnet_at_ilsvrc12_run.py ${DATA_DIR_LOCAL} -n=8 \\ --learner dis-chn-pruned \\ --data_disk local \\ --data_dir_local /opt/ml/data \\ --hdfs_url ${HDFS_URL} # seven mode with 8 GPUs $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py -n=8 \\ --learner dis-chn-pruned \\ --data_disk seven \\ --data_dir_seven ${DATA_DIR_SEVEN} \\ --hdfs_url ${HDFS_URL} Let's take the execution command for the local mode as an example. In this command, run_local.sh is a shell script that executes the specified Python script with user-provided arguments. Here, we ask it to run the Python script named nets/resnet_at_ilsvrc12_run.py , which is the execution script for ResNet models on the ImageNet data set. After that, we use --learner dis-chn-pruned to specify that the DisChnPrunedLearner should be used for model compression. You may also use other learners by specifying the corresponding learner name. Below is a full list of available learners in PocketFlow: Learner name Learner class Note full-prec FullPrecLearner No model compression channel ChannelPruningLearner Channel pruning [2] dis-chn-pruned DisChnPrunedLearner Discrimination-aware channel pruning [1] weight-sparse WeightSparseLearner Weight sparsification [3] uniform UniformQuantizedLearner Uniform weight quantization [] tf-uniform UniformQuantizedLearnerTF Uniform weight quantization in TensorFlow [] non-uniform NonUniformQuantizedLearner Non-uniform weight quantization [] The local mode only uses 1 GPU for the training process, which takes approximately 20-30 hours to complete. This can be accelerated by multi-GPU training in the docker and seven mode, which is enabled by adding -n=x right after the specified Python script, where x is the number of GPUs to be used. Optionally, you can pass some extra arguments to customize the training process. For the discrimination-aware channel pruning algorithm, some of key arguments are: Name Definition Default Value enbl_dst Enable training with distillation loss True dcp_prune_ratio DCP algorithm's pruning ratio 0.5 You may override the default value by appending customized arguments at the end of the execution command. For instance, the following command: $ ./scripts/run_local.sh nets/resnet_at_ilsvrc12_run.py \\ --learner dis-chn-pruned \\ --data_disk local \\ --data_dir_local ${DATA_DIR_LOCAL} \\ --hdfs_url ${HDFS_URL} \\ --noenbl_dst \\ --dcp_prune_ratio 0.75 requires the DisChnPrunedLearner to achieve an overall pruning ratio of 0.75 and the training process will be carried out without the distillation loss. As a result, the number of channels in each convolutional layer of the compressed model will be one quarter of the original one. After the training process is completed, you should be able to find a sub-directory named models_dcp_eval created in the home directory of PocketFlow. This sub-directory contains all the files that define the compressed model, and we will export them to a TensorFlow Lite formatted model file for deployment in the next section. Export to TensorFlow Lite TensorFlow's checkpoint files cannot be directly used for deployment on mobile devices. Instead, we need to firstly convert them into a single *.tflite file that is supported by the TensorFlow Lite Interpreter. For model compressed with channel-pruning based algorithms, e.g. ChannelPruningLearner and DisChnPrunedLearner , we have prepared a model conversion script, tools/conversion/export_pb_tflite_models.py , to generate a TF-Lite model from TensorFlow's checkpoint files. To convert checkpoint files into a *.tflite file, use the following command: # convert checkpoint files into a *.tflite model $ python tools/conversion/export_pb_tflite_models.py \\ --model_dir models_dcp_eval In the above command, we specify the model directory containing checkpoint files generated in the previous training process. The conversion script automatically detects which channels can be safely pruned, and then produces a light-weighted compressed model. The resulting TensorFlow Lite file is also placed at the models_dcp_eval directory, named as model_transformed.tflite . Deploy on Mobile Devices After exporting the compressed model to the TensorFlow Lite file format, you may follow the official guide for creating an Android demo App from it. Basically, this demo App uses a TensorFlow Lite model to continuously classifies images captured by the camera, and all the computation are performed on mobile devices in real time. To use the model_transformed.tflite model file, you need to place it in the asserts directory and create a Java class named ImageClassifierFloatResNet to use this model for classification. Below is the example code, which is modified from ImageClassifierFloatInception.java used in the official demo project: /* Copyright 2017 The TensorFlow Authors. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the License ); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an AS IS BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. ==============================================================================*/ package com.example.android.tflitecamerademo; import android.app.Activity; import java.io.IOException; /** * This classifier works with the ResNet-18 model. * It applies floating point inference rather than using a quantized model. */ public class ImageClassifierFloatResNet extends ImageClassifier { /** * The ResNet requires additional normalization of the used input. */ private static final float IMAGE_MEAN_RED = 123.58f; private static final float IMAGE_MEAN_GREEN = 116.779f; private static final float IMAGE_MEAN_BLUE = 103.939f; /** * An array to hold inference results, to be feed into Tensorflow Lite as outputs. * This isn't part of the super class, because we need a primitive array here. */ private float[][] labelProbArray = null; /** * Initializes an {@code ImageClassifier}. * * @param activity */ ImageClassifierFloatResNet(Activity activity) throws IOException { super(activity); labelProbArray = new float[1][getNumLabels()]; } @Override protected String getModelPath() { return model_transformed.tflite ; } @Override protected String getLabelPath() { return labels_imagenet_slim.txt ; } @Override protected int getImageSizeX() { return 224; } @Override protected int getImageSizeY() { return 224; } @Override protected int getNumBytesPerChannel() { // a 32bit float value requires 4 bytes return 4; } @Override protected void addPixelValue(int pixelValue) { imgData.putFloat(((pixelValue 16) 0xFF) - IMAGE_MEAN_RED); imgData.putFloat(((pixelValue 8) 0xFF) - IMAGE_MEAN_GREEN); imgData.putFloat((pixelValue 0xFF) - IMAGE_MEAN_BLUE); } @Override protected float getProbability(int labelIndex) { return labelProbArray[0][labelIndex]; } @Override protected void setProbability(int labelIndex, Number value) { labelProbArray[0][labelIndex] = value.floatValue(); } @Override protected float getNormalizedProbability(int labelIndex) { // TODO the following value isn't in [0,1] yet, but may be greater. Why? return getProbability(labelIndex); } @Override protected void runInference() { tflite.run(imgData, labelProbArray); } } After that, you need to change the image classifier class used in Camera2BasicFragment.java . Locate the function named onActivityCreated and change its content as below. Now you will be able to use the compressed ResNet-18 model to classify objects on your mobile phone in real time. /** Load the model and labels. */ @Override public void onActivityCreated(Bundle savedInstanceState) { super.onActivityCreated(savedInstanceState); try { classifier = new ImageClassifierFloatResNet(getActivity()); } catch (IOException e) { Log.e(TAG, Failed to initialize an image classifier. , e); } startBackgroundThread(); } Reference [1] Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Jiezhang Cao, Qingyao Wu, Junzhou Huang, Jinhui Zhu, Discrimination-aware Channel Pruning for Deep Neural Networks , In Proc. of the Annual Conference on Neural Information Processing Systems (NIPS), 2018. [2] Yihui He, Xiangyu Zhang, Jian Sun, Channel Pruning for Accelerating Very Deep Neural Networks , In Proc. of the IEEE International Conference on Computer Vision (ICCV), 2017. [3] Michael Zhu, Suyog Gupta, To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression , CoRR, abs/1710.01878, 2017.","title":"Quick Start"},{"location":"quick_start/#quick-start","text":"In this tutorial, we will demonstrate how to compress a convolutional neural network and export the compressed model into a *.tflite file for deployment on mobile devices. The model we used here is a 18-layer residual network (denoted as \"ResNet-18\") trained for the ImageNet classification task. We will compress it with the discrimination-aware channel pruning algorithm (Zhuang et al., NIPS '18) to reduce the number of convolutional channels used in the network for speed-up.","title":"Quick Start"},{"location":"quick_start/#prepare-the-data","text":"To start with, we need to convert the ImageNet data set (ILSVRC-12) into TensorFlow's native TFRecord file format. You may follow the data preparation guide here to download the full data set and convert it into TFRecord files. After that, you should be able to find 4,096 training files and 128 validation files in the data directory, like this: # training files train-00000-of-04096 train-00001-of-04096 ... train-04095-of-04096 # validation files validation-00000-of-00128 validation-00001-of-00128 ... validation-00127-of-00128","title":"Prepare the Data"},{"location":"quick_start/#prepare-the-pre-trained-model","text":"The discrimination-aware channel pruning algorithm requires a pre-trained uncompressed model provided in advance, so that a channel-pruned model can be trained with warm-start. You can download a pre-trained model from here , and then unzip files into the models sub-directory. Alternatively, you can train an uncompressed full-precision model from scratch using FullPrecLearner with the following command (choose whatever mode that fits you): # local mode with 1 GPU $ ./scripts/run_local.sh nets/resnet_at_ilsvrc12_run.py \\ --learner full-prec \\ --data_disk local \\ --data_dir_local ${DATA_DIR_LOCAL} \\ --hdfs_url ${HDFS_URL} # docker mode with 8 GPUs $ ./scripts/run_docker.sh nets/resnet_at_ilsvrc12_run.py ${DATA_DIR_LOCAL} -n=8 \\ --learner full-prec \\ --data_disk local \\ --data_dir_local /opt/ml/data \\ --hdfs_url ${HDFS_URL} # seven mode with 8 GPUs $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py -n=8 \\ --learner full-prec \\ --data_disk seven \\ --data_dir_seven ${DATA_DIR_SEVEN} \\ --hdfs_url ${HDFS_URL} After the training process, you should be able to find the resulting model files located at the models sub-directory in PocketFlow's home directory.","title":"Prepare the Pre-trained Model"},{"location":"quick_start/#train-the-compressed-model","text":"Now, we can train a compressed model with the discrimination-aware channel pruning algorithm, as implemented by DisChnPrunedLearner . Assuming you are now in PocketFlow's home directory, the training process of model compression can be started using the following command (choose whatever mode that fits you): # local mode with 1 GPU $ ./scripts/run_local.sh nets/resnet_at_ilsvrc12_run.py \\ --learner dis-chn-pruned \\ --data_disk local \\ --data_dir_local ${DATA_DIR_LOCAL} \\ --hdfs_url ${HDFS_URL} # docker mode with 8 GPUs $ ./scripts/run_docker.sh nets/resnet_at_ilsvrc12_run.py ${DATA_DIR_LOCAL} -n=8 \\ --learner dis-chn-pruned \\ --data_disk local \\ --data_dir_local /opt/ml/data \\ --hdfs_url ${HDFS_URL} # seven mode with 8 GPUs $ ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py -n=8 \\ --learner dis-chn-pruned \\ --data_disk seven \\ --data_dir_seven ${DATA_DIR_SEVEN} \\ --hdfs_url ${HDFS_URL} Let's take the execution command for the local mode as an example. In this command, run_local.sh is a shell script that executes the specified Python script with user-provided arguments. Here, we ask it to run the Python script named nets/resnet_at_ilsvrc12_run.py , which is the execution script for ResNet models on the ImageNet data set. After that, we use --learner dis-chn-pruned to specify that the DisChnPrunedLearner should be used for model compression. You may also use other learners by specifying the corresponding learner name. Below is a full list of available learners in PocketFlow: Learner name Learner class Note full-prec FullPrecLearner No model compression channel ChannelPruningLearner Channel pruning [2] dis-chn-pruned DisChnPrunedLearner Discrimination-aware channel pruning [1] weight-sparse WeightSparseLearner Weight sparsification [3] uniform UniformQuantizedLearner Uniform weight quantization [] tf-uniform UniformQuantizedLearnerTF Uniform weight quantization in TensorFlow [] non-uniform NonUniformQuantizedLearner Non-uniform weight quantization [] The local mode only uses 1 GPU for the training process, which takes approximately 20-30 hours to complete. This can be accelerated by multi-GPU training in the docker and seven mode, which is enabled by adding -n=x right after the specified Python script, where x is the number of GPUs to be used. Optionally, you can pass some extra arguments to customize the training process. For the discrimination-aware channel pruning algorithm, some of key arguments are: Name Definition Default Value enbl_dst Enable training with distillation loss True dcp_prune_ratio DCP algorithm's pruning ratio 0.5 You may override the default value by appending customized arguments at the end of the execution command. For instance, the following command: $ ./scripts/run_local.sh nets/resnet_at_ilsvrc12_run.py \\ --learner dis-chn-pruned \\ --data_disk local \\ --data_dir_local ${DATA_DIR_LOCAL} \\ --hdfs_url ${HDFS_URL} \\ --noenbl_dst \\ --dcp_prune_ratio 0.75 requires the DisChnPrunedLearner to achieve an overall pruning ratio of 0.75 and the training process will be carried out without the distillation loss. As a result, the number of channels in each convolutional layer of the compressed model will be one quarter of the original one. After the training process is completed, you should be able to find a sub-directory named models_dcp_eval created in the home directory of PocketFlow. This sub-directory contains all the files that define the compressed model, and we will export them to a TensorFlow Lite formatted model file for deployment in the next section.","title":"Train the Compressed Model"},{"location":"quick_start/#export-to-tensorflow-lite","text":"TensorFlow's checkpoint files cannot be directly used for deployment on mobile devices. Instead, we need to firstly convert them into a single *.tflite file that is supported by the TensorFlow Lite Interpreter. For model compressed with channel-pruning based algorithms, e.g. ChannelPruningLearner and DisChnPrunedLearner , we have prepared a model conversion script, tools/conversion/export_pb_tflite_models.py , to generate a TF-Lite model from TensorFlow's checkpoint files. To convert checkpoint files into a *.tflite file, use the following command: # convert checkpoint files into a *.tflite model $ python tools/conversion/export_pb_tflite_models.py \\ --model_dir models_dcp_eval In the above command, we specify the model directory containing checkpoint files generated in the previous training process. The conversion script automatically detects which channels can be safely pruned, and then produces a light-weighted compressed model. The resulting TensorFlow Lite file is also placed at the models_dcp_eval directory, named as model_transformed.tflite .","title":"Export to TensorFlow Lite"},{"location":"quick_start/#deploy-on-mobile-devices","text":"After exporting the compressed model to the TensorFlow Lite file format, you may follow the official guide for creating an Android demo App from it. Basically, this demo App uses a TensorFlow Lite model to continuously classifies images captured by the camera, and all the computation are performed on mobile devices in real time. To use the model_transformed.tflite model file, you need to place it in the asserts directory and create a Java class named ImageClassifierFloatResNet to use this model for classification. Below is the example code, which is modified from ImageClassifierFloatInception.java used in the official demo project: /* Copyright 2017 The TensorFlow Authors. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the License ); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an AS IS BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. ==============================================================================*/ package com.example.android.tflitecamerademo; import android.app.Activity; import java.io.IOException; /** * This classifier works with the ResNet-18 model. * It applies floating point inference rather than using a quantized model. */ public class ImageClassifierFloatResNet extends ImageClassifier { /** * The ResNet requires additional normalization of the used input. */ private static final float IMAGE_MEAN_RED = 123.58f; private static final float IMAGE_MEAN_GREEN = 116.779f; private static final float IMAGE_MEAN_BLUE = 103.939f; /** * An array to hold inference results, to be feed into Tensorflow Lite as outputs. * This isn't part of the super class, because we need a primitive array here. */ private float[][] labelProbArray = null; /** * Initializes an {@code ImageClassifier}. * * @param activity */ ImageClassifierFloatResNet(Activity activity) throws IOException { super(activity); labelProbArray = new float[1][getNumLabels()]; } @Override protected String getModelPath() { return model_transformed.tflite ; } @Override protected String getLabelPath() { return labels_imagenet_slim.txt ; } @Override protected int getImageSizeX() { return 224; } @Override protected int getImageSizeY() { return 224; } @Override protected int getNumBytesPerChannel() { // a 32bit float value requires 4 bytes return 4; } @Override protected void addPixelValue(int pixelValue) { imgData.putFloat(((pixelValue 16) 0xFF) - IMAGE_MEAN_RED); imgData.putFloat(((pixelValue 8) 0xFF) - IMAGE_MEAN_GREEN); imgData.putFloat((pixelValue 0xFF) - IMAGE_MEAN_BLUE); } @Override protected float getProbability(int labelIndex) { return labelProbArray[0][labelIndex]; } @Override protected void setProbability(int labelIndex, Number value) { labelProbArray[0][labelIndex] = value.floatValue(); } @Override protected float getNormalizedProbability(int labelIndex) { // TODO the following value isn't in [0,1] yet, but may be greater. Why? return getProbability(labelIndex); } @Override protected void runInference() { tflite.run(imgData, labelProbArray); } } After that, you need to change the image classifier class used in Camera2BasicFragment.java . Locate the function named onActivityCreated and change its content as below. Now you will be able to use the compressed ResNet-18 model to classify objects on your mobile phone in real time. /** Load the model and labels. */ @Override public void onActivityCreated(Bundle savedInstanceState) { super.onActivityCreated(savedInstanceState); try { classifier = new ImageClassifierFloatResNet(getActivity()); } catch (IOException e) { Log.e(TAG, Failed to initialize an image classifier. , e); } startBackgroundThread(); }","title":"Deploy on Mobile Devices"},{"location":"quick_start/#reference","text":"[1] Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Jiezhang Cao, Qingyao Wu, Junzhou Huang, Jinhui Zhu, Discrimination-aware Channel Pruning for Deep Neural Networks , In Proc. of the Annual Conference on Neural Information Processing Systems (NIPS), 2018. [2] Yihui He, Xiangyu Zhang, Jian Sun, Channel Pruning for Accelerating Very Deep Neural Networks , In Proc. of the IEEE International Conference on Computer Vision (ICCV), 2017. [3] Michael Zhu, Suyog Gupta, To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression , CoRR, abs/1710.01878, 2017.","title":"Reference"},{"location":"reinforcement_learning/","text":"","title":"Reinforcement Learning"},{"location":"self_defined_models/","text":"Self-defined Models Under construction ...","title":"Self-defined Models"},{"location":"self_defined_models/#self-defined-models","text":"Under construction ...","title":"Self-defined Models"},{"location":"test_cases/","text":"Test Cases This document contains various test cases to cover different combinations of learners and hyper-parameter settings. Any merge request to the master branch should be able to pass all the test cases to be approved. FP: Full-Precision # local machine + local data ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner full-prec \\ --data_disk local \\ --data_dir_local ${PF_CIFAR10_LOCAL} \\ --hdfs_url ${PF_HDFS_URL} # local machine + HDFS data ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner full-prec \\ --data_disk hdfs \\ --hdfs_host ${PF_HDFS_HOST} \\ --hdfs_url ${PF_HDFS_URL} # SEVEN cluster + SEVEN-disk data ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py \\ --learner full-prec \\ --data_disk seven \\ --hdfs_url ${PF_HDFS_URL} # SEVEN cluster + HDFS data ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py \\ --learner full-prec \\ --data_disk hdfs \\ --hdfs_host ${PF_HDFS_HOST} \\ --hdfs_url ${PF_HDFS_URL} CP: Channel Pruning # uniform preserve ratios for all layers ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --learner channel \\ --prune_option uniform \\ --uniform_preserve_ratio 0.5 \\ --hdfs_url ${PF_HDFS_URL} # auto-tuned preserve ratios for each layer ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --learner channel \\ --prune_option auto \\ --preserve_ratio 0.3 \\ --hdfs_url ${PF_HDFS_URL} DCP: Discrimination-aware Channel Pruning # network distillation ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner dis-chn-pruned \\ --dcp_nb_stages 3 \\ --data_disk hdfs \\ --hdfs_host ${PF_HDFS_HOST} \\ --hdfs_url ${PF_HDFS_URL} # no network distillation ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py \\ --learner dis-chn-pruned \\ --noenbl_dst \\ --dcp_nb_stages 4 \\ --hdfs_url ${PF_HDFS_URL} WS: Weight Sparsification # uniform pruning ratios for all layers ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner weight-sparse \\ --ws_prune_ratio_prtl uniform \\ --data_disk hdfs \\ --hdfs_host ${PF_HDFS_HOST} \\ --hdfs_url ${PF_HDFS_URL} # optimal pruning ratios for each layer ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner weight-sparse \\ --ws_prune_ratio_prtl optimal \\ --data_disk hdfs \\ --hdfs_host ${PF_HDFS_HOST} \\ --hdfs_url ${PF_HDFS_URL} # heurist pruning ratios for each layer ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py \\ --learner weight-sparse \\ --ws_prune_ratio_prtl heurist \\ --hdfs_url ${PF_HDFS_URL} # optimal pruning ratios for each layer ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py \\ --learner weight-sparse \\ --ws_prune_ratio_prtl optimal \\ --hdfs_url ${PF_HDFS_URL} UQ: Uniform Quantization # channel-based bucketing ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner uniform \\ --uql_use_buckets \\ --uql_bucket_type channel \\ --data_disk hdfs \\ --hdfs_host ${PF_HDFS_HOST} \\ --hdfs_url ${PF_HDFS_URL} # split-based bucketing ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner uniform \\ --uql_use_buckets \\ --uql_bucket_type split \\ --data_disk hdfs \\ --hdfs_host ${PF_HDFS_HOST} \\ --hdfs_url ${PF_HDFS_URL} # channel-based bucketing + RL ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py -n=2 \\ --learner uniform \\ --uql_enbl_rl_agent \\ --uql_use_buckets \\ --uql_bucket_type channel \\ --hdfs_url ${PF_HDFS_URL} # split-based bucketing + RL ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py -n=2 \\ --learner uniform \\ --uql_enbl_rl_agent \\ --uql_use_buckets \\ --uql_bucket_type split \\ --hdfs_url ${PF_HDFS_URL} NUQ: Non-uniform Quantization # channel-based bucketing + RL + optimize clusters ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner non-uniform \\ --nuql_enbl_rl_agent \\ --nuql_use_buckets \\ --nuql_bucket_type channel \\ --nuql_opt_mode clusters \\ --data_disk hdfs \\ --hdfs_host ${PF_HDFS_HOST} \\ --hdfs_url ${PF_HDFS_URL} # split-based bucketing + RL + optimize weights ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner non-uniform \\ --nuql_enbl_rl_agent \\ --nuql_use_buckets \\ --nuql_bucket_type split \\ --nuql_opt_mode weights \\ --data_disk hdfs \\ --hdfs_host ${PF_HDFS_HOST} \\ --hdfs_url ${PF_HDFS_URL} # channel-based bucketing + RL + optimize weights ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py -n=2 \\ --learner non-uniform \\ --nuql_enbl_rl_agent \\ --nuql_use_buckets \\ --nuql_bucket_type channel \\ --nuql_opt_mode weights \\ --hdfs_url ${PF_HDFS_URL} # split-based bucketing + RL + optimize clusters ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py -n=2 \\ --learner non-uniform \\ --nuql_enbl_rl_agent \\ --nuql_use_buckets \\ --nuql_bucket_type split \\ --nuql_opt_mode clusters \\ --hdfs_url ${PF_HDFS_URL}","title":"Test Cases"},{"location":"test_cases/#test-cases","text":"This document contains various test cases to cover different combinations of learners and hyper-parameter settings. Any merge request to the master branch should be able to pass all the test cases to be approved.","title":"Test Cases"},{"location":"test_cases/#fp-full-precision","text":"# local machine + local data ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner full-prec \\ --data_disk local \\ --data_dir_local ${PF_CIFAR10_LOCAL} \\ --hdfs_url ${PF_HDFS_URL} # local machine + HDFS data ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner full-prec \\ --data_disk hdfs \\ --hdfs_host ${PF_HDFS_HOST} \\ --hdfs_url ${PF_HDFS_URL} # SEVEN cluster + SEVEN-disk data ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py \\ --learner full-prec \\ --data_disk seven \\ --hdfs_url ${PF_HDFS_URL} # SEVEN cluster + HDFS data ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py \\ --learner full-prec \\ --data_disk hdfs \\ --hdfs_host ${PF_HDFS_HOST} \\ --hdfs_url ${PF_HDFS_URL}","title":"FP: Full-Precision"},{"location":"test_cases/#cp-channel-pruning","text":"# uniform preserve ratios for all layers ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --learner channel \\ --prune_option uniform \\ --uniform_preserve_ratio 0.5 \\ --hdfs_url ${PF_HDFS_URL} # auto-tuned preserve ratios for each layer ./scripts/run_seven.sh nets/resnet_at_cifar10_run.py \\ --learner channel \\ --prune_option auto \\ --preserve_ratio 0.3 \\ --hdfs_url ${PF_HDFS_URL}","title":"CP: Channel Pruning"},{"location":"test_cases/#dcp-discrimination-aware-channel-pruning","text":"# network distillation ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner dis-chn-pruned \\ --dcp_nb_stages 3 \\ --data_disk hdfs \\ --hdfs_host ${PF_HDFS_HOST} \\ --hdfs_url ${PF_HDFS_URL} # no network distillation ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py \\ --learner dis-chn-pruned \\ --noenbl_dst \\ --dcp_nb_stages 4 \\ --hdfs_url ${PF_HDFS_URL}","title":"DCP: Discrimination-aware Channel Pruning"},{"location":"test_cases/#ws-weight-sparsification","text":"# uniform pruning ratios for all layers ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner weight-sparse \\ --ws_prune_ratio_prtl uniform \\ --data_disk hdfs \\ --hdfs_host ${PF_HDFS_HOST} \\ --hdfs_url ${PF_HDFS_URL} # optimal pruning ratios for each layer ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner weight-sparse \\ --ws_prune_ratio_prtl optimal \\ --data_disk hdfs \\ --hdfs_host ${PF_HDFS_HOST} \\ --hdfs_url ${PF_HDFS_URL} # heurist pruning ratios for each layer ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py \\ --learner weight-sparse \\ --ws_prune_ratio_prtl heurist \\ --hdfs_url ${PF_HDFS_URL} # optimal pruning ratios for each layer ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py \\ --learner weight-sparse \\ --ws_prune_ratio_prtl optimal \\ --hdfs_url ${PF_HDFS_URL}","title":"WS: Weight Sparsification"},{"location":"test_cases/#uq-uniform-quantization","text":"# channel-based bucketing ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner uniform \\ --uql_use_buckets \\ --uql_bucket_type channel \\ --data_disk hdfs \\ --hdfs_host ${PF_HDFS_HOST} \\ --hdfs_url ${PF_HDFS_URL} # split-based bucketing ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner uniform \\ --uql_use_buckets \\ --uql_bucket_type split \\ --data_disk hdfs \\ --hdfs_host ${PF_HDFS_HOST} \\ --hdfs_url ${PF_HDFS_URL} # channel-based bucketing + RL ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py -n=2 \\ --learner uniform \\ --uql_enbl_rl_agent \\ --uql_use_buckets \\ --uql_bucket_type channel \\ --hdfs_url ${PF_HDFS_URL} # split-based bucketing + RL ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py -n=2 \\ --learner uniform \\ --uql_enbl_rl_agent \\ --uql_use_buckets \\ --uql_bucket_type split \\ --hdfs_url ${PF_HDFS_URL}","title":"UQ: Uniform Quantization"},{"location":"test_cases/#nuq-non-uniform-quantization","text":"# channel-based bucketing + RL + optimize clusters ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner non-uniform \\ --nuql_enbl_rl_agent \\ --nuql_use_buckets \\ --nuql_bucket_type channel \\ --nuql_opt_mode clusters \\ --data_disk hdfs \\ --hdfs_host ${PF_HDFS_HOST} \\ --hdfs_url ${PF_HDFS_URL} # split-based bucketing + RL + optimize weights ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --learner non-uniform \\ --nuql_enbl_rl_agent \\ --nuql_use_buckets \\ --nuql_bucket_type split \\ --nuql_opt_mode weights \\ --data_disk hdfs \\ --hdfs_host ${PF_HDFS_HOST} \\ --hdfs_url ${PF_HDFS_URL} # channel-based bucketing + RL + optimize weights ./scripts/run_seven.sh nets/mobilenet_at_ilsvrc12_run.py -n=2 \\ --learner non-uniform \\ --nuql_enbl_rl_agent \\ --nuql_use_buckets \\ --nuql_bucket_type channel \\ --nuql_opt_mode weights \\ --hdfs_url ${PF_HDFS_URL} # split-based bucketing + RL + optimize clusters ./scripts/run_seven.sh nets/resnet_at_ilsvrc12_run.py -n=2 \\ --learner non-uniform \\ --nuql_enbl_rl_agent \\ --nuql_use_buckets \\ --nuql_bucket_type split \\ --nuql_opt_mode clusters \\ --hdfs_url ${PF_HDFS_URL}","title":"NUQ: Non-uniform Quantization"},{"location":"uq_learner/","text":"Uniform Quantization This document describes how to set up uniform quantization with PocketFlow. Uniform quantization is widely used for model compression and acceleration. Originally the weights in the network are represented by 32-bit float numbers. With uniform quantization, low-precision (e.g., 4 bit, 8 bit) and evenly distributed float numbers are used to approximate the full precision networks. For 8 bit quantization, the network size can be reduced by 4 folds with little drop of performance. Currently PocketFlow supports two types of uniform quantization: Uniform Quantization Learner: the self-developed learner. Aside from uniform quantization, the learner is carefully optimized with various extensions supported. The detailed algorithm of the Uniform Quantized Learner will be introduced at the end of the algorithm. The algorithm details are presented at the end of the document. TensorFlow Quantization Wrapper: a wrapper based on the post training quantization in TensorFlow. The wrapper currently only supports 8-bit quantization, enjoying 4x reduction of memory and nearly 4x times speed up of inference. A comparison of the two learners are shown below: Features Uniform Quantization Learner TensorFlow Quantization Wrapper Compression yes Yes Acceleration Yes Fine-tuning Yes Bucketing Yes Hyper-param Searching Yes Uniform Quantization Learner The uniform quantization learner supports both weight quantization and activation quantization, where users can manually set up the bits for quantization. The uniform quantization learner also supports bucketing, which leads to more fine-grained quantization and better performance. The users can also turn on the hyper parameter optimizer with reinforcement learning to search for the optimal bit allocation for the learner. Prepare the Model To quantize the network, users should first get the model prepared. Users can either use the pre-built models in PocketFlow, or develop their custom models according to TODO . Configure the Learner To configure the learner, users can pass the options via the TensorFlow flag interface. The available options are as follows: Options Default Value Description --uql_weight_bits 4 the number of bits for weight --uql_activation_bits 32 the number of bits for activation\uff0c by default it remains full precision --uql_save_quant_mode_path TODO the save path for quantized models --uql_use_buckets False use bucketing or not --uql_bucket_type channel two bucket type available: ['split', 'channel'] --uql_bucket_size 256 quantize the first and last layers of the network or not --uql_enbl_rl_agent False enable reinforcement learning to learn the optimal bit allocation or not --uql_quantize_all_layers False quantize the first and last layers of the network or not --uql_quant_epoch 60 the number of epochs for fine-tuning Examples Once the model is built, the quantization can be easily triggered by directly passing the Uniform Quantization Learner in the command line as follows: # quantize resnet-20 on CIFAR-10 # you can also configure the sh ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --data_disk local \\ --data_dir_local ${PF_CIFAR10_LOCAL} \\ --learner=uniform \\ --uql_weight_bits=4 \\ --uql_activation_bits=4 \\ # quantize the resnet-18 on ILSVRC-12 sh ./scripts/run_local.sh nets/resnet_at_ilsvrc12_run.py \\ --learner=uniform \\ --data_disk local \\ --data_dir_local ${PF_ILSVRC12_LOCAL} \\ --uql_weight_bits=8 \\ --uql_activation_bits=8 \\ --uql_use_buckets=True \\ --uql_bucket_type=channel Configure the Hyper Parameter Optimizer Once the hyper parameter optimizer is turned on, i.e., uql_enbl_rl_agent==True , the reinforcement learning agents will search for the optimal allocation of bits to each layers. Before the search, users are supposed set up the bit constraints via --uql_evquivalent_bits , so that the optimal bits searched by the RL agent will not exceed the bit number without RL agent. For example, TODO Users can also configure other options in the RL agent, such as the number of roll-outs, the fine-tuning steps to get the reward, e.t.c.. Full list of options are listed as follows: Options Default Value Description --uql_evquivalent_bits 4 the number of re-allocated bits that is equivalent to uniform quantization without RL agent --uql_nb_rlouts 200 the number of roll outs for training the RL agent --uql_w_bit_min 2 the minimal number of bits for each layer --uql_w_bit_max 8 the maximal number of bits for each layer --uql_enbl_rl_global_tune True enable fine-tuning all layers of the network or not --uql_enbl_rl_layerwise_tune False enable fine-tuning the network layer by layer or not --uql_tune_layerwise_steps 100 the number of steps for layerwise fine-tuning --uql_tune_global_steps 2000 the number of steps for global fine-tuning --uql_tune_disp_steps 300 the display steps to show the fine-tuning progress --uql_enbl_random_layers True randomly permute the layers during RL agent training Examples # quantize mobilenet-v1 on ILSVRC-12 sh ./scripts/run_local.sh nets/mobilnet_at_ilsvrc12_run.py \\ --data_disk local \\ --data_dir_local ${PF_CIFAR10_LOCAL} \\ --learner=uniform \\ --uql_enbl_rl_agent=True \\ --uql_equivalent_bits=4 \\ --uql_tune_global_steps=1200 Performance Here we list some of the performance on Cifar-10 using the Uniform Quantization Learner and the built-in models in PocketFlow. The options not displayed remain the default values. Model Weight Bit Activation Bit Acc ResNet-20 32 32 91.96 ResNet-20 4 4 90.73 ResNet-20 8 8 92.25 Model Weight Bit Bucketing Acc ResNet-20 2 channel 89.67 ResNet-20 4 channel 92.02 ResNet-20 2 split 91.15 ResNet-20 4 split 91.98 Model Weight Bit RL search Acc ResNet-20 2 FALSE 86.17 ResNet-20 4 FALSE 91.76 ResNet-20 2 TRUE 90.30 ResNet-20 4 TRUE 91.88 TensorFlow Quantization Wrapper PocketFlow wraps the post training quantization in Tensorflow, and include all the necessary steps to convert the model to the .tflite format, which can be deployed on Andriod devices. To run the wrapper, users only need to get the checkpoint files ready, and then run the script. Prepare the Checkpoint Files Generally in TensorFlow, the checkpoints of a model include three files: .data, .index, .meta. Users are also supposed to add the input and output to collections, and configure the wrapper to acquire the corresponding collections. An example is as follows: TODO add quantization to the conversion tools # load the checkpoints in ./models, and read the collections of 'inputs' and 'outputs' python export_pb_tflite_models.py \\ --model_dir ./models --input_coll inputs --output_coll outputs --quantize True If successfully transformed, the .pb and .tflite files will be saved in ./models . Deploy on Mobile Devices TODO Algorithms Now we introduce the detailed algorithm in the Uniform Quantization Learner. As is shown in the following graph, given a full precision model, the Uniform Quantization Learner inserts quantization nodes into the computation graph of the model. To enable activation quantization, the quantization nodes shall also be inserted after the activation function. In the training phase, both full-precision and quantized weights are stored. During the forward pass, quantized weights are obtained by applying the quantization functions on the full precision weights. For the backward propagation of gradients, since the gradients w.r.t. the quantized weights are 0 almost everywhere, we use the straight-through estimator (STE) ( Hinton et.al 2012 , Bengio et.al 2013 ) to pass the gradient of quantized weights directly to the full precision weights for update. Uniform Quantization Function Uniform quantization distributed the quantization points evenly across the distribution of weights, and the full precision numbers are then assigned to the closest quantization point. To achieve this, we first normalize the full precision weights x x of one layer to [0, 1] [0, 1] , i.e., $$ sc(x) = \\frac{x-\\beta}{\\alpha}, $$ where \\alpha=\\max{x}-\\min{x} \\alpha=\\max{x}-\\min{x} and \\beta = \\min{x} \\beta = \\min{x} are the scaling factors. Then we assign sc(x) sc(x) to the discrete value by $$ \\hat{x}=\\frac{1}{2^k-1}\\mathrm{round}((2^k-1)\\cdot sc(x)), $$ and finally we do the inverse linear transformation to recover the quantized weights to the original scale, $$ Q(x)=\\alpha\\hat{x}+\\beta. $$ References Bengio Y, L\u00e9onard N, Courville A. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013 Geoffrey Hinton, Nitish Srivastava, Kevin Swersky, Tijmen Tieleman and Abdelrahman Mohamed. Neural Networks for Machine Learning. Coursera, video lectures, 2012","title":"Uniform Quantization"},{"location":"uq_learner/#uniform-quantization","text":"This document describes how to set up uniform quantization with PocketFlow. Uniform quantization is widely used for model compression and acceleration. Originally the weights in the network are represented by 32-bit float numbers. With uniform quantization, low-precision (e.g., 4 bit, 8 bit) and evenly distributed float numbers are used to approximate the full precision networks. For 8 bit quantization, the network size can be reduced by 4 folds with little drop of performance. Currently PocketFlow supports two types of uniform quantization: Uniform Quantization Learner: the self-developed learner. Aside from uniform quantization, the learner is carefully optimized with various extensions supported. The detailed algorithm of the Uniform Quantized Learner will be introduced at the end of the algorithm. The algorithm details are presented at the end of the document. TensorFlow Quantization Wrapper: a wrapper based on the post training quantization in TensorFlow. The wrapper currently only supports 8-bit quantization, enjoying 4x reduction of memory and nearly 4x times speed up of inference. A comparison of the two learners are shown below: Features Uniform Quantization Learner TensorFlow Quantization Wrapper Compression yes Yes Acceleration Yes Fine-tuning Yes Bucketing Yes Hyper-param Searching Yes","title":"Uniform Quantization"},{"location":"uq_learner/#uniform-quantization-learner","text":"The uniform quantization learner supports both weight quantization and activation quantization, where users can manually set up the bits for quantization. The uniform quantization learner also supports bucketing, which leads to more fine-grained quantization and better performance. The users can also turn on the hyper parameter optimizer with reinforcement learning to search for the optimal bit allocation for the learner.","title":"Uniform Quantization Learner"},{"location":"uq_learner/#prepare-the-model","text":"To quantize the network, users should first get the model prepared. Users can either use the pre-built models in PocketFlow, or develop their custom models according to TODO .","title":"Prepare the Model"},{"location":"uq_learner/#configure-the-learner","text":"To configure the learner, users can pass the options via the TensorFlow flag interface. The available options are as follows: Options Default Value Description --uql_weight_bits 4 the number of bits for weight --uql_activation_bits 32 the number of bits for activation\uff0c by default it remains full precision --uql_save_quant_mode_path TODO the save path for quantized models --uql_use_buckets False use bucketing or not --uql_bucket_type channel two bucket type available: ['split', 'channel'] --uql_bucket_size 256 quantize the first and last layers of the network or not --uql_enbl_rl_agent False enable reinforcement learning to learn the optimal bit allocation or not --uql_quantize_all_layers False quantize the first and last layers of the network or not --uql_quant_epoch 60 the number of epochs for fine-tuning","title":"Configure the Learner"},{"location":"uq_learner/#examples","text":"Once the model is built, the quantization can be easily triggered by directly passing the Uniform Quantization Learner in the command line as follows: # quantize resnet-20 on CIFAR-10 # you can also configure the sh ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \\ --data_disk local \\ --data_dir_local ${PF_CIFAR10_LOCAL} \\ --learner=uniform \\ --uql_weight_bits=4 \\ --uql_activation_bits=4 \\ # quantize the resnet-18 on ILSVRC-12 sh ./scripts/run_local.sh nets/resnet_at_ilsvrc12_run.py \\ --learner=uniform \\ --data_disk local \\ --data_dir_local ${PF_ILSVRC12_LOCAL} \\ --uql_weight_bits=8 \\ --uql_activation_bits=8 \\ --uql_use_buckets=True \\ --uql_bucket_type=channel","title":"Examples"},{"location":"uq_learner/#configure-the-hyper-parameter-optimizer","text":"Once the hyper parameter optimizer is turned on, i.e., uql_enbl_rl_agent==True , the reinforcement learning agents will search for the optimal allocation of bits to each layers. Before the search, users are supposed set up the bit constraints via --uql_evquivalent_bits , so that the optimal bits searched by the RL agent will not exceed the bit number without RL agent. For example, TODO Users can also configure other options in the RL agent, such as the number of roll-outs, the fine-tuning steps to get the reward, e.t.c.. Full list of options are listed as follows: Options Default Value Description --uql_evquivalent_bits 4 the number of re-allocated bits that is equivalent to uniform quantization without RL agent --uql_nb_rlouts 200 the number of roll outs for training the RL agent --uql_w_bit_min 2 the minimal number of bits for each layer --uql_w_bit_max 8 the maximal number of bits for each layer --uql_enbl_rl_global_tune True enable fine-tuning all layers of the network or not --uql_enbl_rl_layerwise_tune False enable fine-tuning the network layer by layer or not --uql_tune_layerwise_steps 100 the number of steps for layerwise fine-tuning --uql_tune_global_steps 2000 the number of steps for global fine-tuning --uql_tune_disp_steps 300 the display steps to show the fine-tuning progress --uql_enbl_random_layers True randomly permute the layers during RL agent training","title":"Configure the Hyper Parameter Optimizer"},{"location":"uq_learner/#examples_1","text":"# quantize mobilenet-v1 on ILSVRC-12 sh ./scripts/run_local.sh nets/mobilnet_at_ilsvrc12_run.py \\ --data_disk local \\ --data_dir_local ${PF_CIFAR10_LOCAL} \\ --learner=uniform \\ --uql_enbl_rl_agent=True \\ --uql_equivalent_bits=4 \\ --uql_tune_global_steps=1200","title":"Examples"},{"location":"uq_learner/#performance","text":"Here we list some of the performance on Cifar-10 using the Uniform Quantization Learner and the built-in models in PocketFlow. The options not displayed remain the default values. Model Weight Bit Activation Bit Acc ResNet-20 32 32 91.96 ResNet-20 4 4 90.73 ResNet-20 8 8 92.25 Model Weight Bit Bucketing Acc ResNet-20 2 channel 89.67 ResNet-20 4 channel 92.02 ResNet-20 2 split 91.15 ResNet-20 4 split 91.98 Model Weight Bit RL search Acc ResNet-20 2 FALSE 86.17 ResNet-20 4 FALSE 91.76 ResNet-20 2 TRUE 90.30 ResNet-20 4 TRUE 91.88","title":"Performance"},{"location":"uq_learner/#tensorflow-quantization-wrapper","text":"PocketFlow wraps the post training quantization in Tensorflow, and include all the necessary steps to convert the model to the .tflite format, which can be deployed on Andriod devices. To run the wrapper, users only need to get the checkpoint files ready, and then run the script.","title":"TensorFlow Quantization Wrapper"},{"location":"uq_learner/#prepare-the-checkpoint-files","text":"Generally in TensorFlow, the checkpoints of a model include three files: .data, .index, .meta. Users are also supposed to add the input and output to collections, and configure the wrapper to acquire the corresponding collections. An example is as follows: TODO add quantization to the conversion tools # load the checkpoints in ./models, and read the collections of 'inputs' and 'outputs' python export_pb_tflite_models.py \\ --model_dir ./models --input_coll inputs --output_coll outputs --quantize True If successfully transformed, the .pb and .tflite files will be saved in ./models .","title":"Prepare the Checkpoint Files"},{"location":"uq_learner/#deploy-on-mobile-devices","text":"TODO","title":"Deploy on Mobile Devices"},{"location":"uq_learner/#algorithms","text":"Now we introduce the detailed algorithm in the Uniform Quantization Learner. As is shown in the following graph, given a full precision model, the Uniform Quantization Learner inserts quantization nodes into the computation graph of the model. To enable activation quantization, the quantization nodes shall also be inserted after the activation function. In the training phase, both full-precision and quantized weights are stored. During the forward pass, quantized weights are obtained by applying the quantization functions on the full precision weights. For the backward propagation of gradients, since the gradients w.r.t. the quantized weights are 0 almost everywhere, we use the straight-through estimator (STE) ( Hinton et.al 2012 , Bengio et.al 2013 ) to pass the gradient of quantized weights directly to the full precision weights for update.","title":"Algorithms"},{"location":"uq_learner/#uniform-quantization-function","text":"Uniform quantization distributed the quantization points evenly across the distribution of weights, and the full precision numbers are then assigned to the closest quantization point. To achieve this, we first normalize the full precision weights x x of one layer to [0, 1] [0, 1] , i.e., $$ sc(x) = \\frac{x-\\beta}{\\alpha}, $$ where \\alpha=\\max{x}-\\min{x} \\alpha=\\max{x}-\\min{x} and \\beta = \\min{x} \\beta = \\min{x} are the scaling factors. Then we assign sc(x) sc(x) to the discrete value by $$ \\hat{x}=\\frac{1}{2^k-1}\\mathrm{round}((2^k-1)\\cdot sc(x)), $$ and finally we do the inverse linear transformation to recover the quantized weights to the original scale, $$ Q(x)=\\alpha\\hat{x}+\\beta. $$","title":"Uniform Quantization Function"},{"location":"uq_learner/#references","text":"Bengio Y, L\u00e9onard N, Courville A. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013 Geoffrey Hinton, Nitish Srivastava, Kevin Swersky, Tijmen Tieleman and Abdelrahman Mohamed. Neural Networks for Machine Learning. Coursera, video lectures, 2012","title":"References"},{"location":"ws_learner/","text":"","title":"Weight Sparsification"}]}