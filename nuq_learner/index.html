<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Non-uniform Quantization - PocketFlow Docs</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Non-uniform Quantization";
    var mkdocs_page_input_path = "nuq_learner.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> PocketFlow Docs</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../installation/">Installation</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../quick_start/">Quick Start</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../self_defined_models/">Self-defined Models</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Learners</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../cp_learner/">Channel Pruning</a>
                </li>
                <li class="">
                    
    <a class="" href="../dcp_learner/">Discrimination-aware Channel Pruning</a>
                </li>
                <li class="">
                    
    <a class="" href="../ws_learner/">Weight Sparsification</a>
                </li>
                <li class="">
                    
    <a class="" href="../uq_learner/">Uniform Quantization</a>
                </li>
                <li class=" current">
                    
    <a class="current" href="./">Non-uniform Quantization</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#non-uniform-quantization-learner">Non-Uniform Quantization Learner</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#prepare-the-model">Prepare the Model</a></li>
        
            <li><a class="toctree-l4" href="#configure-the-learner">Configure the Learner</a></li>
        
            <li><a class="toctree-l4" href="#examples">Examples</a></li>
        
            <li><a class="toctree-l4" href="#performance">Performance</a></li>
        
            <li><a class="toctree-l4" href="#algorithm">Algorithm</a></li>
        
            <li><a class="toctree-l4" href="#references">References</a></li>
        
        </ul>
    

    </ul>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../distillation/">Distillation</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../multi_gpu/">Multi-GPU Training</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Hyper-parameter Optimizers</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../reinforcement_learning/">Reinforcement Learning</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../performance/">Performance</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../faq/">Frequently Asked Questions</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Appendix</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../pre_trained_models/">Pre-trained Models</a>
                </li>
                <li class="">
                    
    <a class="" href="../test_cases/">Test Cases</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">PocketFlow Docs</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
        
          <li>Learners &raquo;</li>
        
      
    
    <li>Non-uniform Quantization</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="non-uniform-quantization-learner">Non-Uniform Quantization Learner</h1>
<p>This document describes how to set up the Non-Uniform Quantization Learner in PocketFlow. In non-uniform quantization, the quantization points are not distributed evenly, and can be optimized via the back-propagation of the network gradients.
Consequently, with the same number of bits, non-uniform quantization is more expressive to approximate the original full-precision network comparing to uniform quantization.</p>
<p>Following a similar pattern in the previous sections, we first show how to configure the Non-Uniform Quantization Learner, followed by the algorithms used in the learner.</p>
<h3 id="prepare-the-model">Prepare the Model</h3>
<p>Again, users should first get the model prepared. Users can either use the pre-built models in PocketFlow, or develop their custom models according to <a href="???">TODO</a>.</p>
<h3 id="configure-the-learner">Configure the Learner</h3>
<p>To configure the learner, users can pass the options via the TensorFlow flag interface. The available options are as follows:</p>
<table>
<thead>
<tr>
<th align="left">Options</th>
<th align="center">Default Value</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><code>--nuql_opt_mode</code></td>
<td align="center">weight</td>
<td align="left">variables to optimize: ['weights', 'clusters', 'both']</td>
</tr>
<tr>
<td align="left"><code>--nuql_init_style</code></td>
<td align="center">quantile</td>
<td align="left">the initialization of quantization points: ['quantile', 'uniform']</td>
</tr>
<tr>
<td align="left"><code>--nuql_weight_bits</code></td>
<td align="center">4</td>
<td align="left">the number of bits for weight</td>
</tr>
<tr>
<td align="left"><code>--nuql_activation_bits</code></td>
<td align="center">32</td>
<td align="left">the number of bits for activationï¼Œ by default it remains full precision</td>
</tr>
<tr>
<td align="left"><code>--nuql_save_quant_mode_path</code></td>
<td align="center"><em>TODO</em></td>
<td align="left">the save path for quantized models</td>
</tr>
<tr>
<td align="left"><code>--nuql_use_buckets</code></td>
<td align="center">False</td>
<td align="left">use bucketing or not</td>
</tr>
<tr>
<td align="left"><code>--nuql_bucket_type</code></td>
<td align="center">channel</td>
<td align="left">two bucket type available: ['split', 'channel']</td>
</tr>
<tr>
<td align="left"><code>--nuql_bucket_size</code></td>
<td align="center">256</td>
<td align="left">quantize the first and last layers of the network or not</td>
</tr>
<tr>
<td align="left"><code>--nuql_enbl_rl_agent</code></td>
<td align="center">False</td>
<td align="left">enable reinforcement learning  to learn the optimal bit allocation or not</td>
</tr>
<tr>
<td align="left"><code>--nuql_quantize_all_layers</code></td>
<td align="center">False</td>
<td align="left">quantize the first and last layers of the network or not</td>
</tr>
<tr>
<td align="left"><code>--nuql_quant_epoch</code></td>
<td align="center">60</td>
<td align="left">the number of epochs for fine-tuning</td>
</tr>
</tbody>
</table>
<p>Note that since non-uniform quantization cannot be accelerated directly, by default we do not quantize the activations.</p>
<h3 id="examples">Examples</h3>
<p>Once the model is built, the Non-Uniform Quantization Learner can be easily triggered by passing the Uniform Quantization Learner in the command line as follows:</p>
<pre><code class="bash"># quantize resnet-20 on CIFAR-10
# you can also configure the
sh ./scripts/run_local.sh nets/resnet_at_cifar10_run.py \
--data_disk local \
--data_dir_local ${PF_CIFAR10_LOCAL} \
--learner=non-uniform \
--nuql_weight_bits=4 \
--nuql_activation_bits=4 \

# quantize the resnet-18 on ILSVRC-12
sh ./scripts/run_local.sh nets/resnet_at_ilsvrc12_run.py \
--learner=uniform \
--data_disk local \
--data_dir_local ${PF_ILSVRC12_LOCAL} \
--nuql_weight_bits=8 \
--nuql_activation_bits=8 \
--nuql_use_buckets=True \
--nuql_bucket_type=channel
</code></pre>

<p>To enable the RL agent, one can follow similar patterns as those in the Uniform Quantization Learner:</p>
<pre><code class="bash"># quantize mobilenet-v1 on ILSVRC-12
sh ./scripts/run_local.sh nets/mobilnet_at_ilsvrc12_run.py \
--data_disk local \
--data_dir_local ${PF_CIFAR10_LOCAL} \
--learner=uniform \
--nuql_enbl_rl_agent=True \
--nuql_equivalent_bits=4 \
--nuql_tune_global_steps=1200
</code></pre>

<h3 id="performance">Performance</h3>
<p>Here we list some of the performance on Cifar-10 using the Non-Uniform Quantization Learner and the built-in models in PocketFlow. The options not displayed remain the default values.</p>
<table>
<thead>
<tr>
<th align="center">Model</th>
<th align="center">Weight Bit</th>
<th align="center">Activation Bit</th>
<th align="center">Acc</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">ResNet-20</td>
<td align="center">32</td>
<td align="center">32</td>
<td align="center">91.96</td>
</tr>
<tr>
<td align="center">ResNet-20</td>
<td align="center">2</td>
<td align="center">4</td>
<td align="center">90.31</td>
</tr>
<tr>
<td align="center">ResNet-20</td>
<td align="center">4</td>
<td align="center">8</td>
<td align="center">91.70</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th align="center">Model</th>
<th align="center">Weight Bit</th>
<th align="center">Bucketing</th>
<th align="center">Acc</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">ResNet-20</td>
<td align="center">2</td>
<td align="center">channel</td>
<td align="center">90.90</td>
</tr>
<tr>
<td align="center">ResNet-20</td>
<td align="center">4</td>
<td align="center">channel</td>
<td align="center">91.97</td>
</tr>
<tr>
<td align="center">ResNet-20</td>
<td align="center">2</td>
<td align="center">split</td>
<td align="center">90.02</td>
</tr>
<tr>
<td align="center">ResNet-20</td>
<td align="center">4</td>
<td align="center">split</td>
<td align="center">91.56</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th align="center">Model</th>
<th align="center">Weight Bit</th>
<th align="center">RL search</th>
<th align="center">Acc</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">ResNet-20</td>
<td align="center">2</td>
<td align="center">FALSE</td>
<td align="center">90.31</td>
</tr>
<tr>
<td align="center">ResNet-20</td>
<td align="center">4</td>
<td align="center">FALSE</td>
<td align="center">91.70</td>
</tr>
<tr>
<td align="center">ResNet-20</td>
<td align="center">2</td>
<td align="center">TRUE</td>
<td align="center">90.60</td>
</tr>
<tr>
<td align="center">ResNet-20</td>
<td align="center">4</td>
<td align="center">TRUE</td>
<td align="center">91.79</td>
</tr>
</tbody>
</table>
<h2 id="algorithm">Algorithm</h2>
<p>Non-Uniform Quantization Learner adopts a similar training and evaluation procedure to the Uniform Quantization. In the training process, the quantized weights are forwarded. In the backward pass, the full precision weights are updated via the STE estimator. The major difference from uniform quantization is that, the location of quantization points are not evenly distributed, but can be optimized and initialized differently. In the following, we introduce the scheme to update and initialize the quantization points.</p>
<h3 id="optimization-the-quantization-points">Optimization the quantization points</h3>
<p>Unlike uniform quantization, non-uniform quantization can optimize the location of quantization points dynamically during the training of the network, and thereon leads to less quantization loss. The location of quantization points can be updated by summing the gradients of weights that fall into the point (<a href="https://arxiv.org/abs/1510.00149">Han et.al 2015</a>), i.e.,:
$$
\frac{\partial \mathcal{L}}{\partial c_k} = \sum_{i,j}\frac{\partial\mathcal{L}}{\partial w_{ij}}\frac{\partial{w_{ij}}}{\partial c_k}=\sum_{ij}\frac{\partial\mathcal{L}}{\partial{w_{ij}}}1(I_{ij}=k)
$$</p>
<p>The following figure taken from <a href="https://arxiv.org/abs/1510.00149">Han et.al 2015</a> shows the process of updating the clusters:
<img alt="" src="../1539834279663.png" /></p>
<h3 id="initialization-of-quantization-points">Initialization of quantization points</h3>
<p>Aside from optimizing the quantization points, another helpful strategy is to properly initialize the quantization points according to the distribution of weights. PocketFlow currently supports two kinds of initialization: uniform initialization and quantile initialization. Comparing to uniform initialization, quantile initialization uses the quantiles of weights as the initial locations of quantization points. Quantile initialization considers the distribution of weights and can usually lead to better performance.</p>
<h2 id="references">References</h2>
<p>Han S, Mao H, and Dally W J. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. <a href="https://arxiv.org/abs/1510.00149">arXiv:1510.00149, 2015</a></p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../distillation/" class="btn btn-neutral float-right" title="Distillation">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../uq_learner/" class="btn btn-neutral" title="Uniform Quantization"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../uq_learner/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../distillation/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../mathjax-config.js" defer></script>
      <script src="../MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../search/main.js" defer></script>

</body>
</html>
