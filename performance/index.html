<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Performance - PocketFlow Docs</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Performance";
    var mkdocs_page_input_path = "performance.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> PocketFlow Docs</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../installation/">Installation</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../tutorial/">Tutorial</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Learners - Algorithms</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../cp_learner/">Channel Pruning</a>
                </li>
                <li class="">
                    
    <a class="" href="../dcp_learner/">Discrimination-aware Channel Pruning</a>
                </li>
                <li class="">
                    
    <a class="" href="../ws_learner/">Weight Sparsification</a>
                </li>
                <li class="">
                    
    <a class="" href="../uq_learner/">Uniform Quantization</a>
                </li>
                <li class="">
                    
    <a class="" href="../nuq_learner/">Non-uniform Quantization</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Learners - Misc.</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../distillation/">Distillation</a>
                </li>
                <li class="">
                    
    <a class="" href="../multi_gpu_training/">Multi-GPU Training</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Hyper-parameter Optimizers</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../reinforcement_learning/">Reinforcement Learning</a>
                </li>
                <li class="">
                    
    <a class="" href="../automl_based_methods/">AutoML-based Methods</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1 current">
		
    <a class="current" href="./">Performance</a>
    <ul class="subnav">
            
    <li class="toctree-l2"><a href="#performance">Performance</a></li>
    

    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../faq/">Frequently Asked Questions</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Appendix</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../pre_trained_models/">Pre-trained Models</a>
                </li>
                <li class="">
                    
    <a class="" href="../test_cases/">Test Cases</a>
                </li>
                <li class="">
                    
    <a class="" href="../reference/">Reference</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">PocketFlow Docs</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Performance</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="performance">Performance</h1>
<p>In this documentation, we present some of our results for applying various model compression methods for ResNet and MobileNet models on the ImageNet classification task, including channel pruning, weight sparsification, and uniform quantization.</p>
<p>We adopt <code>ChannelPrunedLearner</code> to shrink the number of channels for convolutional layers to reduce the computation complexity.
Instead of using the same pruning ratio for all layers, we utilize the DDPG algorithm as the RL agent to iteratively search for the optimal pruning ratio of each layer.
After obtaining the optimal pruning ratios, group fine-tuning is adopted to further improve the compressed model's accuracy, as demonstrated below:</p>
<table>
<thead>
<tr>
<th align="center">Model</th>
<th align="center">Pruning Ratio</th>
<th align="center">Uniform</th>
<th align="center">RL-based</th>
<th align="center">RL-based + Group Fine-tuning</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">MobileNet-v1</td>
<td align="center">50%</td>
<td align="center">66.5%</td>
<td align="center">67.8% (+1.3%)</td>
<td align="center">67.9% (+1.4%)</td>
</tr>
<tr>
<td align="center">MobileNet-v1</td>
<td align="center">60%</td>
<td align="center">66.2%</td>
<td align="center">66.9% (+0.7%)</td>
<td align="center">67.0% (+0.8%)</td>
</tr>
<tr>
<td align="center">MobileNet-v1</td>
<td align="center">70%</td>
<td align="center">64.4%</td>
<td align="center">64.5% (+0.1%)</td>
<td align="center">64.8% (+0.4%)</td>
</tr>
<tr>
<td align="center">Mobilenet-v1</td>
<td align="center">80%</td>
<td align="center">61.4%</td>
<td align="center">61.4% (+0.0%)</td>
<td align="center">62.2% (+0.8%)</td>
</tr>
</tbody>
</table>
<p>We adopt <code>WeightSparseLearner</code> to introduce the sparsity constraint so that a large portion of model weights can be removed, which leads to smaller model and lower FLOPs for inference.
Comparing with the original algorithm proposed in (Zhu &amp; Gupta, 2017), we also incorporate network distillation and reinforcement learning algorithms to further improve the compressed model's accuracy, as shown in the table below (top-1 accuracy is reported):</p>
<table>
<thead>
<tr>
<th align="center">Model</th>
<th align="center">Sparsity</th>
<th align="center">(Zhu &amp; Gupta, 2017)</th>
<th align="center">RL-based + Distillation</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">MobileNet-v1</td>
<td align="center">0%</td>
<td align="center">70.6%</td>
<td align="center">-</td>
</tr>
<tr>
<td align="center">MobileNet-v1</td>
<td align="center">50%</td>
<td align="center">69.5%</td>
<td align="center">70.5% (+1.0%)</td>
</tr>
<tr>
<td align="center">MobileNet-v1</td>
<td align="center">75%</td>
<td align="center">67.7%</td>
<td align="center">68.5% (+0.8%)</td>
</tr>
<tr>
<td align="center">MobileNet-v1</td>
<td align="center">90%</td>
<td align="center">61.8%</td>
<td align="center">63.4% (+1.6%)</td>
</tr>
<tr>
<td align="center">MobileNet-v1</td>
<td align="center">95%</td>
<td align="center">53.6%</td>
<td align="center">56.8% (+3.2%)</td>
</tr>
</tbody>
</table>
<p>We adopt <code>UniformQuantTFLearner</code> to uniformly quantize model weights from 32-bit floating-point numbers to 8-bit fixed-point numbers.
The resulting model can be converted into the TensorFlow Lite format for deployment on mobile devices.
In the following two tables, we show that 8-bit quantized models can be as accurate as (or even better than) the original 32-bit ones, and the inference time can be significantly reduced after quantization.</p>
<table>
<thead>
<tr>
<th align="center">Model</th>
<th align="center">Top-1 Acc. (32-bit)</th>
<th align="center">Top-5 Acc. (32-bit)</th>
<th align="center">Top-1 Acc. (8-bit)</th>
<th align="center">Top-5 Acc. (8-bit)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">ResNet-18</td>
<td align="center">70.28%</td>
<td align="center">89.38%</td>
<td align="center">70.31% (+0.03%)</td>
<td align="center">89.40% (+0.02%)</td>
</tr>
<tr>
<td align="center">ResNet-50</td>
<td align="center">75.97%</td>
<td align="center">92.88%</td>
<td align="center">76.01% (+0.04%)</td>
<td align="center">92.87% (-0.01%)</td>
</tr>
<tr>
<td align="center">MobileNet-v1</td>
<td align="center">70.89%</td>
<td align="center">89.56%</td>
<td align="center">71.29% (+0.40%)</td>
<td align="center">89.79% (+0.23%)</td>
</tr>
<tr>
<td align="center">MobileNet-v2</td>
<td align="center">71.84%</td>
<td align="center">90.60%</td>
<td align="center">72.26% (+0.42%)</td>
<td align="center">90.77% (+0.17%)</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th align="center">Model</th>
<th align="center">Hardware</th>
<th align="center">CPU</th>
<th align="center">Time (32-bit)</th>
<th align="center">Time (8-bit)</th>
<th align="center">Speed-up</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">MobileNet-v1</td>
<td align="center">XiaoMi 8 SE</td>
<td align="center">Snapdragon 710</td>
<td align="center">156.33</td>
<td align="center">62.60</td>
<td align="center">2.50<span><span class="MathJax_Preview">\times</span><script type="math/tex">\times</script></span></td>
</tr>
<tr>
<td align="center">MobileNet-v1</td>
<td align="center">XiaoMI 8</td>
<td align="center">Snapdragon 845</td>
<td align="center">124.53</td>
<td align="center">56.12</td>
<td align="center">2.22<span><span class="MathJax_Preview">\times</span><script type="math/tex">\times</script></span></td>
</tr>
<tr>
<td align="center">MobileNet-v1</td>
<td align="center">Huawei P20</td>
<td align="center">Kirin 970</td>
<td align="center">152.54</td>
<td align="center">68.43</td>
<td align="center">2.23<span><span class="MathJax_Preview">\times</span><script type="math/tex">\times</script></span></td>
</tr>
<tr>
<td align="center">MobileNet-v2</td>
<td align="center">XiaoMi 8 SE</td>
<td align="center">Snapdragon 710</td>
<td align="center">153.18</td>
<td align="center">57.55</td>
<td align="center">2.50<span><span class="MathJax_Preview">\times</span><script type="math/tex">\times</script></span></td>
</tr>
<tr>
<td align="center">MobileNet-v2</td>
<td align="center">XiaoMi 8</td>
<td align="center">Snapdragon 845</td>
<td align="center">120.59</td>
<td align="center">49.04</td>
<td align="center">2.22<span><span class="MathJax_Preview">\times</span><script type="math/tex">\times</script></span></td>
</tr>
<tr>
<td align="center">MobileNet-v2</td>
<td align="center">Huawei P20</td>
<td align="center">Kirin 970</td>
<td align="center">226.61</td>
<td align="center">61.38</td>
<td align="center">2.23<span><span class="MathJax_Preview">\times</span><script type="math/tex">\times</script></span></td>
</tr>
</tbody>
</table>
<ul>
<li>All the reported time are in milliseconds.</li>
</ul>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../faq/" class="btn btn-neutral float-right" title="Frequently Asked Questions">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../automl_based_methods/" class="btn btn-neutral" title="AutoML-based Methods"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../automl_based_methods/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../faq/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../mathjax-config.js" defer></script>
      <script src="../MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../search/main.js" defer></script>

</body>
</html>
