<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Distillation - PocketFlow Docs</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Distillation";
    var mkdocs_page_input_path = "distillation.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> PocketFlow Docs</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../installation/">Installation</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../tutorial/">Tutorial</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Learners - Algorithms</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../cp_learner/">Channel Pruning</a>
                </li>
                <li class="">
                    
    <a class="" href="../dcp_learner/">Discrimination-aware Channel Pruning</a>
                </li>
                <li class="">
                    
    <a class="" href="../ws_learner/">Weight Sparsification</a>
                </li>
                <li class="">
                    
    <a class="" href="../uq_learner/">Uniform Quantization</a>
                </li>
                <li class="">
                    
    <a class="" href="../nuq_learner/">Non-uniform Quantization</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Learners - Misc.</span>
    <ul class="subnav">
                <li class=" current">
                    
    <a class="current" href="./">Distillation</a>
    <ul class="subnav">
            
    <li class="toctree-l3"><a href="#knowledge-distilation">Knowledge Distilation</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#compression-with-other-model-compression-approaches">Compression with Other Model Compression Approaches</a></li>
        
        </ul>
    

    </ul>
                </li>
                <li class="">
                    
    <a class="" href="../multi_gpu_training/">Multi-GPU Training</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Hyper-parameter Optimizers</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../reinforcement_learning/">Reinforcement Learning</a>
                </li>
                <li class="">
                    
    <a class="" href="../automl_based_methods/">AutoML-based Methods</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../performance/">Performance</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../faq/">Frequently Asked Questions</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Appendix</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../pre_trained_models/">Pre-trained Models</a>
                </li>
                <li class="">
                    
    <a class="" href="../test_cases/">Test Cases</a>
                </li>
                <li class="">
                    
    <a class="" href="../reference/">Reference</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">PocketFlow Docs</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
        
          <li>Learners - Misc. &raquo;</li>
        
      
    
    <li>Distillation</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="knowledge-distilation">Knowledge Distilation</h1>
<p>wledge Distillation
Knowledge Distillation is a kind of model compression approach in which a pre-trained large model teaches a smaller model to achieve the similar prediction capacity. It often named 'teacher-student' training, where the large model is the teacher and the smaller model is the student. The implementation used in PocketFlow is based on <a href="https://nervanasystems.github.io/distiller/knowledge_distillation/index.html#hinton-et-al-2015">Hinton et al., 2015</a>. </p>
<p>With distillation, knowledge can be transferred from the teacher model to the student by minimizing a loss function where the target is the distribution of class probabilities predicted by the teacher model. In most situations, the probability of the correct class predicted by the teacher model is very high, and the probabilities of other classes are closed to 0, which may provide very limited information beyond the ground truth labels. We can use the class probabilities produced by the cumbersome model as "soft targets" for training the small model. The solution is to raise the temperature of the final softmax until the cumbersome model produces a suitably soft set of targets.  The probability <span><span class="MathJax_Preview">q_i</span><script type="math/tex">q_i</script></span> of class <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> is calculated from the logits <span><span class="MathJax_Preview">z_i</span><script type="math/tex">z_i</script></span>:</p>
<div>
<div class="MathJax_Preview">q_i = \frac{exp(z_i / T)}{\sum_j{exp(z_j/T)}}</div>
<script type="math/tex; mode=display">q_i = \frac{exp(z_i / T)}{\sum_j{exp(z_j/T)}}</script>
</div>
<p>where <span><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span> is the temperature. As <span><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span> grows, the probability distribution is more softer, providing more information as to which classes the cumbersome model more similar to the predicted class. It is better to include the standard loss (<span><span class="MathJax_Preview">T=1</span><script type="math/tex">T=1</script></span>) between the predicted class probabilities and the ground-truth labels.  The overall loss function is:</p>
<div>
<div class="MathJax_Preview">L(x;W)=H(y,\sigma(z_s;T=1))+\alpha*H(\sigma(z_t;T=\tau),\sigma(z_s,T=\tau))</div>
<script type="math/tex; mode=display">L(x;W)=H(y,\sigma(z_s;T=1))+\alpha*H(\sigma(z_t;T=\tau),\sigma(z_s,T=\tau))</script>
</div>
<p>where <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> is the input, <span><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span> are parameters of the distilled small model and <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> is the ground truth label,  <span><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span> is the softmax parameterized by temperature <span><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span>, <span><span class="MathJax_Preview">H</span><script type="math/tex">H</script></span> is the cross entropy loss. <span><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> is the coefficient of the distillation loss. User can set <span><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> by <code>loss_w_dst</code> and set temperature <span><span class="MathJax_Preview">\tau</span><script type="math/tex">\tau</script></span> by <code>--tempr_dst</code>. </p>
<h3 id="compression-with-other-model-compression-approaches">Compression with Other Model Compression Approaches</h3>
<p>For other model model compression techniques such as channel pruning, weight pruning and quantization can be combined with the knowledge distilling. To use it, just enable it by setting <code>--enbl_dst=True</code>.</p>
<p>[1] Hinton, G., Vinyals, O. and Dean, J., 2015. Distilling the knowledge in a neural network. <em>arXiv preprint arXiv:1503.02531</em>.</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../multi_gpu_training/" class="btn btn-neutral float-right" title="Multi-GPU Training">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../nuq_learner/" class="btn btn-neutral" title="Non-uniform Quantization"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../nuq_learner/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../multi_gpu_training/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../mathjax-config.js" defer></script>
      <script src="../MathJax.js?config=TeX-AMS-MML_HTMLorMML" defer></script>
      <script src="../search/main.js" defer></script>

</body>
</html>
